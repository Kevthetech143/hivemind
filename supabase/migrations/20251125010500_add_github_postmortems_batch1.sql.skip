-- Add GitHub incident postmortems batch 1
-- Mined from official GitHub Status page and GitHub Engineering Blog availability reports
-- DOC-MINER-24: GitHub incident postmortem category - database outages, Actions downtime, authentication failures, webhook issues
-- Focus: Root cause → mitigation → prevention from real production incidents

INSERT INTO knowledge_entries (
    query, category, hit_frequency, solutions, prerequisites, success_indicators,
    common_pitfalls, success_rate, claude_version, last_verified, source_url
) VALUES (
    'GitHub Actions workflows failed to start after database load balancer change',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Immediately rollback database load balancer configuration changes if connection failures occur across multiple critical databases", "percentage": 95, "note": "GitHub April 5, 2024 incident - 100k+ workflows failed in 47 minutes"},
        {"solution": "Implement enhanced detection measures in deployment pipeline to catch database connection failures before full rollout", "percentage": 90, "note": "Prevention step from GitHub postmortem"},
        {"solution": "Monitor web request error rates (6%) and API error rates (10%) as early warning signals of database connectivity issues", "percentage": 85, "note": "These were the observable symptoms during the incident"},
        {"solution": "Establish rapid rollback procedures for database infrastructure changes - aim for sub-10 minute detection and rollback", "percentage": 80, "note": "Time to detection critical for limiting blast radius"}
    ]'::jsonb,
    'Database load balancer configuration deployed, Multiple critical databases affected, Rollback capability for recent changes',
    'Workflow start success rate returns to baseline, Database connection errors eliminated, Web/API error rates return to < 1%',
    'Database load balancer changes can cascade across multiple services simultaneously. Do not assume gradual rollout protects against configuration errors - connection failures were immediate and widespread. Enhanced monitoring must be in place BEFORE deployment, not after.',
    0.95,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-april-2024/'
),
(
    'GitHub webhook delivery failures and Actions delays due to missing authentication',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Verify webhook background job requests include proper authentication headers after configuration changes", "percentage": 95, "note": "GitHub July 5, 2024 - config change removed auth causing 97-minute outage"},
        {"solution": "Implement health checks that validate authentication on background job API endpoints", "percentage": 90, "note": "GitHub prevention: better health checks would have caught auth removal"},
        {"solution": "Add workload isolation for webhook delivery to prevent cascading failures to Actions", "percentage": 85, "note": "Webhook delays averaged 24 min (max 71 min), Actions delayed avg 45 sec"},
        {"solution": "Deploy service fix for crash loop in background job API if authentication failures cause job processor instability", "percentage": 80, "note": "Secondary issue occurred 18:21-21:14 UTC after initial fix"},
        {"solution": "Monitor webhook delivery latency as early warning - sustained delays indicate infrastructure issues", "percentage": 85, "command": "Track p50, p95, p99 webhook delivery times"}
    ]'::jsonb,
    'Webhook delivery infrastructure configured, Background job API running, Authentication configuration in place',
    'Webhook delivery returns to < 5 minute latency, Actions workflow start times normal, No authentication errors in background job logs',
    'Configuration changes can silently remove authentication - validate auth tokens remain in config after every change. Background job API crashes can compound webhook delays. Workload isolation prevents webhook issues from cascading to Actions.',
    0.92,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-july-2024/'
),
(
    'GitHub database split-brain after network partition between regions',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Reconfigure Orchestrator to prevent cross-regional database primary promotion during network partitions", "percentage": 95, "note": "GitHub Oct 21, 2018 - 24 hour outage from 43-second partition"},
        {"solution": "Pause webhook delivery and Pages builds immediately when database inconsistency detected to preserve data integrity", "percentage": 90, "note": "GitHub paused at 23:19 UTC - prevented further data conflicts"},
        {"solution": "Restore from remote backup storage when split-brain creates irreconcilable writes (30+ minutes of conflicting writes)", "percentage": 85, "note": "Multiple terabytes restored from cloud backup"},
        {"solution": "Lock deployment tooling during incident to prevent additional changes compounding the outage", "percentage": 90, "note": "Locked at 23:07 UTC - critical early step"},
        {"solution": "Accelerate migration to active/active/active multi-data center architecture to eliminate regional failover dependencies", "percentage": 80, "note": "Long-term prevention from GitHub engineering"}
    ]'::jsonb,
    'Multi-region database deployment, Orchestrator for cluster management, Remote backup storage configured, Regional data centers with replication',
    'Database primaries in single region, No conflicting writes across regions, Webhook backlog processed (5M+ events), Replication lag eliminated',
    'Network partitions as brief as 43 seconds can trigger catastrophic database failovers. Orchestrator will organize West Coast topologies during partition creating unsupported configs. Replication lag recovery follows power-decay not linear trajectory. ~200k webhook payloads can exceed TTL during extended outage.',
    0.88,
    'sonnet-4',
    NOW(),
    'https://github.blog/2018-10-30-oct21-post-incident-analysis/'
),
(
    'GitHub Actions runner misconfiguration causing CPU throttling',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Divert runner connections away from misconfigured nodes to healthy capacity immediately", "percentage": 95, "note": "GitHub Sept 16, 2024 - started at 21:16 UTC, 5 min after detection"},
        {"solution": "Monitor Actions job delays - average 23 minutes with peaks at 45 minutes indicates runner infrastructure issues", "percentage": 90, "note": "At peak, 80% of runs had delays exceeding 5 minutes"},
        {"solution": "Enhance monitoring to reduce detection time and enable automated mitigation of runner CPU throttling", "percentage": 85, "note": "GitHub prevention step - automated response for similar issues"},
        {"solution": "Verify runner connection manager configuration after deployments to catch CPU throttling settings", "percentage": 80, "note": "Misconfiguration in service managing runner connections"}
    ]'::jsonb,
    'GitHub Actions runner infrastructure deployed, Runner connection manager service configured, Monitoring for job delays in place',
    'Actions job delays return to < 5 minutes, CPU throttling eliminated on runner nodes, 80%+ of jobs start within expected timeframe',
    'Runner misconfigurations cause exponential delays - 5 minute detection delay resulted in 80% of jobs delayed. CPU throttling on connection manager affects all runners, not individual nodes. Automated mitigation critical - manual response too slow.',
    0.93,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-september-2024/'
),
(
    'GitHub Codespaces SNAT port exhaustion disconnecting active sessions',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Increase SNAT port allocations to provide buffer for increased outbound connections", "percentage": 95, "note": "GitHub Sept 24, 2024 - 25% error rate for 44 minutes"},
        {"solution": "Scale outbound connectivity capacity before hitting port exhaustion thresholds", "percentage": 90, "note": "Prevention: plans to scale outbound connectivity"},
        {"solution": "Add improved network capacity monitoring specifically for SNAT port utilization", "percentage": 85, "note": "GitHub identified monitoring gap for port exhaustion"},
        {"solution": "Monitor codespace disconnection rates - 25% error rate indicates SNAT exhaustion after deployment", "percentage": 80, "note": "Port exhaustion followed a deployment change"}
    ]'::jsonb,
    'GitHub Codespaces deployed, SNAT configured for outbound connections, Network capacity monitoring in place',
    'Codespace connection error rate < 1%, SNAT port utilization under threshold, Active sessions remain stable after deployments',
    'SNAT port exhaustion happens suddenly after deployments that increase outbound connections. Port allocation must include buffer for traffic spikes. 25% error rate indicates severe exhaustion. Monitoring port utilization is distinct from general network monitoring.',
    0.90,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-september-2024/'
),
(
    'GitHub unbounded database query overloading primary instance',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Scale up overloaded primary database instance immediately as emergency mitigation", "percentage": 90, "note": "GitHub April 10, 2024 - 120 minute outage from unbounded query"},
        {"solution": "Deploy improved query version to read replicas to reduce load on primary", "percentage": 85, "note": "Shipped improved query while primary scaled"},
        {"solution": "Roll back compute-intensive database queries that prevent cluster from serving other queries", "percentage": 95, "note": "Second incident same day - 30 min outage from bad query"},
        {"solution": "Enhance CI testing gap coverage to catch unbounded queries before production deployment", "percentage": 80, "note": "GitHub prevention: improved CI testing"},
        {"solution": "Add regression detection for database query performance degradation", "percentage": 75, "note": "Prevent reintroduction of problematic queries"}
    ]'::jsonb,
    'Database query deployed to production, Primary database instance running, Read replicas configured, CI testing pipeline in place',
    'Repository file edit success rate > 83%, Search failure rate < 5%, Database cluster serving all queries normally, No query CPU spikes',
    'Unbounded queries can overload primary database causing 17% file edit failures. Compute-intensive queries block entire cluster, not just slow down. Same root cause hit twice in one day (08:18 and 18:33 UTC). CI testing must include query performance benchmarks.',
    0.92,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-april-2024/'
),
(
    'GitHub email delivery delays up to 2 hours from unhealthy job queue',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Fix unhealthy internal job queue blocking email processing", "percentage": 95, "note": "GitHub April 11-14, 2024 - 3+ day outage for time-sensitive emails"},
        {"solution": "Pause job queue to prevent resource contention when queue becomes unhealthy", "percentage": 90, "note": "Immediate mitigation step taken"},
        {"solution": "Implement queue-bypass mechanism for time-sensitive emails (password resets, device verification)", "percentage": 85, "note": "Prevention: bypass queue for critical emails"},
        {"solution": "Add improved anomaly detection for shared resource usage spikes affecting email delivery", "percentage": 80, "note": "Root cause: increased shared resource usage"},
        {"solution": "Monitor email delivery latency for password reset and 2FA emails specifically - 2+ hour delays indicate queue issues", "percentage": 75, "note": "Time-sensitive emails most impacted"}
    ]'::jsonb,
    'Email delivery infrastructure running, Internal job queue configured, Shared resource monitoring in place, Password reset and 2FA flows active',
    'Password reset emails arrive < 5 minutes, Device verification emails < 5 minutes, Job queue health status normal, Shared resource utilization under threshold',
    'Job queue health affects all email delivery but time-sensitive emails (password reset, 2FA) most critical. 3+ day outage went undetected for time-sensitive category. Queue-bypass should be architectural requirement for security emails. Shared resource contention compounds queue issues.',
    0.88,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-april-2024/'
),
(
    'GitHub MySQL query syntax incompatible with database proxy after framework upgrade',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Rollback framework upgrade immediately when MySQL query syntax incompatibility detected with database proxy", "percentage": 95, "note": "GitHub March 15, 2024 - 42 minute outage affecting Codespaces, Actions, Pages"},
        {"solution": "Fix misconfiguration in development and CI environments to match production database proxy version", "percentage": 90, "note": "Prevention: ensure dev/CI mirrors production"},
        {"solution": "Test framework upgrades against database proxy service before production deployment", "percentage": 85, "note": "Proxy compatibility testing gap"},
        {"solution": "Monitor Codespaces, Actions, and Pages services together - simultaneous failures indicate database proxy issue", "percentage": 80, "note": "All three services failed from same root cause"}
    ]'::jsonb,
    'Framework upgrade deployed, MySQL database with proxy service, Development and CI environments configured, Codespaces/Actions/Pages services running',
    'No query syntax errors in database proxy logs, Codespaces/Actions/Pages operational, Framework upgrade successful without rollback',
    'Framework upgrades can introduce MySQL syntax incompatible with database proxy even if syntax is valid MySQL. Dev and CI environment misconfiguration means tests pass locally but fail in production. Query syntax errors manifest immediately as service failures, not gradual degradation.',
    0.90,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-march-2024/'
),
(
    'GitHub network configuration deployed to wrong environment causing API failures',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Initiate rollback within 4 minutes of network configuration deployment to wrong environment", "percentage": 95, "note": "GitHub March 11, 2024 - rollback at 22:49, majority resolved by 22:54 UTC"},
        {"solution": "Manually correct missing configuration fields affecting residual 0.4% of requests after initial rollback", "percentage": 85, "note": "Edge cases required manual intervention beyond rollback"},
        {"solution": "Implement safer configuration change deployment preventing obsolete configuration records", "percentage": 90, "note": "Prevention: safer config changes"},
        {"solution": "Add faster issue detection mechanisms for network configuration errors - aim for < 4 minute detection", "percentage": 80, "note": "4 minute detection was fast but still caused 2+ hour outage"},
        {"solution": "Monitor API requests, Copilot, Secret Scanning, and 2FA services together - simultaneous failures indicate network config issue", "percentage": 75, "note": "All affected by same network misconfiguration"}
    ]'::jsonb,
    'Network configuration management system in place, Rollback capability for config changes, Multi-service monitoring enabled, GitHub Mobile 2FA configured',
    'API request error rate < 1%, Copilot operational, Secret scanning processing normally, 2FA via GitHub Mobile functioning, No network config errors',
    'Network config deployed to wrong environment can cause 2+ hour outage despite 4-minute rollback. Rollback does not fix all issues - 0.4% required manual correction. Obsolete configuration records persist after rollback. Multiple service failures indicate infrastructure issue, not application bug.',
    0.88,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-march-2024/'
),
(
    'GitHub live updates service failure causing aggressive client refresh and server overload',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Rollback planned maintenance changes immediately when live updates service fails during maintenance window", "percentage": 95, "note": "GitHub Dec 17, 2024 - 17 min outage, 44.3% error rate peak"},
        {"solution": "Scale up services to handle increased WebSocket client traffic from aggressive refresh behavior", "percentage": 90, "note": "Emergency mitigation when clients refresh without live updates"},
        {"solution": "Reduce live updates service impact on github.com availability through architectural changes", "percentage": 85, "note": "Prevention: decouple live updates from core availability"},
        {"solution": "Add monitoring higher in request path to detect live updates failures before client refresh storm", "percentage": 80, "note": "Earlier detection critical - clients amplify failures"},
        {"solution": "Improve alerting to better identify incident scope when error rates spike to 8.5% average", "percentage": 75, "note": "Broad impact: login, repositories, pull requests, issues"}
    ]'::jsonb,
    'Live updates service deployed, WebSocket infrastructure configured, Planned maintenance windows scheduled, Login and core services running',
    'Error rate returns to < 1%, WebSocket connections stable, No aggressive client refresh behavior, Login and repository access operational',
    'Live updates service failure triggers client refresh storm that amplifies outage. 8.5% average error rate with 44.3% peak means over half of users affected at peak. Planned maintenance can unintentionally break critical services. Monitor request path early to detect issues before client behavior compounds them.',
    0.93,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-december-2024/'
),
(
    'GitHub Actions and Copilot latency from upstream cloud provider OS upgrade',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Increase number of network routes between data centers and cloud provider during provider maintenance", "percentage": 90, "note": "GitHub May 21, 2024 - 7.5 hour outage from upstream provider OS upgrade"},
        {"solution": "Monitor Copilot Chat p50 latency (2.5s) and p95 latency (6s) as indicators of network routing issues", "percentage": 85, "note": "Latency metrics revealed routing problems"},
        {"solution": "Identify gaps in monitoring and alerting for load thresholds on upstream dependencies", "percentage": 80, "note": "Prevention: better monitoring of provider dependencies"},
        {"solution": "Track Actions workflow delays (20-60 minutes) as signal of network or infrastructure degradation", "percentage": 75, "note": "Workflow updates stuck despite successful completion"},
        {"solution": "Implement improved detection and response for unintended traffic distribution in upstream clusters", "percentage": 70, "note": "Root cause: uneven traffic distribution after OS upgrade"}
    ]'::jsonb,
    'Upstream cloud provider hosting GitHub infrastructure, Scheduled OS upgrades from provider, Network routes between data centers configured, Load monitoring in place',
    'Copilot Chat p50 latency < 500ms, Actions workflow delays < 5 minutes, Billing metrics updated in real-time, No stuck workflow runs',
    'Upstream provider OS upgrades can create unintended traffic distribution affecting multiple services. Network routing issues manifest as latency, not errors. Workflow stuck UI while actually completing indicates metadata update lag. Provider maintenance schedules do not guarantee smooth traffic handling.',
    0.85,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-may-2024/'
),
(
    'GitHub Copilot outage from partner service mistakenly deleting essential resources',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Stop resource cleanup job immediately when partner service targets wrong resource group", "percentage": 95, "note": "GitHub July 13, 2024 - 19.5 hour Copilot outage from partner cleanup job"},
        {"solution": "Reroute Copilot Chat traffic to preserve remaining resources while investigating deletion", "percentage": 90, "note": "Traffic rerouting prevented complete service loss"},
        {"solution": "Collaborate with partner services to implement safeguards preventing accidental resource deletion", "percentage": 85, "note": "Prevention: cross-team safeguards"},
        {"solution": "Enhance traffic rerouting processes for faster recovery from resource deletion incidents", "percentage": 80, "note": "Rerouting capability critical for mitigation"},
        {"solution": "Monitor Copilot error rates - 1.16% for completions, 63% peak for Chat indicates resource availability issue", "percentage": 75, "note": "Error rate spike pattern diagnostic for resource deletion"}
    ]'::jsonb,
    'GitHub Copilot infrastructure deployed, Partner services with cleanup jobs configured, Resource groups properly labeled, Traffic routing capability available',
    'Copilot completion error rate < 1%, Chat error rate < 5%, Essential resources protected from cleanup jobs, Code scanning autofix operational',
    'Partner service cleanup jobs can target production resources if resource groups mislabeled. 19+ hour outage from resource deletion vs minutes for config rollback. Chat error rate (63%) much higher than completions (1.16%) indicates different resource dependencies. Resource deletion cannot be quickly undone like configuration changes.',
    0.88,
    'sonnet-4',
    NOW(),
    'https://github.blog/news-insights/company-news/github-availability-report-july-2024/'
);
