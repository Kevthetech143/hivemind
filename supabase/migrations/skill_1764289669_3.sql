INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, preview_summary, source_url, contributor_email)
VALUES (
  'LLM Evaluation - Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking',
  'claude-code',
  'skill',
  '[{"solution": "Automated Metrics Setup", "cli": {"macos": "pip install bert-score rouge-score nltk detoxify", "linux": "pip install bert-score rouge-score nltk detoxify", "windows": "pip install bert-score rouge-score nltk detoxify"}, "manual": "Implement BLEU, ROUGE, BERTScore, custom metrics like groundedness (NLI) and toxicity detection. Use sentence_bleu() for n-gram overlap, RougeScorer for summarization, BERTScore for semantic similarity, Detoxify for safety metrics.", "note": "Automated metrics enable fast, repeatable evaluation at scale"}, {"solution": "LLM-as-Judge Implementation", "manual": "Use stronger LLMs (GPT-5, Claude) to evaluate outputs. Implement pointwise rating (score individual responses), pairwise comparison (compare two responses), and reference-free judgment. Structure prompts to request structured JSON output for ratings and reasoning.", "note": "LLM judges provide nuanced evaluation without ground truth"}, {"solution": "Human Evaluation Framework", "manual": "Define annotation guidelines with clear rating scales (1-5), evaluation dimensions (accuracy, relevance, coherence, safety), and binary issue flags (hallucination, off-topic, unsafe). Measure inter-rater agreement using Cohen''s kappa. Track agreement quality.", "note": "Human evaluation captures quality aspects difficult to automate"}, {"solution": "A/B Testing Framework", "cli": {"macos": "pip install scipy numpy", "linux": "pip install scipy numpy", "windows": "pip install scipy numpy"}, "manual": "Implement ABTest class with t-tests for statistical significance. Calculate effect size using Cohen''s d. Set alpha=0.05 for significance threshold. Report relative improvement percentage and confidence levels.", "note": "Statistical testing ensures results aren''t due to random variation"}, {"solution": "Regression Detection", "manual": "Create RegressionDetector to compare new results against baselines. Flag metrics with relative change > -5% threshold. Integrate into CI/CD to prevent performance degradation. Store baseline results in version control.", "note": "Regression testing catches performance drops before production"}, {"solution": "Benchmarking Setup", "manual": "Use standard datasets (MMLU, HumanEval, TruthfulQA). Run BenchmarkRunner on model with multiple metrics. Report mean, std, min, max for each metric. Compare against published baselines and previous runs.", "note": "Benchmarking enables fair comparison across models and time"}]'::jsonb,
  'script',
  'Python 3.8+, understanding of statistics and evaluation metrics, access to LLM APIs, knowledge of NLP evaluation',
  'Using single metrics to optimize performance, small sample sizes, testing on training data, ignoring statistical variance, mismatching metrics to business goals, contaminated evaluation sets, not accounting for multiple models/configurations',
  'Evaluation suite runs without errors, all metrics report numerical scores, human annotations show acceptable inter-rater agreement (kappa > 0.6), A/B test results statistically significant (p < 0.05), regressions detected before deployment, benchmarks report mean and variance',
  'Comprehensive LLM evaluation with automated metrics, LLM-as-judge, human feedback, A/B testing, regression detection, and benchmarking',
  'https://skillsmp.com/skills/wshobson-agents-plugins-llm-application-dev-skills-llm-evaluation-skill-md',
  'admin:HAIKU_SKILL_1764289669_91423'
);
