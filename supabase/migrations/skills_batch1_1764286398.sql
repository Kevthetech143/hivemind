INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'at-dispatch-v2 - Convert PyTorch AT_DISPATCH macros to AT_DISPATCH_V2 format in ATen C++ code. Use when porting AT_DISPATCH_ALL_TYPES_AND*, AT_DISPATCH_FLOATING_TYPES*',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# AT_DISPATCH to AT_DISPATCH_V2 Converter\n\nThis skill helps convert PyTorch''''s legacy AT_DISPATCH macros to the new AT_DISPATCH_V2 format, as defined in `aten/src/ATen/Dispatch_v2.h`.\n\n## When to use this skill\n\nUse this skill when:\n- Converting AT_DISPATCH_* macros to AT_DISPATCH_V2\n- Porting ATen kernels to use the new dispatch API\n- Working with files in `aten/src/ATen/native/` that use dispatch macros\n- User mentions \"AT_DISPATCH\", \"dispatch v2\", \"Dispatch_v2.h\", or macro conversion\n\n## Quick reference\n\n**Old format:**\n```cpp\nAT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, dtype, \"kernel_name\", [&]() {\n  // lambda body\n});\n```\n\n**New format:**\n```cpp\nAT_DISPATCH_V2(dtype, \"kernel_name\", AT_WRAP([&]() {\n  // lambda body\n}), AT_EXPAND(AT_ALL_TYPES), kBFloat16, kHalf, kBool);\n```\n\n## Key transformations\n\n1. **Reorder arguments**: `scalar_type` and `name` come first, then lambda, then types\n2. **Wrap the lambda**: Use `AT_WRAP(lambda)` to handle internal commas\n3. **Expand type groups**: Use `AT_EXPAND(AT_ALL_TYPES)` instead of implicit expansion\n4. **List individual types**: Add extra types (kHalf, kBFloat16, etc.) after expanded groups\n5. **Add include**: `#include <ATen/Dispatch_v2.h>` near other Dispatch includes\n\n## Instructions\n\n### Step 1: Add the Dispatch_v2.h include\n\nAdd the v2 header near the existing `#include <ATen/Dispatch.h>`:\n\n```cpp\n#include <ATen/Dispatch.h>\n#include <ATen/Dispatch_v2.h>\n```\n\nKeep the old Dispatch.h include for now (other code may still need it).\n\n### Step 2: Identify the old dispatch pattern\n\nCommon patterns to convert:\n\n- `AT_DISPATCH_ALL_TYPES_AND{2,3,4}(type1, type2, ..., scalar_type, name, lambda)`\n- `AT_DISPATCH_FLOATING_TYPES_AND{2,3}(type1, type2, ..., scalar_type, name, lambda)`\n- `AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND{2,3}(type1, ..., scalar_type, name, lambda)`\n- `AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND{2,3}(type1, ..., scalar_type, name, lambda)`\n\n### Step 3: Map the old macro to type groups\n\nIdentify which type group macro corresponds to the base types:\n\n| Old macro base | AT_DISPATCH_V2 type group |\n|----------------|---------------------------|\n| `ALL_TYPES` | `AT_EXPAND(AT_ALL_TYPES)` |\n| `FLOATING_TYPES` | `AT_EXPAND(AT_FLOATING_TYPES)` |\n| `INTEGRAL_TYPES` | `AT_EXPAND(AT_INTEGRAL_TYPES)` |\n| `COMPLEX_TYPES` | `AT_EXPAND(AT_COMPLEX_TYPES)` |\n| `ALL_TYPES_AND_COMPLEX` | `AT_EXPAND(AT_ALL_TYPES_AND_COMPLEX)` |\n\nFor combined patterns, use multiple `AT_EXPAND()` entries:\n```cpp\n// Old: AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(...)\n// New: AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_COMPLEX_TYPES), type1, type2\n```\n\n### Step 4: Extract the individual types\n\nFrom `AT_DISPATCH_*_AND2(type1, type2, ...)` or `AT_DISPATCH_*_AND3(type1, type2, type3, ...)`, extract the individual types (type1, type2, etc.).\n\nThese become the trailing arguments after the type group:\n```cpp\nAT_DISPATCH_V2(..., AT_EXPAND(AT_ALL_TYPES), kBFloat16, kHalf, kBool)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^\n                                             Individual types from AND3\n```\n\n### Step 5: Transform to AT_DISPATCH_V2\n\nApply the transformation:\n\n**Pattern:**\n```cpp\nAT_DISPATCH_V2(\n  scalar_type,           // 1st: The dtype expression\n  \"name\",                // 2nd: The debug string\n  AT_WRAP(lambda),       // 3rd: The lambda wrapped in AT_WRAP\n  type_groups,           // 4th+: Type groups with AT_EXPAND()\n  individual_types       // Last: Individual types\n)\n```\n\n**Example transformation:**\n```cpp\n// BEFORE\nAT_DISPATCH_ALL_TYPES_AND3(\n    kBFloat16, kHalf, kBool,\n    iter.dtype(),\n    \"min_values_cuda\",\n    [&]() {\n      min_values_kernel_cuda_impl<scalar_t>(iter);\n    }\n);\n\n// AFTER\nAT_DISPATCH_V2(\n    iter.dtype(),\n    \"min_values_cuda\",\n    AT_WRAP([&]() {\n      min_values_kernel_cuda_impl<scalar_t>(iter);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    kBFloat16, kHalf, kBool\n);\n```\n\n### Step 6: Handle multi-line lambdas\n\nFor lambdas with internal commas or complex expressions, AT_WRAP is essential:\n\n```cpp\nAT_DISPATCH_V2(\n    dtype,\n    \"complex_kernel\",\n    AT_WRAP([&]() {\n      gpu_reduce_kernel<scalar_t, scalar_t>(\n        iter,\n        MinOps<scalar_t>{},\n        thrust::pair<scalar_t, int64_t>(upper_bound(), 0)  // Commas inside!\n      );\n    }),\n    AT_EXPAND(AT_ALL_TYPES)\n);\n```\n\n### Step 7: Verify the conversion\n\nCheck that:\n- [ ] `AT_WRAP()` wraps the entire lambda\n- [ ] Type groups use `AT_EXPAND()`\n- [ ] Individual types don''''t have `AT_EXPAND()` (just `kBFloat16`, not `AT_EXPAND(kBFloat16)`)\n- [ ] Argument order is: scalar_type, name, lambda, types\n- [ ] Include added: `#include <ATen/Dispatch_v2.h>`\n\n## Type group reference\n\nAvailable type group macros (use with `AT_EXPAND()`):\n\n```cpp\nAT_INTEGRAL_TYPES      // kByte, kChar, kInt, kLong, kShort\nAT_FLOATING_TYPES      // kDouble, kFloat\nAT_COMPLEX_TYPES       // kComplexDouble, kComplexFloat\nAT_QINT_TYPES         // kQInt8, kQUInt8, kQInt32\nAT_ALL_TYPES          // INTEGRAL_TYPES + FLOATING_TYPES\nAT_ALL_TYPES_AND_COMPLEX  // ALL_TYPES + COMPLEX_TYPES\nAT_INTEGRAL_TYPES_V2  // INTEGRAL_TYPES + unsigned types\nAT_BAREBONES_UNSIGNED_TYPES  // kUInt16, kUInt32, kUInt64\nAT_FLOAT8_TYPES       // Float8 variants\n```\n\n## Common patterns\n\n### Pattern: AT_DISPATCH_ALL_TYPES_AND2\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, \"op\", [&]() {\n  kernel<scalar_t>(data);\n});\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>(data);\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n```\n\n### Pattern: AT_DISPATCH_FLOATING_TYPES_AND3\n\n```cpp\n// Before\nAT_DISPATCH_FLOATING_TYPES_AND3(kHalf, kBFloat16, kFloat8_e4m3fn,\n    tensor.scalar_type(), \"float_op\", [&] {\n  process<scalar_t>(tensor);\n});\n\n// After\nAT_DISPATCH_V2(tensor.scalar_type(), \"float_op\", AT_WRAP([&] {\n  process<scalar_t>(tensor);\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf, kBFloat16, kFloat8_e4m3fn);\n```\n\n### Pattern: AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(\n    kComplexHalf, kHalf,\n    self.scalar_type(),\n    \"complex_op\",\n    [&] {\n      result = compute<scalar_t>(self);\n    }\n);\n\n// After\nAT_DISPATCH_V2(\n    self.scalar_type(),\n    \"complex_op\",\n    AT_WRAP([&] {\n      result = compute<scalar_t>(self);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    AT_EXPAND(AT_COMPLEX_TYPES),\n    kComplexHalf,\n    kHalf\n);\n```\n\n## Edge cases\n\n### Case 1: No extra types (rare)\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES(dtype, \"op\", [&]() { kernel<scalar_t>(); });\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES));\n```\n\n### Case 2: Many individual types (AND4, AND5, etc.)\n\n```cpp\n// Before\nAT_DISPATCH_FLOATING_TYPES_AND4(kHalf, kBFloat16, kFloat8_e4m3fn, kFloat8_e5m2,\n    dtype, \"float8_op\", [&]() { kernel<scalar_t>(); });\n\n// After\nAT_DISPATCH_V2(dtype, \"float8_op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf, kBFloat16, kFloat8_e4m3fn, kFloat8_e5m2);\n```\n\n### Case 3: Lambda with no captures\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBool, dtype, \"op\", []() {\n  static_kernel<scalar_t>();\n});\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([]() {\n  static_kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBool);\n```\n\n## Benefits of AT_DISPATCH_V2\n\n1. **No arity in macro name**: Don''''t need different macros for AND2, AND3, AND4\n2. **Composable type sets**: Mix and match type groups with `AT_EXPAND()`\n3. **Extensible**: Easy to add more types without hitting macro limits\n4. **Clearer**: Type groups are explicit, not implicit in macro name\n\n## Important notes\n\n- Keep `#include <ATen/Dispatch.h>` - other code may need it\n- The `AT_WRAP()` is mandatory - prevents comma parsing issues in the lambda\n- Type groups need `AT_EXPAND()`, individual types don''''t\n- The v2 API is in `aten/src/ATen/Dispatch_v2.h` - refer to it for full docs\n- See the header file for the Python script to regenerate the macro implementation\n\n## Workflow\n\nWhen asked to convert AT_DISPATCH macros:\n\n1. Read the file to identify all AT_DISPATCH uses\n2. Add `#include <ATen/Dispatch_v2.h>` if not present\n3. For each dispatch macro:\n   - Identify the pattern and extract components\n   - Map the base type group\n   - Extract individual types\n   - Construct the AT_DISPATCH_V2 call\n   - Apply with Edit tool\n4. Show the user the complete converted file\n5. Explain what was changed\n\nDo NOT compile or test the code - focus on accurate conversion only.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 10, error_handling: 8',
  'Score: 85, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-at-dispatch-v2-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'add-uint-support - Add unsigned integer (uint) type support to PyTorch operators by updating AT_DISPATCH macros. Use when adding support for uint16, uint32, uint64 types',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Add Unsigned Integer (uint) Support to Operators\n\nThis skill helps add support for unsigned integer types (uint16, uint32, uint64) to PyTorch operators by updating their AT_DISPATCH macros.\n\n## When to use this skill\n\nUse this skill when:\n- Adding uint16, uint32, or uint64 support to an operator\n- User mentions \"unsigned types\", \"uint support\", \"barebones unsigned types\"\n- Enabling support for kUInt16, kUInt32, kUInt64 in kernels\n- Working with operator implementations that need expanded type coverage\n\n## Quick reference\n\n**Add unsigned types to existing dispatch:**\n```cpp\n// Before\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES));\n\n// After (method 1: add unsigned types explicitly)\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES));\n\n// After (method 2: use V2 integral types if AT_INTEGRAL_TYPES present)\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_INTEGRAL_TYPES_V2), AT_EXPAND(AT_FLOATING_TYPES));\n```\n\n## Type group reference\n\n**Unsigned type groups:**\n- `AT_BAREBONES_UNSIGNED_TYPES`: kUInt16, kUInt32, kUInt64\n- `AT_INTEGRAL_TYPES_V2`: AT_INTEGRAL_TYPES + AT_BAREBONES_UNSIGNED_TYPES\n\n**Relationship:**\n```cpp\nAT_INTEGRAL_TYPES          // kByte, kChar, kInt, kLong, kShort\nAT_BAREBONES_UNSIGNED_TYPES  // kUInt16, kUInt32, kUInt64\nAT_INTEGRAL_TYPES_V2       // INTEGRAL_TYPES + BAREBONES_UNSIGNED_TYPES\n```\n\n## Instructions\n\n### Step 1: Determine if conversion to V2 is needed\n\nCheck if the file uses AT_DISPATCH_V2:\n\n**If using old AT_DISPATCH:**\n- First convert to AT_DISPATCH_V2 using the at-dispatch-v2 skill\n- Then proceed with adding uint support\n\n**If already using AT_DISPATCH_V2:**\n- Proceed directly to Step 2\n\n### Step 2: Analyze the current dispatch macro\n\nIdentify what type groups are currently in use:\n\n```cpp\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  // body\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\n    Current type coverage\n```\n\nCommon patterns:\n- `AT_EXPAND(AT_ALL_TYPES)` \u2192 includes AT_INTEGRAL_TYPES + AT_FLOATING_TYPES\n- `AT_EXPAND(AT_INTEGRAL_TYPES)` \u2192 signed integers only\n- `AT_EXPAND(AT_FLOATING_TYPES)` \u2192 floating point types\n\n### Step 3: Choose the uint addition method\n\nTwo approaches:\n\n**Method 1: Add AT_BAREBONES_UNSIGNED_TYPES explicitly**\n- Use when: You want to be explicit about adding uint support\n- Add `AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES)` to the type list\n\n**Method 2: Substitute AT_INTEGRAL_TYPES with AT_INTEGRAL_TYPES_V2**\n- Use when: The dispatch already uses `AT_EXPAND(AT_INTEGRAL_TYPES)`\n- More concise: replaces one type group with its superset\n- Only applicable if AT_INTEGRAL_TYPES is present\n\n### Step 4: Apply the transformation\n\n**Method 1 example:**\n```cpp\n// Before\nAT_DISPATCH_V2(\n    dtype,\n    \"min_values_cuda\",\n    AT_WRAP([&]() {\n      kernel_impl<scalar_t>(iter);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    kBFloat16, kHalf, kBool\n);\n\n// After (add unsigned types)\nAT_DISPATCH_V2(\n    dtype,\n    \"min_values_cuda\",\n    AT_WRAP([&]() {\n      kernel_impl<scalar_t>(iter);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES),\n    kBFloat16, kHalf, kBool\n);\n```\n\n**Method 2 example:**\n```cpp\n// Before\nAT_DISPATCH_V2(\n    dtype,\n    \"integral_op\",\n    AT_WRAP([&]() {\n      kernel<scalar_t>();\n    }),\n    AT_EXPAND(AT_INTEGRAL_TYPES)\n);\n\n// After (substitute with V2)\nAT_DISPATCH_V2(\n    dtype,\n    \"integral_op\",\n    AT_WRAP([&]() {\n      kernel<scalar_t>();\n    }),\n    AT_EXPAND(AT_INTEGRAL_TYPES_V2)\n);\n```\n\n### Step 5: Handle AT_ALL_TYPES vs individual type groups\n\nIf the dispatch uses `AT_EXPAND(AT_ALL_TYPES)`:\n- `AT_ALL_TYPES` = `AT_INTEGRAL_TYPES` + `AT_FLOATING_TYPES`\n- To add uint: add `AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES)` to the list\n\nIf the dispatch separately lists INTEGRAL and FLOATING:\n```cpp\n// Before\nAT_EXPAND(AT_INTEGRAL_TYPES), AT_EXPAND(AT_FLOATING_TYPES)\n\n// After (Method 2 preferred)\nAT_EXPAND(AT_INTEGRAL_TYPES_V2), AT_EXPAND(AT_FLOATING_TYPES)\n```\n\n### Step 6: Verify all dispatch sites\n\nCheck the file for ALL dispatch macros that need uint support:\n- Some operators have multiple dispatch sites (CPU, CUDA, different functions)\n- Apply the transformation consistently across all sites\n- Ensure each gets the same type coverage updates\n\n### Step 7: Validate the changes\n\nCheck that:\n- [ ] AT_DISPATCH_V2 format is used (not old AT_DISPATCH)\n- [ ] Unsigned types are added via one of the two methods\n- [ ] All relevant dispatch sites in the file are updated\n- [ ] Type groups use `AT_EXPAND()`\n- [ ] Arguments are properly formatted and comma-separated\n\n## Common patterns\n\n### Pattern 1: AT_ALL_TYPES + extras\n\n```cpp\n// Before\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kHalf, kBFloat16);\n```\n\n### Pattern 2: Separate INTEGRAL + FLOATING\n\n```cpp\n// Before\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_INTEGRAL_TYPES), AT_EXPAND(AT_FLOATING_TYPES));\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_INTEGRAL_TYPES_V2), AT_EXPAND(AT_FLOATING_TYPES));\n```\n\n### Pattern 3: Old dispatch needs conversion first\n\n```cpp\n// Before (needs v2 conversion first)\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, \"op\", [&]() {\n  kernel<scalar_t>();\n});\n\n// After v2 conversion\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n\n// After adding uint support\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kHalf, kBFloat16);\n```\n\n## Multiple dispatch sites example\n\nFor a file with multiple functions:\n\n```cpp\nvoid min_values_kernel_cuda(TensorIterator& iter) {\n  AT_DISPATCH_V2(iter.dtype(), \"min_values_cuda\", AT_WRAP([&]() {\n    impl<scalar_t>(iter);\n  }), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kBFloat16, kHalf);\n  //                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  //                           Added uint support\n}\n\nvoid min_launch_kernel(TensorIterator &iter) {\n  AT_DISPATCH_V2(iter.input_dtype(), \"min_cuda\", AT_WRAP([&]() {\n    gpu_reduce_kernel<scalar_t>(iter);\n  }), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kBFloat16, kHalf);\n  //                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  //                           Added uint support here too\n}\n```\n\n## Decision tree\n\nUse this decision tree to determine the approach:\n\n```\nIs the file using AT_DISPATCH_V2?\n\u251c\u2500 No \u2192 Use at-dispatch-v2 skill first, then continue\n\u2514\u2500 Yes\n   \u2514\u2500 Does it use AT_EXPAND(AT_INTEGRAL_TYPES)?\n      \u251c\u2500 Yes \u2192 Replace with AT_EXPAND(AT_INTEGRAL_TYPES_V2)\n      \u2514\u2500 No \u2192 Add AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES) to type list\n```\n\n## Edge cases\n\n### Case 1: Dispatch with only floating types\n\nIf the operator only supports floating point types, don''''t add uint support:\n\n```cpp\n// Leave as-is - floating point only operator\nAT_DISPATCH_V2(dtype, \"float_op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf);\n```\n\n### Case 2: Complex types present\n\nUnsigned types work alongside complex types:\n\n```cpp\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES),\n    AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES),\n    AT_EXPAND(AT_COMPLEX_TYPES),\n    kHalf, kBFloat16);\n```\n\n### Case 3: Already has uint support\n\nCheck if uint types are already present:\n- If `AT_INTEGRAL_TYPES_V2` is used \u2192 already has uint support\n- If `AT_BAREBONES_UNSIGNED_TYPES` is already in list \u2192 already has uint support\n- Skip the file if uint support is already present\n\n## Workflow\n\nWhen asked to add uint support:\n\n1. Read the target file\n2. Check if using AT_DISPATCH_V2:\n   - If not \u2192 use at-dispatch-v2 skill first\n3. Identify all dispatch macro sites\n4. For each dispatch:\n   - Analyze current type groups\n   - Choose method (add BAREBONES_UNSIGNED or upgrade to V2)\n   - Apply transformation with Edit tool\n5. Show the user the changes\n6. Explain what was modified\n\n## Important notes\n\n- Always check if v2 conversion is needed first\n- Apply changes consistently across all dispatch sites in the file\n- Method 2 (AT_INTEGRAL_TYPES_V2) is cleaner when applicable\n- Method 1 (explicit AT_BAREBONES_UNSIGNED_TYPES) is more explicit\n- Unsigned types are: kUInt16, kUInt32, kUInt64 (not kByte which is uint8)\n- Some operators may not semantically support unsigned types - use judgment\n\n## Testing\n\nAfter adding uint support, the operator should accept uint16, uint32, and uint64 tensors. The user is responsible for functional testing.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 12, completeness: 10, error_handling: 7',
  'Score: 81, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-add-uint-support-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'skill-writer - Guide users through creating Agent Skills for Claude Code. Use when the user wants to create, write, author, or design a new Skill, or needs help with',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Skill Writer\n\nThis Skill helps you create well-structured Agent Skills for Claude Code that follow best practices and validation requirements.\n\n## When to use this Skill\n\nUse this Skill when:\n- Creating a new Agent Skill\n- Writing or updating SKILL.md files\n- Designing skill structure and frontmatter\n- Troubleshooting skill discovery issues\n- Converting existing prompts or workflows into Skills\n\n## Instructions\n\n### Step 1: Determine Skill scope\n\nFirst, understand what the Skill should do:\n\n1. **Ask clarifying questions**:\n   - What specific capability should this Skill provide?\n   - When should Claude use this Skill?\n   - What tools or resources does it need?\n   - Is this for personal use or team sharing?\n\n2. **Keep it focused**: One Skill = one capability\n   - Good: \"PDF form filling\", \"Excel data analysis\"\n   - Too broad: \"Document processing\", \"Data tools\"\n\n### Step 2: Choose Skill location\n\nDetermine where to create the Skill:\n\n**Personal Skills** (`~/.claude/skills/`):\n- Individual workflows and preferences\n- Experimental Skills\n- Personal productivity tools\n\n**Project Skills** (`.claude/skills/`):\n- Team workflows and conventions\n- Project-specific expertise\n- Shared utilities (committed to git)\n\n### Step 3: Create Skill structure\n\nCreate the directory and files:\n\n```bash\n# Personal\nmkdir -p ~/.claude/skills/skill-name\n\n# Project\nmkdir -p .claude/skills/skill-name\n```\n\nFor multi-file Skills:\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md (required)\n\u251c\u2500\u2500 reference.md (optional)\n\u251c\u2500\u2500 examples.md (optional)\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 helper.py (optional)\n\u2514\u2500\u2500 templates/\n    \u2514\u2500\u2500 template.txt (optional)\n```\n\n### Step 4: Write SKILL.md frontmatter\n\nCreate YAML frontmatter with required fields:\n\n```yaml\n---\nname: skill-name\ndescription: Brief description of what this does and when to use it\n---\n```\n\n**Field requirements**:\n\n- **name**:\n  - Lowercase letters, numbers, hyphens only\n  - Max 64 characters\n  - Must match directory name\n  - Good: `pdf-processor`, `git-commit-helper`\n  - Bad: `PDF_Processor`, `Git Commits!`\n\n- **description**:\n  - Max 1024 characters\n  - Include BOTH what it does AND when to use it\n  - Use specific trigger words users would say\n  - Mention file types, operations, and context\n\n**Optional frontmatter fields**:\n\n- **allowed-tools**: Restrict tool access (comma-separated list)\n  ```yaml\n  allowed-tools: Read, Grep, Glob\n  ```\n  Use for:\n  - Read-only Skills\n  - Security-sensitive workflows\n  - Limited-scope operations\n\n### Step 5: Write effective descriptions\n\nThe description is critical for Claude to discover your Skill.\n\n**Formula**: `[What it does] + [When to use it] + [Key triggers]`\n\n**Examples**:\n\n\u2705 **Good**:\n```yaml\ndescription: Extract text and tables from PDF files, fill forms, merge documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.\n```\n\n\u2705 **Good**:\n```yaml\ndescription: Analyze Excel spreadsheets, create pivot tables, and generate charts. Use when working with Excel files, spreadsheets, or analyzing tabular data in .xlsx format.\n```\n\n\u274c **Too vague**:\n```yaml\ndescription: Helps with documents\ndescription: For data analysis\n```\n\n**Tips**:\n- Include specific file extensions (.pdf, .xlsx, .json)\n- Mention common user phrases (\"analyze\", \"extract\", \"generate\")\n- List concrete operations (not generic verbs)\n- Add context clues (\"Use when...\", \"For...\")\n\n### Step 6: Structure the Skill content\n\nUse clear Markdown sections:\n\n```markdown\n# Skill Name\n\nBrief overview of what this Skill does.\n\n## Quick start\n\nProvide a simple example to get started immediately.\n\n## Instructions\n\nStep-by-step guidance for Claude:\n1. First step with clear action\n2. Second step with expected outcome\n3. Handle edge cases\n\n## Examples\n\nShow concrete usage examples with code or commands.\n\n## Best practices\n\n- Key conventions to follow\n- Common pitfalls to avoid\n- When to use vs. not use\n\n## Requirements\n\nList any dependencies or prerequisites:\n```bash\npip install package-name\n```\n\n## Advanced usage\n\nFor complex scenarios, see [reference.md](reference.md).\n```\n\n### Step 7: Add supporting files (optional)\n\nCreate additional files for progressive disclosure:\n\n**reference.md**: Detailed API docs, advanced options\n**examples.md**: Extended examples and use cases\n**scripts/**: Helper scripts and utilities\n**templates/**: File templates or boilerplate\n\nReference them from SKILL.md:\n```markdown\nFor advanced usage, see [reference.md](reference.md).\n\nRun the helper script:\n\\`\\`\\`bash\npython scripts/helper.py input.txt\n\\`\\`\\`\n```\n\n### Step 8: Validate the Skill\n\nCheck these requirements:\n\n\u2705 **File structure**:\n- [ ] SKILL.md exists in correct location\n- [ ] Directory name matches frontmatter `name`\n\n\u2705 **YAML frontmatter**:\n- [ ] Opening `---` on line 1\n- [ ] Closing `---` before content\n- [ ] Valid YAML (no tabs, correct indentation)\n- [ ] `name` follows naming rules\n- [ ] `description` is specific and < 1024 chars\n\n\u2705 **Content quality**:\n- [ ] Clear instructions for Claude\n- [ ] Concrete examples provided\n- [ ] Edge cases handled\n- [ ] Dependencies listed (if any)\n\n\u2705 **Testing**:\n- [ ] Description matches user questions\n- [ ] Skill activates on relevant queries\n- [ ] Instructions are clear and actionable\n\n### Step 9: Test the Skill\n\n1. **Restart Claude Code** (if running) to load the Skill\n\n2. **Ask relevant questions** that match the description:\n   ```\n   Can you help me extract text from this PDF?\n   ```\n\n3. **Verify activation**: Claude should use the Skill automatically\n\n4. **Check behavior**: Confirm Claude follows the instructions correctly\n\n### Step 10: Debug if needed\n\nIf Claude doesn''''t use the Skill:\n\n1. **Make description more specific**:\n   - Add trigger words\n   - Include file types\n   - Mention common user phrases\n\n2. **Check file location**:\n   ```bash\n   ls ~/.claude/skills/skill-name/SKILL.md\n   ls .claude/skills/skill-name/SKILL.md\n   ```\n\n3. **Validate YAML**:\n   ```bash\n   cat SKILL.md | head -n 10\n   ```\n\n4. **Run debug mode**:\n   ```bash\n   claude --debug\n   ```\n\n## Common patterns\n\n### Read-only Skill\n\n```yaml\n---\nname: code-reader\ndescription: Read and analyze code without making changes. Use for code review, understanding codebases, or documentation.\nallowed-tools: Read, Grep, Glob\n---\n```\n\n### Script-based Skill\n\n```yaml\n---\nname: data-processor\ndescription: Process CSV and JSON data files with Python scripts. Use when analyzing data files or transforming datasets.\n---\n\n# Data Processor\n\n## Instructions\n\n1. Use the processing script:\n\\`\\`\\`bash\npython scripts/process.py input.csv --output results.json\n\\`\\`\\`\n\n2. Validate output with:\n\\`\\`\\`bash\npython scripts/validate.py results.json\n\\`\\`\\`\n```\n\n### Multi-file Skill with progressive disclosure\n\n```yaml\n---\nname: api-designer\ndescription: Design REST APIs following best practices. Use when creating API endpoints, designing routes, or planning API architecture.\n---\n\n# API Designer\n\nQuick start: See [examples.md](examples.md)\n\nDetailed reference: See [reference.md](reference.md)\n\n## Instructions\n\n1. Gather requirements\n2. Design endpoints (see examples.md)\n3. Document with OpenAPI spec\n4. Review against best practices (see reference.md)\n```\n\n## Best practices for Skill authors\n\n1. **One Skill, one purpose**: Don''''t create mega-Skills\n2. **Specific descriptions**: Include trigger words users will say\n3. **Clear instructions**: Write for Claude, not humans\n4. **Concrete examples**: Show real code, not pseudocode\n5. **List dependencies**: Mention required packages in description\n6. **Test with teammates**: Verify activation and clarity\n7. **Version your Skills**: Document changes in content\n8. **Use progressive disclosure**: Put advanced details in separate files\n\n## Validation checklist\n\nBefore finalizing a Skill, verify:\n\n- [ ] Name is lowercase, hyphens only, max 64 chars\n- [ ] Description is specific and < 1024 chars\n- [ ] Description includes \"what\" and \"when\"\n- [ ] YAML frontmatter is valid\n- [ ] Instructions are step-by-step\n- [ ] Examples are concrete and realistic\n- [ ] Dependencies are documented\n- [ ] File paths use forward slashes\n- [ ] Skill activates on relevant queries\n- [ ] Claude follows instructions correctly\n\n## Troubleshooting\n\n**Skill doesn''''t activate**:\n- Make description more specific with trigger words\n- Include file types and operations in description\n- Add \"Use when...\" clause with user phrases\n\n**Multiple Skills conflict**:\n- Make descriptions more distinct\n- Use different trigger words\n- Narrow the scope of each Skill\n\n**Skill has errors**:\n- Check YAML syntax (no tabs, proper indentation)\n- Verify file paths (use forward slashes)\n- Ensure scripts have execute permissions\n- List all dependencies\n\n## Examples\n\nSee the documentation for complete examples:\n- Simple single-file Skill (commit-helper)\n- Skill with tool permissions (code-reviewer)\n- Multi-file Skill (pdf-processing)\n\n## Output format\n\nWhen creating a Skill, I will:\n\n1. Ask clarifying questions about scope and requirements\n2. Suggest a Skill name and location\n3. Create the SKILL.md file with proper frontmatter\n4. Include clear instructions and examples\n5. Add supporting files if needed\n6. Provide testing instructions\n7. Validate against all requirements\n\nThe result will be a complete, working Skill that follows all best practices and validation rules.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 10',
  'Score: 95, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-skill-writer-skill-md',
  'admin:MINER_BATCH_1'
);