-- AWS S3 Error Codes - Official Docs Mining
-- Source: docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html
-- Mined: 2025-11-25

INSERT INTO knowledge_entries (
    query, category, hit_frequency, solutions, prerequisites, success_indicators,
    common_pitfalls, success_rate, claude_version, last_verified, source_url
) VALUES (
    'AWS S3 error: AccessDenied - Access Denied insufficient permissions',
    'aws-s3',
    'CRITICAL',
    $$[{"solution": "1. Check IAM policies for your AWS user/role. 2. Verify the principal has s3:GetObject, s3:PutObject, or required S3 actions. 3. Ensure bucket policy allows access from your principal. 4. Check if bucket is in a different account - may need cross-account access setup.", "percentage": 95, "note": "Most common cause is missing IAM permissions or bucket policy restrictions"}, {"solution": "If using temporary credentials (STS), verify session token is valid and hasn''t expired", "percentage": 85, "note": "Token expiration is frequent cause in automated workflows"}]$$::jsonb,
    'AWS credentials configured, IAM console access',
    'Can perform S3 operation without 403 errors',
    'Forgetting bucket policies exist separately from IAM; not checking for bucket-level restrictions; expired temporary credentials',
    0.92,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: NoSuchBucket - The specified bucket does not exist',
    'aws-s3',
    'CRITICAL',
    $$[{"solution": "1. Verify bucket name spelling matches exactly. 2. Confirm bucket exists in AWS S3 console. 3. Check bucket exists in correct AWS region. 4. Ensure bucket wasn''t deleted or typo in configuration. 5. Use aws s3 ls command to list all buckets accessible to your credentials.", "percentage": 98, "note": "Simplest to resolve - almost always a naming or region issue"}, {"solution": "If bucket is in different AWS account, verify cross-account access is configured", "percentage": 80, "note": "Less common but critical for multi-account setups"}]$$::jsonb,
    'AWS CLI installed and configured, bucket exists in account',
    'Successful S3 list-objects or head-bucket call',
    'Case sensitivity in bucket name; using region-specific endpoint without changing region config; typos in bucket reference',
    0.96,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: NoSuchKey - The resource you requested does not exist',
    'aws-s3',
    'CRITICAL',
    $$[{"solution": "1. Verify object key (path) spelling is exact. 2. Confirm object exists using aws s3 ls s3://bucket-name/path/ command. 3. Check for leading/trailing spaces or special characters in key. 4. Confirm object wasn''t deleted or moved. 5. Review object prefix if using folder structure simulation.", "percentage": 94, "note": "Key names are case-sensitive and include full path from bucket root"}, {"solution": "If using version IDs, verify VersionId is correct for the object version you need", "percentage": 75, "note": "Version-specific requests fail if version was deleted"}]$$::jsonb,
    'AWS S3 access confirmed, bucket exists',
    'Successful object retrieval or HEAD object call without 404',
    'Forgetting S3 has no real folders - keys are full paths; assuming case-insensitive keys; not accounting for URL encoding of special characters',
    0.91,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: InvalidAccessKeyId - The AWS access key ID does not exist',
    'aws-s3',
    'CRITICAL',
    $$[{"solution": "1. Verify AWS_ACCESS_KEY_ID environment variable or config file has correct value. 2. Check IAM user still exists and hasn''t been deleted. 3. Confirm access key hasn''t been deactivated in IAM console. 4. Regenerate access key if uncertain - create new key pair in IAM. 5. Update all config files and environment variables with new credentials.", "percentage": 93, "note": "Usually caused by wrong credentials or key deactivation"}, {"solution": "If using shared credentials file (~/.aws/credentials), ensure correct profile is selected with AWS_PROFILE env var", "percentage": 88, "note": "Profile misconfiguration causes this in SDK usage"}]$$::jsonb,
    'IAM console access, ability to generate new credentials',
    'Successful AWS API call after credential update',
    'Hardcoding credentials instead of using environment variables; forgetting to export AWS_ACCESS_KEY_ID after updating; using revoked credentials without regenerating',
    0.89,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: SignatureDoesNotMatch - Request signature does not match',
    'aws-s3',
    'CRITICAL',
    $$[{"solution": "1. Verify AWS_SECRET_ACCESS_KEY is correct and matches the Access Key ID. 2. Confirm system clock is synced (signature includes timestamp). 3. Check request body hasn''t been modified after signing. 4. Verify signing algorithm version matches AWS SDK version. 5. Re-initialize AWS SDK client with correct credentials.", "percentage": 90, "note": "Most common in request signing or clock skew scenarios"}, {"solution": "If using custom request signing, verify algorithm follows AWS Signature V4 specification exactly", "percentage": 82, "note": "Manual signing implementations often have subtle bugs"}]$$::jsonb,
    'Correct AWS credentials, system time within 5min of AWS servers',
    'Successful S3 request without signature validation error',
    'Using old/wrong secret key; system clock far out of sync (>5 min difference); modifying request body after signing; incorrect request signing implementation',
    0.88,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: BucketAlreadyExists - The bucket name already exists',
    'aws-s3',
    'HIGH',
    $$[{"solution": "1. Choose a globally unique bucket name. 2. Append timestamp or UUID to name: my-bucket-1732000000. 3. Use your organization identifier: company-projectname-timestamp. 4. Check if your org already owns the bucket - search AWS console. 5. Create bucket with unique name in CreateBucket operation.", "percentage": 97, "note": "Bucket namespace is global across all AWS accounts - uniqueness required"}, {"solution": "If bucket exists and you own it, import into infrastructure code instead of recreating", "percentage": 85, "note": "Use terraform import or CDK import for existing buckets"}]$$::jsonb,
    'Permission to create S3 buckets, unique name generated',
    'Successful bucket creation without 409 conflict error',
    'Using generic bucket names (e.g., ''my-bucket'' without suffix); forgetting bucket names are global not regional; trying to recreate deleted bucket without waiting for DNS propagation',
    0.94,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: SlowDown - Request rate exceeds provisioned throughput',
    'aws-s3',
    'HIGH',
    $$[{"solution": "1. Implement exponential backoff: wait 1s, then 2s, 4s, 8s between retries. 2. Add random jitter to prevent thundering herd. 3. Reduce concurrent request rate - limit to 100-200 concurrent operations. 4. Increase delay between batch operations. 5. Use AWS SDK built-in retry logic with exponential backoff.", "percentage": 92, "note": "SDK retry handling usually solves this - configure max retries"}, {"solution": "For high-throughput workloads, design keys with random prefix to distribute across partition keys", "percentage": 80, "note": "S3 internally partitions by key prefix - randomness improves parallelism"}]$$::jsonb,
    'AWS SDK configured, ability to modify request rate',
    'Requests succeed after backoff implemented, no more 503 SlowDown errors',
    'Immediate retry without backoff causing thundering herd; all operations using same key prefix causing partition hot-spot; not implementing jitter in exponential backoff',
    0.86,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: ServiceUnavailable - S3 service temporarily unavailable',
    'aws-s3',
    'MEDIUM',
    $$[{"solution": "1. Implement automatic retry with exponential backoff (1-30 second delays). 2. Check AWS Service Health Dashboard for ongoing incidents. 3. Add circuit breaker pattern to fail fast if service down >5 min. 4. Route traffic to alternate region if available. 5. Log incident and alert operations team.", "percentage": 89, "note": "Usually resolves within 1-5 minutes - most clients don''t need action"}, {"solution": "Verify your network can reach S3 endpoints - test with aws s3 ls after service returns", "percentage": 75, "note": "Distinguish network issues from actual service unavailability"}]$$::jsonb,
    'AWS SDK configured, ability to retry requests',
    'Successful S3 requests resume after service recovery',
    'Not implementing any retry logic; giving up after single failure; confusing with network/permissions errors; not checking AWS status page before escalating',
    0.84,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: InvalidBucketName - Bucket name does not follow DNS rules',
    'aws-s3',
    'MEDIUM',
    $$[{"solution": "1. Ensure bucket name is 3-63 characters long. 2. Use only lowercase letters, numbers, periods, and hyphens. 3. Start and end with letter or number. 4. No consecutive periods (..) allowed. 5. Cannot be IP address format (e.g., 192.168.1.1). 6. Regenerate name following S3 naming rules.", "percentage": 96, "note": "S3 bucket names must be valid DNS hostnames"}, {"solution": "Avoid common mistakes: underscores not allowed, no uppercase letters, no leading/trailing hyphens", "percentage": 94, "note": "Many developers unfamiliar with DNS naming constraints"}]$$::jsonb,
    'Understanding of DNS naming rules',
    'Successful bucket creation with valid name',
    'Using underscores (not valid in DNS); uppercase letters; consecutive hyphens or periods; names starting/ending with hyphen; IP address format names',
    0.95,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: MalformedACLHeader - Invalid ACL in request header',
    'aws-s3',
    'MEDIUM',
    $$[{"solution": "1. Use only valid canned ACL values: private, public-read, public-read-write, authenticated-read, bucket-owner-read, bucket-owner-full-control. 2. If using custom ACL header, ensure proper format with grantee and permissions. 3. Verify x-amz-acl header value is one of approved values. 4. Use AWS SDK to set ACL instead of manual headers for safety.", "percentage": 93, "note": "SDK handles ACL formatting automatically - prefer SDK over raw headers"}, {"solution": "For fine-grained access, use bucket policies and IAM instead of object ACLs", "percentage": 80, "note": "ACLs are legacy - policies provide better control and auditability"}]$$::jsonb,
    'AWS credentials, understanding of S3 ACL options',
    'Successful object creation or ACL modification without 400 error',
    'Using invalid canned ACL values; manual header creation with syntax errors; not realizing ACLs are per-object not bucket-wide; using deprecated grant syntax',
    0.90,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: NoSuchUpload - Multipart upload ID does not exist',
    'aws-s3',
    'MEDIUM',
    $$[{"solution": "1. Verify UploadId matches initiated multipart upload. 2. Confirm upload hasn''t timed out (multipart uploads auto-expire after 7 days). 3. List active uploads: aws s3api list-multipart-uploads --bucket mybucket. 4. If upload expired, initiate new multipart upload and retry. 5. Store UploadId immediately after InitiateMultipartUpload call.", "percentage": 91, "note": "Usually caused by stale UploadIds from incomplete workflows"}, {"solution": "Implement multipart upload with automatic cleanup to abort incomplete uploads >24hrs old", "percentage": 78, "note": "Prevents orphaned uploads consuming storage"}]$$::jsonb,
    'AWS S3 multipart upload in progress, bucket write permissions',
    'Successful CompleteMultipartUpload without 404 NoSuchUpload',
    'Storing old UploadIds and reusing after timeout; not aborting failed multipart uploads; losing UploadId in application crash without persistence',
    0.87,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: RequestTimeTooSkewed - Request timestamp too far from current time',
    'aws-s3',
    'HIGH',
    $$[{"solution": "1. Sync system clock with NTP: ntpdate -s time.nist.gov (Linux/Mac) or sync with internet time (Windows). 2. Verify system clock is within 15 minutes of AWS servers. 3. Check timezone settings are correct. 4. For containers/VMs, ensure host time is correct before starting instance. 5. Disable system clock skew checks in development only (insecure for production).", "percentage": 96, "note": "Clock skew is common in development VMs and containers"}, {"solution": "Use AWS CloudWatch Events to monitor for time-skew related failures", "percentage": 72, "note": "Helps detect systematic time issues in infrastructure"}]$$::jsonb,
    'System administrator access, NTP utilities available',
    'Successful S3 requests without RequestTimeTooSkewed error after time sync',
    'Ignoring system clock in development; not syncing time in containerized environments; blaming AWS instead of checking local time; hardcoding time bypass for production',
    0.93,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
),
(
    'AWS S3 error: EntityTooLarge - Request entity exceeds maximum allowed size',
    'aws-s3',
    'HIGH',
    $$[{"solution": "1. For PUT operations, limit object size to 5GB. 2. For objects >5GB, use multipart upload (unlimited size). 3. If body too large in request, compress before upload: gzip or brotli. 4. Check Content-Length header doesn''t exceed limits. 5. For POST operations, verify form data size <5GB.", "percentage": 94, "note": "S3 has hard 5GB limit on single PUT - multipart for larger"}, {"solution": "Implement streaming upload with multipart upload for large files to reduce memory", "percentage": 85, "note": "Streaming prevents loading entire file in memory"}]$$::jsonb,
    'S3 PUT/POST permissions, ability to implement multipart upload',
    'Successful large file upload without 400 EntityTooLarge error',
    'Trying to PUT objects >5GB without multipart; not compressing large payloads; loading entire file in memory instead of streaming; not checking actual content size vs declared size',
    0.91,
    'sonnet-4',
    NOW(),
    'https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html'
);
