INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'at-dispatch-v2 - Convert PyTorch AT_DISPATCH macros to AT_DISPATCH_V2 format in ATen C++ code. Use when porting AT_DISPATCH_ALL_TYPES_AND*, AT_DISPATCH_FLOATING_TYPES*',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# AT_DISPATCH to AT_DISPATCH_V2 Converter\n\nThis skill helps convert PyTorch''''s legacy AT_DISPATCH macros to the new AT_DISPATCH_V2 format, as defined in `aten/src/ATen/Dispatch_v2.h`.\n\n## When to use this skill\n\nUse this skill when:\n- Converting AT_DISPATCH_* macros to AT_DISPATCH_V2\n- Porting ATen kernels to use the new dispatch API\n- Working with files in `aten/src/ATen/native/` that use dispatch macros\n- User mentions \"AT_DISPATCH\", \"dispatch v2\", \"Dispatch_v2.h\", or macro conversion\n\n## Quick reference\n\n**Old format:**\n```cpp\nAT_DISPATCH_ALL_TYPES_AND3(kBFloat16, kHalf, kBool, dtype, \"kernel_name\", [&]() {\n  // lambda body\n});\n```\n\n**New format:**\n```cpp\nAT_DISPATCH_V2(dtype, \"kernel_name\", AT_WRAP([&]() {\n  // lambda body\n}), AT_EXPAND(AT_ALL_TYPES), kBFloat16, kHalf, kBool);\n```\n\n## Key transformations\n\n1. **Reorder arguments**: `scalar_type` and `name` come first, then lambda, then types\n2. **Wrap the lambda**: Use `AT_WRAP(lambda)` to handle internal commas\n3. **Expand type groups**: Use `AT_EXPAND(AT_ALL_TYPES)` instead of implicit expansion\n4. **List individual types**: Add extra types (kHalf, kBFloat16, etc.) after expanded groups\n5. **Add include**: `#include <ATen/Dispatch_v2.h>` near other Dispatch includes\n\n## Instructions\n\n### Step 1: Add the Dispatch_v2.h include\n\nAdd the v2 header near the existing `#include <ATen/Dispatch.h>`:\n\n```cpp\n#include <ATen/Dispatch.h>\n#include <ATen/Dispatch_v2.h>\n```\n\nKeep the old Dispatch.h include for now (other code may still need it).\n\n### Step 2: Identify the old dispatch pattern\n\nCommon patterns to convert:\n\n- `AT_DISPATCH_ALL_TYPES_AND{2,3,4}(type1, type2, ..., scalar_type, name, lambda)`\n- `AT_DISPATCH_FLOATING_TYPES_AND{2,3}(type1, type2, ..., scalar_type, name, lambda)`\n- `AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND{2,3}(type1, ..., scalar_type, name, lambda)`\n- `AT_DISPATCH_FLOATING_AND_COMPLEX_TYPES_AND{2,3}(type1, ..., scalar_type, name, lambda)`\n\n### Step 3: Map the old macro to type groups\n\nIdentify which type group macro corresponds to the base types:\n\n| Old macro base | AT_DISPATCH_V2 type group |\n|----------------|---------------------------|\n| `ALL_TYPES` | `AT_EXPAND(AT_ALL_TYPES)` |\n| `FLOATING_TYPES` | `AT_EXPAND(AT_FLOATING_TYPES)` |\n| `INTEGRAL_TYPES` | `AT_EXPAND(AT_INTEGRAL_TYPES)` |\n| `COMPLEX_TYPES` | `AT_EXPAND(AT_COMPLEX_TYPES)` |\n| `ALL_TYPES_AND_COMPLEX` | `AT_EXPAND(AT_ALL_TYPES_AND_COMPLEX)` |\n\nFor combined patterns, use multiple `AT_EXPAND()` entries:\n```cpp\n// Old: AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(...)\n// New: AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_COMPLEX_TYPES), type1, type2\n```\n\n### Step 4: Extract the individual types\n\nFrom `AT_DISPATCH_*_AND2(type1, type2, ...)` or `AT_DISPATCH_*_AND3(type1, type2, type3, ...)`, extract the individual types (type1, type2, etc.).\n\nThese become the trailing arguments after the type group:\n```cpp\nAT_DISPATCH_V2(..., AT_EXPAND(AT_ALL_TYPES), kBFloat16, kHalf, kBool)\n                                             ^^^^^^^^^^^^^^^^^^^^^^^^\n                                             Individual types from AND3\n```\n\n### Step 5: Transform to AT_DISPATCH_V2\n\nApply the transformation:\n\n**Pattern:**\n```cpp\nAT_DISPATCH_V2(\n  scalar_type,           // 1st: The dtype expression\n  \"name\",                // 2nd: The debug string\n  AT_WRAP(lambda),       // 3rd: The lambda wrapped in AT_WRAP\n  type_groups,           // 4th+: Type groups with AT_EXPAND()\n  individual_types       // Last: Individual types\n)\n```\n\n**Example transformation:**\n```cpp\n// BEFORE\nAT_DISPATCH_ALL_TYPES_AND3(\n    kBFloat16, kHalf, kBool,\n    iter.dtype(),\n    \"min_values_cuda\",\n    [&]() {\n      min_values_kernel_cuda_impl<scalar_t>(iter);\n    }\n);\n\n// AFTER\nAT_DISPATCH_V2(\n    iter.dtype(),\n    \"min_values_cuda\",\n    AT_WRAP([&]() {\n      min_values_kernel_cuda_impl<scalar_t>(iter);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    kBFloat16, kHalf, kBool\n);\n```\n\n### Step 6: Handle multi-line lambdas\n\nFor lambdas with internal commas or complex expressions, AT_WRAP is essential:\n\n```cpp\nAT_DISPATCH_V2(\n    dtype,\n    \"complex_kernel\",\n    AT_WRAP([&]() {\n      gpu_reduce_kernel<scalar_t, scalar_t>(\n        iter,\n        MinOps<scalar_t>{},\n        thrust::pair<scalar_t, int64_t>(upper_bound(), 0)  // Commas inside!\n      );\n    }),\n    AT_EXPAND(AT_ALL_TYPES)\n);\n```\n\n### Step 7: Verify the conversion\n\nCheck that:\n- [ ] `AT_WRAP()` wraps the entire lambda\n- [ ] Type groups use `AT_EXPAND()`\n- [ ] Individual types don''''t have `AT_EXPAND()` (just `kBFloat16`, not `AT_EXPAND(kBFloat16)`)\n- [ ] Argument order is: scalar_type, name, lambda, types\n- [ ] Include added: `#include <ATen/Dispatch_v2.h>`\n\n## Type group reference\n\nAvailable type group macros (use with `AT_EXPAND()`):\n\n```cpp\nAT_INTEGRAL_TYPES      // kByte, kChar, kInt, kLong, kShort\nAT_FLOATING_TYPES      // kDouble, kFloat\nAT_COMPLEX_TYPES       // kComplexDouble, kComplexFloat\nAT_QINT_TYPES         // kQInt8, kQUInt8, kQInt32\nAT_ALL_TYPES          // INTEGRAL_TYPES + FLOATING_TYPES\nAT_ALL_TYPES_AND_COMPLEX  // ALL_TYPES + COMPLEX_TYPES\nAT_INTEGRAL_TYPES_V2  // INTEGRAL_TYPES + unsigned types\nAT_BAREBONES_UNSIGNED_TYPES  // kUInt16, kUInt32, kUInt64\nAT_FLOAT8_TYPES       // Float8 variants\n```\n\n## Common patterns\n\n### Pattern: AT_DISPATCH_ALL_TYPES_AND2\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, \"op\", [&]() {\n  kernel<scalar_t>(data);\n});\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>(data);\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n```\n\n### Pattern: AT_DISPATCH_FLOATING_TYPES_AND3\n\n```cpp\n// Before\nAT_DISPATCH_FLOATING_TYPES_AND3(kHalf, kBFloat16, kFloat8_e4m3fn,\n    tensor.scalar_type(), \"float_op\", [&] {\n  process<scalar_t>(tensor);\n});\n\n// After\nAT_DISPATCH_V2(tensor.scalar_type(), \"float_op\", AT_WRAP([&] {\n  process<scalar_t>(tensor);\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf, kBFloat16, kFloat8_e4m3fn);\n```\n\n### Pattern: AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(\n    kComplexHalf, kHalf,\n    self.scalar_type(),\n    \"complex_op\",\n    [&] {\n      result = compute<scalar_t>(self);\n    }\n);\n\n// After\nAT_DISPATCH_V2(\n    self.scalar_type(),\n    \"complex_op\",\n    AT_WRAP([&] {\n      result = compute<scalar_t>(self);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    AT_EXPAND(AT_COMPLEX_TYPES),\n    kComplexHalf,\n    kHalf\n);\n```\n\n## Edge cases\n\n### Case 1: No extra types (rare)\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES(dtype, \"op\", [&]() { kernel<scalar_t>(); });\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES));\n```\n\n### Case 2: Many individual types (AND4, AND5, etc.)\n\n```cpp\n// Before\nAT_DISPATCH_FLOATING_TYPES_AND4(kHalf, kBFloat16, kFloat8_e4m3fn, kFloat8_e5m2,\n    dtype, \"float8_op\", [&]() { kernel<scalar_t>(); });\n\n// After\nAT_DISPATCH_V2(dtype, \"float8_op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf, kBFloat16, kFloat8_e4m3fn, kFloat8_e5m2);\n```\n\n### Case 3: Lambda with no captures\n\n```cpp\n// Before\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBool, dtype, \"op\", []() {\n  static_kernel<scalar_t>();\n});\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([]() {\n  static_kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBool);\n```\n\n## Benefits of AT_DISPATCH_V2\n\n1. **No arity in macro name**: Don''''t need different macros for AND2, AND3, AND4\n2. **Composable type sets**: Mix and match type groups with `AT_EXPAND()`\n3. **Extensible**: Easy to add more types without hitting macro limits\n4. **Clearer**: Type groups are explicit, not implicit in macro name\n\n## Important notes\n\n- Keep `#include <ATen/Dispatch.h>` - other code may need it\n- The `AT_WRAP()` is mandatory - prevents comma parsing issues in the lambda\n- Type groups need `AT_EXPAND()`, individual types don''''t\n- The v2 API is in `aten/src/ATen/Dispatch_v2.h` - refer to it for full docs\n- See the header file for the Python script to regenerate the macro implementation\n\n## Workflow\n\nWhen asked to convert AT_DISPATCH macros:\n\n1. Read the file to identify all AT_DISPATCH uses\n2. Add `#include <ATen/Dispatch_v2.h>` if not present\n3. For each dispatch macro:\n   - Identify the pattern and extract components\n   - Map the base type group\n   - Extract individual types\n   - Construct the AT_DISPATCH_V2 call\n   - Apply with Edit tool\n4. Show the user the complete converted file\n5. Explain what was changed\n\nDo NOT compile or test the code - focus on accurate conversion only.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 10, error_handling: 8',
  'Score: 85, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-at-dispatch-v2-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'add-uint-support - Add unsigned integer (uint) type support to PyTorch operators by updating AT_DISPATCH macros. Use when adding support for uint16, uint32, uint64 types',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Add Unsigned Integer (uint) Support to Operators\n\nThis skill helps add support for unsigned integer types (uint16, uint32, uint64) to PyTorch operators by updating their AT_DISPATCH macros.\n\n## When to use this skill\n\nUse this skill when:\n- Adding uint16, uint32, or uint64 support to an operator\n- User mentions \"unsigned types\", \"uint support\", \"barebones unsigned types\"\n- Enabling support for kUInt16, kUInt32, kUInt64 in kernels\n- Working with operator implementations that need expanded type coverage\n\n## Quick reference\n\n**Add unsigned types to existing dispatch:**\n```cpp\n// Before\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES));\n\n// After (method 1: add unsigned types explicitly)\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES));\n\n// After (method 2: use V2 integral types if AT_INTEGRAL_TYPES present)\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_INTEGRAL_TYPES_V2), AT_EXPAND(AT_FLOATING_TYPES));\n```\n\n## Type group reference\n\n**Unsigned type groups:**\n- `AT_BAREBONES_UNSIGNED_TYPES`: kUInt16, kUInt32, kUInt64\n- `AT_INTEGRAL_TYPES_V2`: AT_INTEGRAL_TYPES + AT_BAREBONES_UNSIGNED_TYPES\n\n**Relationship:**\n```cpp\nAT_INTEGRAL_TYPES          // kByte, kChar, kInt, kLong, kShort\nAT_BAREBONES_UNSIGNED_TYPES  // kUInt16, kUInt32, kUInt64\nAT_INTEGRAL_TYPES_V2       // INTEGRAL_TYPES + BAREBONES_UNSIGNED_TYPES\n```\n\n## Instructions\n\n### Step 1: Determine if conversion to V2 is needed\n\nCheck if the file uses AT_DISPATCH_V2:\n\n**If using old AT_DISPATCH:**\n- First convert to AT_DISPATCH_V2 using the at-dispatch-v2 skill\n- Then proceed with adding uint support\n\n**If already using AT_DISPATCH_V2:**\n- Proceed directly to Step 2\n\n### Step 2: Analyze the current dispatch macro\n\nIdentify what type groups are currently in use:\n\n```cpp\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  // body\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n    ^^^^^^^^^^^^^^^^^^^^^^^^^\n    Current type coverage\n```\n\nCommon patterns:\n- `AT_EXPAND(AT_ALL_TYPES)` \u2192 includes AT_INTEGRAL_TYPES + AT_FLOATING_TYPES\n- `AT_EXPAND(AT_INTEGRAL_TYPES)` \u2192 signed integers only\n- `AT_EXPAND(AT_FLOATING_TYPES)` \u2192 floating point types\n\n### Step 3: Choose the uint addition method\n\nTwo approaches:\n\n**Method 1: Add AT_BAREBONES_UNSIGNED_TYPES explicitly**\n- Use when: You want to be explicit about adding uint support\n- Add `AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES)` to the type list\n\n**Method 2: Substitute AT_INTEGRAL_TYPES with AT_INTEGRAL_TYPES_V2**\n- Use when: The dispatch already uses `AT_EXPAND(AT_INTEGRAL_TYPES)`\n- More concise: replaces one type group with its superset\n- Only applicable if AT_INTEGRAL_TYPES is present\n\n### Step 4: Apply the transformation\n\n**Method 1 example:**\n```cpp\n// Before\nAT_DISPATCH_V2(\n    dtype,\n    \"min_values_cuda\",\n    AT_WRAP([&]() {\n      kernel_impl<scalar_t>(iter);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    kBFloat16, kHalf, kBool\n);\n\n// After (add unsigned types)\nAT_DISPATCH_V2(\n    dtype,\n    \"min_values_cuda\",\n    AT_WRAP([&]() {\n      kernel_impl<scalar_t>(iter);\n    }),\n    AT_EXPAND(AT_ALL_TYPES),\n    AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES),\n    kBFloat16, kHalf, kBool\n);\n```\n\n**Method 2 example:**\n```cpp\n// Before\nAT_DISPATCH_V2(\n    dtype,\n    \"integral_op\",\n    AT_WRAP([&]() {\n      kernel<scalar_t>();\n    }),\n    AT_EXPAND(AT_INTEGRAL_TYPES)\n);\n\n// After (substitute with V2)\nAT_DISPATCH_V2(\n    dtype,\n    \"integral_op\",\n    AT_WRAP([&]() {\n      kernel<scalar_t>();\n    }),\n    AT_EXPAND(AT_INTEGRAL_TYPES_V2)\n);\n```\n\n### Step 5: Handle AT_ALL_TYPES vs individual type groups\n\nIf the dispatch uses `AT_EXPAND(AT_ALL_TYPES)`:\n- `AT_ALL_TYPES` = `AT_INTEGRAL_TYPES` + `AT_FLOATING_TYPES`\n- To add uint: add `AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES)` to the list\n\nIf the dispatch separately lists INTEGRAL and FLOATING:\n```cpp\n// Before\nAT_EXPAND(AT_INTEGRAL_TYPES), AT_EXPAND(AT_FLOATING_TYPES)\n\n// After (Method 2 preferred)\nAT_EXPAND(AT_INTEGRAL_TYPES_V2), AT_EXPAND(AT_FLOATING_TYPES)\n```\n\n### Step 6: Verify all dispatch sites\n\nCheck the file for ALL dispatch macros that need uint support:\n- Some operators have multiple dispatch sites (CPU, CUDA, different functions)\n- Apply the transformation consistently across all sites\n- Ensure each gets the same type coverage updates\n\n### Step 7: Validate the changes\n\nCheck that:\n- [ ] AT_DISPATCH_V2 format is used (not old AT_DISPATCH)\n- [ ] Unsigned types are added via one of the two methods\n- [ ] All relevant dispatch sites in the file are updated\n- [ ] Type groups use `AT_EXPAND()`\n- [ ] Arguments are properly formatted and comma-separated\n\n## Common patterns\n\n### Pattern 1: AT_ALL_TYPES + extras\n\n```cpp\n// Before\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kHalf, kBFloat16);\n```\n\n### Pattern 2: Separate INTEGRAL + FLOATING\n\n```cpp\n// Before\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_INTEGRAL_TYPES), AT_EXPAND(AT_FLOATING_TYPES));\n\n// After\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_INTEGRAL_TYPES_V2), AT_EXPAND(AT_FLOATING_TYPES));\n```\n\n### Pattern 3: Old dispatch needs conversion first\n\n```cpp\n// Before (needs v2 conversion first)\nAT_DISPATCH_ALL_TYPES_AND2(kHalf, kBFloat16, dtype, \"op\", [&]() {\n  kernel<scalar_t>();\n});\n\n// After v2 conversion\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), kHalf, kBFloat16);\n\n// After adding uint support\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kHalf, kBFloat16);\n```\n\n## Multiple dispatch sites example\n\nFor a file with multiple functions:\n\n```cpp\nvoid min_values_kernel_cuda(TensorIterator& iter) {\n  AT_DISPATCH_V2(iter.dtype(), \"min_values_cuda\", AT_WRAP([&]() {\n    impl<scalar_t>(iter);\n  }), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kBFloat16, kHalf);\n  //                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  //                           Added uint support\n}\n\nvoid min_launch_kernel(TensorIterator &iter) {\n  AT_DISPATCH_V2(iter.input_dtype(), \"min_cuda\", AT_WRAP([&]() {\n    gpu_reduce_kernel<scalar_t>(iter);\n  }), AT_EXPAND(AT_ALL_TYPES), AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES), kBFloat16, kHalf);\n  //                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  //                           Added uint support here too\n}\n```\n\n## Decision tree\n\nUse this decision tree to determine the approach:\n\n```\nIs the file using AT_DISPATCH_V2?\n\u251c\u2500 No \u2192 Use at-dispatch-v2 skill first, then continue\n\u2514\u2500 Yes\n   \u2514\u2500 Does it use AT_EXPAND(AT_INTEGRAL_TYPES)?\n      \u251c\u2500 Yes \u2192 Replace with AT_EXPAND(AT_INTEGRAL_TYPES_V2)\n      \u2514\u2500 No \u2192 Add AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES) to type list\n```\n\n## Edge cases\n\n### Case 1: Dispatch with only floating types\n\nIf the operator only supports floating point types, don''''t add uint support:\n\n```cpp\n// Leave as-is - floating point only operator\nAT_DISPATCH_V2(dtype, \"float_op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_FLOATING_TYPES), kHalf);\n```\n\n### Case 2: Complex types present\n\nUnsigned types work alongside complex types:\n\n```cpp\nAT_DISPATCH_V2(dtype, \"op\", AT_WRAP([&]() {\n  kernel<scalar_t>();\n}), AT_EXPAND(AT_ALL_TYPES),\n    AT_EXPAND(AT_BAREBONES_UNSIGNED_TYPES),\n    AT_EXPAND(AT_COMPLEX_TYPES),\n    kHalf, kBFloat16);\n```\n\n### Case 3: Already has uint support\n\nCheck if uint types are already present:\n- If `AT_INTEGRAL_TYPES_V2` is used \u2192 already has uint support\n- If `AT_BAREBONES_UNSIGNED_TYPES` is already in list \u2192 already has uint support\n- Skip the file if uint support is already present\n\n## Workflow\n\nWhen asked to add uint support:\n\n1. Read the target file\n2. Check if using AT_DISPATCH_V2:\n   - If not \u2192 use at-dispatch-v2 skill first\n3. Identify all dispatch macro sites\n4. For each dispatch:\n   - Analyze current type groups\n   - Choose method (add BAREBONES_UNSIGNED or upgrade to V2)\n   - Apply transformation with Edit tool\n5. Show the user the changes\n6. Explain what was modified\n\n## Important notes\n\n- Always check if v2 conversion is needed first\n- Apply changes consistently across all dispatch sites in the file\n- Method 2 (AT_INTEGRAL_TYPES_V2) is cleaner when applicable\n- Method 1 (explicit AT_BAREBONES_UNSIGNED_TYPES) is more explicit\n- Unsigned types are: kUInt16, kUInt32, kUInt64 (not kByte which is uint8)\n- Some operators may not semantically support unsigned types - use judgment\n\n## Testing\n\nAfter adding uint support, the operator should accept uint16, uint32, and uint64 tensors. The user is responsible for functional testing.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 12, completeness: 10, error_handling: 7',
  'Score: 81, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-add-uint-support-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'skill-writer - Guide users through creating Agent Skills for Claude Code. Use when the user wants to create, write, author, or design a new Skill, or needs help with',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Skill Writer\n\nThis Skill helps you create well-structured Agent Skills for Claude Code that follow best practices and validation requirements.\n\n## When to use this Skill\n\nUse this Skill when:\n- Creating a new Agent Skill\n- Writing or updating SKILL.md files\n- Designing skill structure and frontmatter\n- Troubleshooting skill discovery issues\n- Converting existing prompts or workflows into Skills\n\n## Instructions\n\n### Step 1: Determine Skill scope\n\nFirst, understand what the Skill should do:\n\n1. **Ask clarifying questions**:\n   - What specific capability should this Skill provide?\n   - When should Claude use this Skill?\n   - What tools or resources does it need?\n   - Is this for personal use or team sharing?\n\n2. **Keep it focused**: One Skill = one capability\n   - Good: \"PDF form filling\", \"Excel data analysis\"\n   - Too broad: \"Document processing\", \"Data tools\"\n\n### Step 2: Choose Skill location\n\nDetermine where to create the Skill:\n\n**Personal Skills** (`~/.claude/skills/`):\n- Individual workflows and preferences\n- Experimental Skills\n- Personal productivity tools\n\n**Project Skills** (`.claude/skills/`):\n- Team workflows and conventions\n- Project-specific expertise\n- Shared utilities (committed to git)\n\n### Step 3: Create Skill structure\n\nCreate the directory and files:\n\n```bash\n# Personal\nmkdir -p ~/.claude/skills/skill-name\n\n# Project\nmkdir -p .claude/skills/skill-name\n```\n\nFor multi-file Skills:\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md (required)\n\u251c\u2500\u2500 reference.md (optional)\n\u251c\u2500\u2500 examples.md (optional)\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 helper.py (optional)\n\u2514\u2500\u2500 templates/\n    \u2514\u2500\u2500 template.txt (optional)\n```\n\n### Step 4: Write SKILL.md frontmatter\n\nCreate YAML frontmatter with required fields:\n\n```yaml\n---\nname: skill-name\ndescription: Brief description of what this does and when to use it\n---\n```\n\n**Field requirements**:\n\n- **name**:\n  - Lowercase letters, numbers, hyphens only\n  - Max 64 characters\n  - Must match directory name\n  - Good: `pdf-processor`, `git-commit-helper`\n  - Bad: `PDF_Processor`, `Git Commits!`\n\n- **description**:\n  - Max 1024 characters\n  - Include BOTH what it does AND when to use it\n  - Use specific trigger words users would say\n  - Mention file types, operations, and context\n\n**Optional frontmatter fields**:\n\n- **allowed-tools**: Restrict tool access (comma-separated list)\n  ```yaml\n  allowed-tools: Read, Grep, Glob\n  ```\n  Use for:\n  - Read-only Skills\n  - Security-sensitive workflows\n  - Limited-scope operations\n\n### Step 5: Write effective descriptions\n\nThe description is critical for Claude to discover your Skill.\n\n**Formula**: `[What it does] + [When to use it] + [Key triggers]`\n\n**Examples**:\n\n\u2705 **Good**:\n```yaml\ndescription: Extract text and tables from PDF files, fill forms, merge documents. Use when working with PDF files or when the user mentions PDFs, forms, or document extraction.\n```\n\n\u2705 **Good**:\n```yaml\ndescription: Analyze Excel spreadsheets, create pivot tables, and generate charts. Use when working with Excel files, spreadsheets, or analyzing tabular data in .xlsx format.\n```\n\n\u274c **Too vague**:\n```yaml\ndescription: Helps with documents\ndescription: For data analysis\n```\n\n**Tips**:\n- Include specific file extensions (.pdf, .xlsx, .json)\n- Mention common user phrases (\"analyze\", \"extract\", \"generate\")\n- List concrete operations (not generic verbs)\n- Add context clues (\"Use when...\", \"For...\")\n\n### Step 6: Structure the Skill content\n\nUse clear Markdown sections:\n\n```markdown\n# Skill Name\n\nBrief overview of what this Skill does.\n\n## Quick start\n\nProvide a simple example to get started immediately.\n\n## Instructions\n\nStep-by-step guidance for Claude:\n1. First step with clear action\n2. Second step with expected outcome\n3. Handle edge cases\n\n## Examples\n\nShow concrete usage examples with code or commands.\n\n## Best practices\n\n- Key conventions to follow\n- Common pitfalls to avoid\n- When to use vs. not use\n\n## Requirements\n\nList any dependencies or prerequisites:\n```bash\npip install package-name\n```\n\n## Advanced usage\n\nFor complex scenarios, see [reference.md](reference.md).\n```\n\n### Step 7: Add supporting files (optional)\n\nCreate additional files for progressive disclosure:\n\n**reference.md**: Detailed API docs, advanced options\n**examples.md**: Extended examples and use cases\n**scripts/**: Helper scripts and utilities\n**templates/**: File templates or boilerplate\n\nReference them from SKILL.md:\n```markdown\nFor advanced usage, see [reference.md](reference.md).\n\nRun the helper script:\n\\`\\`\\`bash\npython scripts/helper.py input.txt\n\\`\\`\\`\n```\n\n### Step 8: Validate the Skill\n\nCheck these requirements:\n\n\u2705 **File structure**:\n- [ ] SKILL.md exists in correct location\n- [ ] Directory name matches frontmatter `name`\n\n\u2705 **YAML frontmatter**:\n- [ ] Opening `---` on line 1\n- [ ] Closing `---` before content\n- [ ] Valid YAML (no tabs, correct indentation)\n- [ ] `name` follows naming rules\n- [ ] `description` is specific and < 1024 chars\n\n\u2705 **Content quality**:\n- [ ] Clear instructions for Claude\n- [ ] Concrete examples provided\n- [ ] Edge cases handled\n- [ ] Dependencies listed (if any)\n\n\u2705 **Testing**:\n- [ ] Description matches user questions\n- [ ] Skill activates on relevant queries\n- [ ] Instructions are clear and actionable\n\n### Step 9: Test the Skill\n\n1. **Restart Claude Code** (if running) to load the Skill\n\n2. **Ask relevant questions** that match the description:\n   ```\n   Can you help me extract text from this PDF?\n   ```\n\n3. **Verify activation**: Claude should use the Skill automatically\n\n4. **Check behavior**: Confirm Claude follows the instructions correctly\n\n### Step 10: Debug if needed\n\nIf Claude doesn''''t use the Skill:\n\n1. **Make description more specific**:\n   - Add trigger words\n   - Include file types\n   - Mention common user phrases\n\n2. **Check file location**:\n   ```bash\n   ls ~/.claude/skills/skill-name/SKILL.md\n   ls .claude/skills/skill-name/SKILL.md\n   ```\n\n3. **Validate YAML**:\n   ```bash\n   cat SKILL.md | head -n 10\n   ```\n\n4. **Run debug mode**:\n   ```bash\n   claude --debug\n   ```\n\n## Common patterns\n\n### Read-only Skill\n\n```yaml\n---\nname: code-reader\ndescription: Read and analyze code without making changes. Use for code review, understanding codebases, or documentation.\nallowed-tools: Read, Grep, Glob\n---\n```\n\n### Script-based Skill\n\n```yaml\n---\nname: data-processor\ndescription: Process CSV and JSON data files with Python scripts. Use when analyzing data files or transforming datasets.\n---\n\n# Data Processor\n\n## Instructions\n\n1. Use the processing script:\n\\`\\`\\`bash\npython scripts/process.py input.csv --output results.json\n\\`\\`\\`\n\n2. Validate output with:\n\\`\\`\\`bash\npython scripts/validate.py results.json\n\\`\\`\\`\n```\n\n### Multi-file Skill with progressive disclosure\n\n```yaml\n---\nname: api-designer\ndescription: Design REST APIs following best practices. Use when creating API endpoints, designing routes, or planning API architecture.\n---\n\n# API Designer\n\nQuick start: See [examples.md](examples.md)\n\nDetailed reference: See [reference.md](reference.md)\n\n## Instructions\n\n1. Gather requirements\n2. Design endpoints (see examples.md)\n3. Document with OpenAPI spec\n4. Review against best practices (see reference.md)\n```\n\n## Best practices for Skill authors\n\n1. **One Skill, one purpose**: Don''''t create mega-Skills\n2. **Specific descriptions**: Include trigger words users will say\n3. **Clear instructions**: Write for Claude, not humans\n4. **Concrete examples**: Show real code, not pseudocode\n5. **List dependencies**: Mention required packages in description\n6. **Test with teammates**: Verify activation and clarity\n7. **Version your Skills**: Document changes in content\n8. **Use progressive disclosure**: Put advanced details in separate files\n\n## Validation checklist\n\nBefore finalizing a Skill, verify:\n\n- [ ] Name is lowercase, hyphens only, max 64 chars\n- [ ] Description is specific and < 1024 chars\n- [ ] Description includes \"what\" and \"when\"\n- [ ] YAML frontmatter is valid\n- [ ] Instructions are step-by-step\n- [ ] Examples are concrete and realistic\n- [ ] Dependencies are documented\n- [ ] File paths use forward slashes\n- [ ] Skill activates on relevant queries\n- [ ] Claude follows instructions correctly\n\n## Troubleshooting\n\n**Skill doesn''''t activate**:\n- Make description more specific with trigger words\n- Include file types and operations in description\n- Add \"Use when...\" clause with user phrases\n\n**Multiple Skills conflict**:\n- Make descriptions more distinct\n- Use different trigger words\n- Narrow the scope of each Skill\n\n**Skill has errors**:\n- Check YAML syntax (no tabs, proper indentation)\n- Verify file paths (use forward slashes)\n- Ensure scripts have execute permissions\n- List all dependencies\n\n## Examples\n\nSee the documentation for complete examples:\n- Simple single-file Skill (commit-helper)\n- Skill with tool permissions (code-reviewer)\n- Multi-file Skill (pdf-processing)\n\n## Output format\n\nWhen creating a Skill, I will:\n\n1. Ask clarifying questions about scope and requirements\n2. Suggest a Skill name and location\n3. Create the SKILL.md file with proper frontmatter\n4. Include clear instructions and examples\n5. Add supporting files if needed\n6. Provide testing instructions\n7. Validate against all requirements\n\nThe result will be a complete, working Skill that follows all best practices and validation rules.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 10',
  'Score: 95, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-skill-writer-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'docstring - Write docstrings for PyTorch functions and methods following PyTorch conventions. Use when writing or updating docstrings in PyTorch code.',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# PyTorch Docstring Writing Guide\n\nThis skill describes how to write docstrings for functions and methods in the PyTorch project, following the conventions in `torch/_tensor_docs.py` and `torch/nn/functional.py`.\n\n## General Principles\n\n- Use **raw strings** (`r\"\"\"...\"\"\"`) for all docstrings to avoid issues with LaTeX/math backslashes\n- Follow **Sphinx/reStructuredText** (reST) format for documentation\n- Be **concise but complete** - include all essential information\n- Always include **examples** when possible\n- Use **cross-references** to related functions/classes\n\n## Docstring Structure\n\n### 1. Function Signature (First Line)\n\nStart with the function signature showing all parameters:\n\n```python\nr\"\"\"function_name(param1, param2, *, kwarg1=default1, kwarg2=default2) -> ReturnType\n```\n\n**Notes:**\n- Include the function name\n- Show positional and keyword-only arguments (use `*` separator)\n- Include default values\n- Show return type annotation\n- This line should NOT end with a period\n\n### 2. Brief Description\n\nProvide a one-line description of what the function does:\n\n```python\nr\"\"\"conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 2D convolution over an input image composed of several input\nplanes.\n```\n\n### 3. Mathematical Formulas (if applicable)\n\nUse Sphinx math directives for mathematical expressions:\n\n```python\n.. math::\n    \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}\n```\n\nOr inline math: `:math:\\`x^2\\``\n\n### 4. Cross-References\n\nLink to related classes and functions using Sphinx roles:\n\n- `:class:\\`~torch.nn.ModuleName\\`` - Link to a class\n- `:func:\\`torch.function_name\\`` - Link to a function\n- `:meth:\\`~Tensor.method_name\\`` - Link to a method\n- `:attr:\\`attribute_name\\`` - Reference an attribute\n- The `~` prefix shows only the last component (e.g., `Conv2d` instead of `torch.nn.Conv2d`)\n\n**Example:**\n```python\nSee :class:`~torch.nn.Conv2d` for details and output shape.\n```\n\n### 5. Notes and Warnings\n\nUse admonitions for important information:\n\n```python\n.. note::\n    This function doesn''''t work directly with NLLLoss,\n    which expects the Log to be computed between the Softmax and itself.\n    Use log_softmax instead (it''''s faster and has better numerical properties).\n\n.. warning::\n    :func:`new_tensor` always copies :attr:`data`. If you have a Tensor\n    ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n    or :func:`torch.Tensor.detach`.\n```\n\n### 6. Args Section\n\nDocument all parameters with type annotations and descriptions:\n\n```python\nArgs:\n    input (Tensor): input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n    weight (Tensor): filters of shape :math:`(\\text{out\\_channels} , kH , kW)`\n    bias (Tensor, optional): optional bias tensor of shape :math:`(\\text{out\\_channels})`. Default: ``None``\n    stride (int or tuple): the stride of the convolving kernel. Can be a single number or a\n      tuple `(sH, sW)`. Default: 1\n```\n\n**Formatting rules:**\n- Parameter name in **lowercase**\n- Type in parentheses: `(Type)`, `(Type, optional)` for optional parameters\n- Description follows the type\n- For optional parameters, include \"Default: ``value``\" at the end\n- Use double backticks for inline code: ``` ``None`` ```\n- Indent continuation lines by 2 spaces\n\n### 7. Keyword Args Section (if applicable)\n\nSometimes keyword arguments are documented separately:\n\n```python\nKeyword args:\n    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n        Default: if None, same :class:`torch.dtype` as this tensor.\n    device (:class:`torch.device`, optional): the desired device of returned tensor.\n        Default: if None, same :class:`torch.device` as this tensor.\n    requires_grad (bool, optional): If autograd should record operations on the\n        returned tensor. Default: ``False``.\n```\n\n### 8. Returns Section (if needed)\n\nDocument the return value:\n\n```python\nReturns:\n    Tensor: Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n        If ``hard=True``, the returned samples will be one-hot, otherwise they will\n        be probability distributions that sum to 1 across `dim`.\n```\n\nOr simply include it in the function signature line if obvious from context.\n\n### 9. Examples Section\n\nAlways include examples when possible:\n\n```python\nExamples::\n\n    >>> inputs = torch.randn(33, 16, 30)\n    >>> filters = torch.randn(20, 16, 5)\n    >>> F.conv1d(inputs, filters)\n\n    >>> # With square kernels and equal stride\n    >>> filters = torch.randn(8, 4, 3, 3)\n    >>> inputs = torch.randn(1, 4, 5, 5)\n    >>> F.conv2d(inputs, filters, padding=1)\n```\n\n**Formatting rules:**\n- Use `Examples::` with double colon\n- Use `>>>` prompt for Python code\n- Include comments with `#` when helpful\n- Show actual output when it helps understanding (indent without `>>>`)\n\n### 10. External References\n\nLink to papers or external documentation:\n\n```python\n.. _Link Name:\n    https://arxiv.org/abs/1611.00712\n```\n\nReference them in text: ```See `Link Name`_```\n\n## Method Types\n\n### Native Python Functions\n\nFor regular Python functions, use a standard docstring:\n\n```python\ndef relu(input: Tensor, inplace: bool = False) -> Tensor:\n    r\"\"\"relu(input, inplace=False) -> Tensor\n\n    Applies the rectified linear unit function element-wise. See\n    :class:`~torch.nn.ReLU` for more details.\n    \"\"\"\n    # implementation\n```\n\n### C-Bound Functions (using add_docstr)\n\nFor C-bound functions, use `_add_docstr`:\n\n```python\nconv1d = _add_docstr(\n    torch.conv1d,\n    r\"\"\"\nconv1d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1) -> Tensor\n\nApplies a 1D convolution over an input signal composed of several input\nplanes.\n\nSee :class:`~torch.nn.Conv1d` for details and output shape.\n\nArgs:\n    input: input tensor of shape :math:`(\\text{minibatch} , \\text{in\\_channels} , iW)`\n    weight: filters of shape :math:`(\\text{out\\_channels} , kW)`\n    ...\n\"\"\",\n)\n```\n\n### In-Place Variants\n\nFor in-place operations (ending with `_`), reference the original:\n\n```python\nadd_docstr_all(\n    \"abs_\",\n    r\"\"\"\nabs_() -> Tensor\n\nIn-place version of :meth:`~Tensor.abs`\n\"\"\",\n)\n```\n\n### Alias Functions\n\nFor aliases, simply reference the original:\n\n```python\nadd_docstr_all(\n    \"absolute\",\n    r\"\"\"\nabsolute() -> Tensor\n\nAlias for :func:`abs`\n\"\"\",\n)\n```\n\n## Common Patterns\n\n### Shape Documentation\n\nUse LaTeX math notation for tensor shapes:\n\n```python\n:math:`(\\text{minibatch} , \\text{in\\_channels} , iH , iW)`\n```\n\n### Reusable Argument Definitions\n\nFor commonly used arguments, define them once and reuse:\n\n```python\ncommon_args = parse_kwargs(\n    \"\"\"\n    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.\n        Default: if None, same as this tensor.\n\"\"\"\n)\n\n# Then use with .format():\nr\"\"\"\n...\n\nKeyword args:\n    {dtype}\n    {device}\n\"\"\".format(**common_args)\n```\n\n### Template Insertion\n\nInsert reproducibility notes or other common text:\n\n```python\nr\"\"\"\n{tf32_note}\n\n{cudnn_reproducibility_note}\n\"\"\".format(**reproducibility_notes, **tf32_notes)\n```\n\n## Complete Example\n\nHere''''s a complete example showing all elements:\n\n```python\ndef gumbel_softmax(\n    logits: Tensor,\n    tau: float = 1,\n    hard: bool = False,\n    eps: float = 1e-10,\n    dim: int = -1,\n) -> Tensor:\n    r\"\"\"\n    Sample from the Gumbel-Softmax distribution and optionally discretize.\n\n    Args:\n        logits (Tensor): `[..., num_features]` unnormalized log probabilities\n        tau (float): non-negative scalar temperature\n        hard (bool): if ``True``, the returned samples will be discretized as one-hot vectors,\n              but will be differentiated as if it is the soft sample in autograd. Default: ``False``\n        dim (int): A dimension along which softmax will be computed. Default: -1\n\n    Returns:\n        Tensor: Sampled tensor of same shape as `logits` from the Gumbel-Softmax distribution.\n            If ``hard=True``, the returned samples will be one-hot, otherwise they will\n            be probability distributions that sum to 1 across `dim`.\n\n    .. note::\n        This function is here for legacy reasons, may be removed from nn.Functional in the future.\n\n    Examples::\n        >>> logits = torch.randn(20, 32)\n        >>> # Sample soft categorical using reparametrization trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=False)\n        >>> # Sample hard categorical using \"Straight-through\" trick:\n        >>> F.gumbel_softmax(logits, tau=1, hard=True)\n\n    .. _Link 1:\n        https://arxiv.org/abs/1611.00712\n    \"\"\"\n    # implementation\n```\n\n## Quick Checklist\n\nWhen writing a PyTorch docstring, ensure:\n\n- [ ] Use raw string (`r\"\"\"`)\n- [ ] Include function signature on first line\n- [ ] Provide brief description\n- [ ] Document all parameters in Args section with types\n- [ ] Include default values for optional parameters\n- [ ] Use Sphinx cross-references (`:func:`, `:class:`, `:meth:`)\n- [ ] Add mathematical formulas if applicable\n- [ ] Include at least one example in Examples section\n- [ ] Add warnings/notes for important caveats\n- [ ] Link to related module class with `:class:`\n- [ ] Use proper math notation for tensor shapes\n- [ ] Follow consistent formatting and indentation\n\n## Common Sphinx Roles Reference\n\n- `:class:\\`~torch.nn.Module\\`` - Class reference\n- `:func:\\`torch.function\\`` - Function reference\n- `:meth:\\`~Tensor.method\\`` - Method reference\n- `:attr:\\`attribute\\`` - Attribute reference\n- `:math:\\`equation\\`` - Inline math\n- `:ref:\\`label\\`` - Internal reference\n- ``` ``code`` ``` - Inline code (use double backticks)\n\n## Additional Notes\n\n- **Indentation**: Use 4 spaces for code, 2 spaces for continuation of parameter descriptions\n- **Line length**: Try to keep lines under 100 characters when possible\n- **Periods**: End sentences with periods, but not the signature line\n- **Backticks**: Use double backticks for code: ``` ``True`` ``None`` ``False`` ```\n- **Types**: Common types are `Tensor`, `int`, `float`, `bool`, `str`, `tuple`, `list`, etc.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 12, workflow_structure: 10, error_handling: 6',
  'Score: 78, Tier: TIER_1_EXCELLENT, Strengths: example_quality: 20, problem_definition: 15, completeness: 15',
  'https://skillsmp.com/skills/pytorch-pytorch-claude-skills-docstring-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'frontend-design - Create distinctive, production-grade frontend interfaces with high design quality. Use this skill when the user asks to build web components, pages, o',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\nThis skill guides creation of distinctive, production-grade frontend interfaces that avoid generic \"AI slop\" aesthetics. Implement real working code with exceptional attention to aesthetic details and creative choices.\n\nThe user provides frontend requirements: a component, page, application, or interface to build. They may include context about the purpose, audience, or technical constraints.\n\n## Design Thinking\n\nBefore coding, understand the context and commit to a BOLD aesthetic direction:\n- **Purpose**: What problem does this interface solve? Who uses it?\n- **Tone**: Pick an extreme: brutally minimal, maximalist chaos, retro-futuristic, organic/natural, luxury/refined, playful/toy-like, editorial/magazine, brutalist/raw, art deco/geometric, soft/pastel, industrial/utilitarian, etc. There are so many flavors to choose from. Use these for inspiration but design one that is true to the aesthetic direction.\n- **Constraints**: Technical requirements (framework, performance, accessibility).\n- **Differentiation**: What makes this UNFORGETTABLE? What''''s the one thing someone will remember?\n\n**CRITICAL**: Choose a clear conceptual direction and execute it with precision. Bold maximalism and refined minimalism both work - the key is intentionality, not intensity.\n\nThen implement working code (HTML/CSS/JS, React, Vue, etc.) that is:\n- Production-grade and functional\n- Visually striking and memorable\n- Cohesive with a clear aesthetic point-of-view\n- Meticulously refined in every detail\n\n## Frontend Aesthetics Guidelines\n\nFocus on:\n- **Typography**: Choose fonts that are beautiful, unique, and interesting. Avoid generic fonts like Arial and Inter; opt instead for distinctive choices that elevate the frontend''''s aesthetics; unexpected, characterful font choices. Pair a distinctive display font with a refined body font.\n- **Color & Theme**: Commit to a cohesive aesthetic. Use CSS variables for consistency. Dominant colors with sharp accents outperform timid, evenly-distributed palettes.\n- **Motion**: Use animations for effects and micro-interactions. Prioritize CSS-only solutions for HTML. Use Motion library for React when available. Focus on high-impact moments: one well-orchestrated page load with staggered reveals (animation-delay) creates more delight than scattered micro-interactions. Use scroll-triggering and hover states that surprise.\n- **Spatial Composition**: Unexpected layouts. Asymmetry. Overlap. Diagonal flow. Grid-breaking elements. Generous negative space OR controlled density.\n- **Backgrounds & Visual Details**: Create atmosphere and depth rather than defaulting to solid colors. Add contextual effects and textures that match the overall aesthetic. Apply creative forms like gradient meshes, noise textures, geometric patterns, layered transparencies, dramatic shadows, decorative borders, custom cursors, and grain overlays.\n\nNEVER use generic AI-generated aesthetics like overused font families (Inter, Roboto, Arial, system fonts), cliched color schemes (particularly purple gradients on white backgrounds), predictable layouts and component patterns, and cookie-cutter design that lacks context-specific character.\n\nInterpret creatively and make unexpected choices that feel genuinely designed for the context. No design should be the same. Vary between light and dark themes, different fonts, different aesthetics. NEVER converge on common choices (Space Grotesk, for example) across generations.\n\n**IMPORTANT**: Match implementation complexity to the aesthetic vision. Maximalist designs need elaborate code with extensive animations and effects. Minimalist or refined designs need restraint, precision, and careful attention to spacing, typography, and subtle details. Elegance comes from executing the vision well.\n\nRemember: Claude is capable of extraordinary creative work. Don''''t hold back, show what can truly be created when thinking outside the box and committing fully to a distinctive vision."}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'workflow_structure: 5, error_handling: 2, example_quality: 0',
  'Score: 39, Tier: TIER_3_MEDIOCRE, Strengths: problem_definition: 15, completeness: 10, clarity_structure: 7',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-frontend-design-skills-frontend-design-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Hook Development - This skill should be used when the user asks to "create a hook", "add a PreToolUse/PostToolUse/Stop hook", "validate tool use", "implement prompt-base',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Hook Development for Claude Code Plugins\n\n## Overview\n\nHooks are event-driven automation scripts that execute in response to Claude Code events. Use hooks to validate operations, enforce policies, add context, and integrate external tools into workflows.\n\n**Key capabilities:**\n- Validate tool calls before execution (PreToolUse)\n- React to tool results (PostToolUse)\n- Enforce completion standards (Stop, SubagentStop)\n- Load project context (SessionStart)\n- Automate workflows across the development lifecycle\n\n## Hook Types\n\n### Prompt-Based Hooks (Recommended)\n\nUse LLM-driven decision making for context-aware validation:\n\n```json\n{\n  \"type\": \"prompt\",\n  \"prompt\": \"Evaluate if this tool use is appropriate: $TOOL_INPUT\",\n  \"timeout\": 30\n}\n```\n\n**Supported events:** Stop, SubagentStop, UserPromptSubmit, PreToolUse\n\n**Benefits:**\n- Context-aware decisions based on natural language reasoning\n- Flexible evaluation logic without bash scripting\n- Better edge case handling\n- Easier to maintain and extend\n\n### Command Hooks\n\nExecute bash commands for deterministic checks:\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\",\n  \"timeout\": 60\n}\n```\n\n**Use for:**\n- Fast deterministic validations\n- File system operations\n- External tool integrations\n- Performance-critical checks\n\n## Hook Configuration Formats\n\n### Plugin hooks.json Format\n\n**For plugin hooks** in `hooks/hooks.json`, use wrapper format:\n\n```json\n{\n  \"description\": \"Brief explanation of hooks (optional)\",\n  \"hooks\": {\n    \"PreToolUse\": [...],\n    \"Stop\": [...],\n    \"SessionStart\": [...]\n  }\n}\n```\n\n**Key points:**\n- `description` field is optional\n- `hooks` field is required wrapper containing actual hook events\n- This is the **plugin-specific format**\n\n**Example:**\n```json\n{\n  \"description\": \"Validation hooks for code quality\",\n  \"hooks\": {\n    \"PreToolUse\": [\n      {\n        \"matcher\": \"Write\",\n        \"hooks\": [\n          {\n            \"type\": \"command\",\n            \"command\": \"${CLAUDE_PLUGIN_ROOT}/hooks/validate.sh\"\n          }\n        ]\n      }\n    ]\n  }\n}\n```\n\n### Settings Format (Direct)\n\n**For user settings** in `.claude/settings.json`, use direct format:\n\n```json\n{\n  \"PreToolUse\": [...],\n  \"Stop\": [...],\n  \"SessionStart\": [...]\n}\n```\n\n**Key points:**\n- No wrapper - events directly at top level\n- No description field\n- This is the **settings format**\n\n**Important:** The examples below show the hook event structure that goes inside either format. For plugin hooks.json, wrap these in `{\"hooks\": {...}}`.\n\n## Hook Events\n\n### PreToolUse\n\nExecute before any tool runs. Use to approve, deny, or modify tool calls.\n\n**Example (prompt-based):**\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Validate file write safety. Check: system paths, credentials, path traversal, sensitive content. Return ''''approve'''' or ''''deny''''.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Output for PreToolUse:**\n```json\n{\n  \"hookSpecificOutput\": {\n    \"permissionDecision\": \"allow|deny|ask\",\n    \"updatedInput\": {\"field\": \"modified_value\"}\n  },\n  \"systemMessage\": \"Explanation for Claude\"\n}\n```\n\n### PostToolUse\n\nExecute after tool completes. Use to react to results, provide feedback, or log.\n\n**Example:**\n```json\n{\n  \"PostToolUse\": [\n    {\n      \"matcher\": \"Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Analyze edit result for potential issues: syntax errors, security vulnerabilities, breaking changes. Provide feedback.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Output behavior:**\n- Exit 0: stdout shown in transcript\n- Exit 2: stderr fed back to Claude\n- systemMessage included in context\n\n### Stop\n\nExecute when main agent considers stopping. Use to validate completeness.\n\n**Example:**\n```json\n{\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Verify task completion: tests run, build succeeded, questions answered. Return ''''approve'''' to stop or ''''block'''' with reason to continue.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Decision output:**\n```json\n{\n  \"decision\": \"approve|block\",\n  \"reason\": \"Explanation\",\n  \"systemMessage\": \"Additional context\"\n}\n```\n\n### SubagentStop\n\nExecute when subagent considers stopping. Use to ensure subagent completed its task.\n\nSimilar to Stop hook, but for subagents.\n\n### UserPromptSubmit\n\nExecute when user submits a prompt. Use to add context, validate, or block prompts.\n\n**Example:**\n```json\n{\n  \"UserPromptSubmit\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Check if prompt requires security guidance. If discussing auth, permissions, or API security, return relevant warnings.\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n### SessionStart\n\nExecute when Claude Code session begins. Use to load context and set environment.\n\n**Example:**\n```json\n{\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\"\n        }\n      ]\n    }\n  ]\n}\n```\n\n**Special capability:** Persist environment variables using `$CLAUDE_ENV_FILE`:\n```bash\necho \"export PROJECT_TYPE=nodejs\" >> \"$CLAUDE_ENV_FILE\"\n```\n\nSee `examples/load-context.sh` for complete example.\n\n### SessionEnd\n\nExecute when session ends. Use for cleanup, logging, and state preservation.\n\n### PreCompact\n\nExecute before context compaction. Use to add critical information to preserve.\n\n### Notification\n\nExecute when Claude sends notifications. Use to react to user notifications.\n\n## Hook Output Format\n\n### Standard Output (All Hooks)\n\n```json\n{\n  \"continue\": true,\n  \"suppressOutput\": false,\n  \"systemMessage\": \"Message for Claude\"\n}\n```\n\n- `continue`: If false, halt processing (default true)\n- `suppressOutput`: Hide output from transcript (default false)\n- `systemMessage`: Message shown to Claude\n\n### Exit Codes\n\n- `0` - Success (stdout shown in transcript)\n- `2` - Blocking error (stderr fed back to Claude)\n- Other - Non-blocking error\n\n## Hook Input Format\n\nAll hooks receive JSON via stdin with common fields:\n\n```json\n{\n  \"session_id\": \"abc123\",\n  \"transcript_path\": \"/path/to/transcript.txt\",\n  \"cwd\": \"/current/working/dir\",\n  \"permission_mode\": \"ask|allow\",\n  \"hook_event_name\": \"PreToolUse\"\n}\n```\n\n**Event-specific fields:**\n\n- **PreToolUse/PostToolUse:** `tool_name`, `tool_input`, `tool_result`\n- **UserPromptSubmit:** `user_prompt`\n- **Stop/SubagentStop:** `reason`\n\nAccess fields in prompts using `$TOOL_INPUT`, `$TOOL_RESULT`, `$USER_PROMPT`, etc.\n\n## Environment Variables\n\nAvailable in all command hooks:\n\n- `$CLAUDE_PROJECT_DIR` - Project root path\n- `$CLAUDE_PLUGIN_ROOT` - Plugin directory (use for portable paths)\n- `$CLAUDE_ENV_FILE` - SessionStart only: persist env vars here\n- `$CLAUDE_CODE_REMOTE` - Set if running in remote context\n\n**Always use ${CLAUDE_PLUGIN_ROOT} in hook commands for portability:**\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\"\n}\n```\n\n## Plugin Hook Configuration\n\nIn plugins, define hooks in `hooks/hooks.json`:\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write|Edit\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Validate file write safety\"\n        }\n      ]\n    }\n  ],\n  \"Stop\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"prompt\",\n          \"prompt\": \"Verify task completion\"\n        }\n      ]\n    }\n  ],\n  \"SessionStart\": [\n    {\n      \"matcher\": \"*\",\n      \"hooks\": [\n        {\n          \"type\": \"command\",\n          \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/load-context.sh\",\n          \"timeout\": 10\n        }\n      ]\n    }\n  ]\n}\n```\n\nPlugin hooks merge with user''''s hooks and run in parallel.\n\n## Matchers\n\n### Tool Name Matching\n\n**Exact match:**\n```json\n\"matcher\": \"Write\"\n```\n\n**Multiple tools:**\n```json\n\"matcher\": \"Read|Write|Edit\"\n```\n\n**Wildcard (all tools):**\n```json\n\"matcher\": \"*\"\n```\n\n**Regex patterns:**\n```json\n\"matcher\": \"mcp__.*__delete.*\"  // All MCP delete tools\n```\n\n**Note:** Matchers are case-sensitive.\n\n### Common Patterns\n\n```json\n// All MCP tools\n\"matcher\": \"mcp__.*\"\n\n// Specific plugin''''s MCP tools\n\"matcher\": \"mcp__plugin_asana_.*\"\n\n// All file operations\n\"matcher\": \"Read|Write|Edit\"\n\n// Bash commands only\n\"matcher\": \"Bash\"\n```\n\n## Security Best Practices\n\n### Input Validation\n\nAlways validate inputs in command hooks:\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\ninput=$(cat)\ntool_name=$(echo \"$input\" | jq -r ''''.tool_name'''')\n\n# Validate tool name format\nif [[ ! \"$tool_name\" =~ ^[a-zA-Z0-9_]+$ ]]; then\n  echo ''''{\"decision\": \"deny\", \"reason\": \"Invalid tool name\"}'''' >&2\n  exit 2\nfi\n```\n\n### Path Safety\n\nCheck for path traversal and sensitive files:\n\n```bash\nfile_path=$(echo \"$input\" | jq -r ''''.tool_input.file_path'''')\n\n# Deny path traversal\nif [[ \"$file_path\" == *\"..\"* ]]; then\n  echo ''''{\"decision\": \"deny\", \"reason\": \"Path traversal detected\"}'''' >&2\n  exit 2\nfi\n\n# Deny sensitive files\nif [[ \"$file_path\" == *\".env\"* ]]; then\n  echo ''''{\"decision\": \"deny\", \"reason\": \"Sensitive file\"}'''' >&2\n  exit 2\nfi\n```\n\nSee `examples/validate-write.sh` and `examples/validate-bash.sh` for complete examples.\n\n### Quote All Variables\n\n```bash\n# GOOD: Quoted\necho \"$file_path\"\ncd \"$CLAUDE_PROJECT_DIR\"\n\n# BAD: Unquoted (injection risk)\necho $file_path\ncd $CLAUDE_PROJECT_DIR\n```\n\n### Set Appropriate Timeouts\n\n```json\n{\n  \"type\": \"command\",\n  \"command\": \"bash script.sh\",\n  \"timeout\": 10\n}\n```\n\n**Defaults:** Command hooks (60s), Prompt hooks (30s)\n\n## Performance Considerations\n\n### Parallel Execution\n\nAll matching hooks run **in parallel**:\n\n```json\n{\n  \"PreToolUse\": [\n    {\n      \"matcher\": \"Write\",\n      \"hooks\": [\n        {\"type\": \"command\", \"command\": \"check1.sh\"},  // Parallel\n        {\"type\": \"command\", \"command\": \"check2.sh\"},  // Parallel\n        {\"type\": \"prompt\", \"prompt\": \"Validate...\"}   // Parallel\n      ]\n    }\n  ]\n}\n```\n\n**Design implications:**\n- Hooks don''''t see each other''''s output\n- Non-deterministic ordering\n- Design for independence\n\n### Optimization\n\n1. Use command hooks for quick deterministic checks\n2. Use prompt hooks for complex reasoning\n3. Cache validation results in temp files\n4. Minimize I/O in hot paths\n\n## Temporarily Active Hooks\n\nCreate hooks that activate conditionally by checking for a flag file or configuration:\n\n**Pattern: Flag file activation**\n```bash\n#!/bin/bash\n# Only active when flag file exists\nFLAG_FILE=\"$CLAUDE_PROJECT_DIR/.enable-strict-validation\"\n\nif [ ! -f \"$FLAG_FILE\" ]; then\n  # Flag not present, skip validation\n  exit 0\nfi\n\n# Flag present, run validation\ninput=$(cat)\n# ... validation logic ...\n```\n\n**Pattern: Configuration-based activation**\n```bash\n#!/bin/bash\n# Check configuration for activation\nCONFIG_FILE=\"$CLAUDE_PROJECT_DIR/.claude/plugin-config.json\"\n\nif [ -f \"$CONFIG_FILE\" ]; then\n  enabled=$(jq -r ''''.strictMode // false'''' \"$CONFIG_FILE\")\n  if [ \"$enabled\" != \"true\" ]; then\n    exit 0  # Not enabled, skip\n  fi\nfi\n\n# Enabled, run hook logic\ninput=$(cat)\n# ... hook logic ...\n```\n\n**Use cases:**\n- Enable strict validation only when needed\n- Temporary debugging hooks\n- Project-specific hook behavior\n- Feature flags for hooks\n\n**Best practice:** Document activation mechanism in plugin README so users know how to enable/disable temporary hooks.\n\n## Hook Lifecycle and Limitations\n\n### Hooks Load at Session Start\n\n**Important:** Hooks are loaded when Claude Code session starts. Changes to hook configuration require restarting Claude Code.\n\n**Cannot hot-swap hooks:**\n- Editing `hooks/hooks.json` won''''t affect current session\n- Adding new hook scripts won''''t be recognized\n- Changing hook commands/prompts won''''t update\n- Must restart Claude Code: exit and run `claude` again\n\n**To test hook changes:**\n1. Edit hook configuration or scripts\n2. Exit Claude Code session\n3. Restart: `claude` or `cc`\n4. New hook configuration loads\n5. Test hooks with `claude --debug`\n\n### Hook Validation at Startup\n\nHooks are validated when Claude Code starts:\n- Invalid JSON in hooks.json causes loading failure\n- Missing scripts cause warnings\n- Syntax errors reported in debug mode\n\nUse `/hooks` command to review loaded hooks in current session.\n\n## Debugging Hooks\n\n### Enable Debug Mode\n\n```bash\nclaude --debug\n```\n\nLook for hook registration, execution logs, input/output JSON, and timing information.\n\n### Test Hook Scripts\n\nTest command hooks directly:\n\n```bash\necho ''''{\"tool_name\": \"Write\", \"tool_input\": {\"file_path\": \"/test\"}}'''' | \\\n  bash ${CLAUDE_PLUGIN_ROOT}/scripts/validate.sh\n\necho \"Exit code: $?\"\n```\n\n### Validate JSON Output\n\nEnsure hooks output valid JSON:\n\n```bash\noutput=$(./your-hook.sh < test-input.json)\necho \"$output\" | jq .\n```\n\n## Quick Reference\n\n### Hook Events Summary\n\n| Event | When | Use For |\n|-------|------|---------|\n| PreToolUse | Before tool | Validation, modification |\n| PostToolUse | After tool | Feedback, logging |\n| UserPromptSubmit | User input | Context, validation |\n| Stop | Agent stopping | Completeness check |\n| SubagentStop | Subagent done | Task validation |\n| SessionStart | Session begins | Context loading |\n| SessionEnd | Session ends | Cleanup, logging |\n| PreCompact | Before compact | Preserve context |\n| Notification | User notified | Logging, reactions |\n\n### Best Practices\n\n**DO:**\n- \u2705 Use prompt-based hooks for complex logic\n- \u2705 Use ${CLAUDE_PLUGIN_ROOT} for portability\n- \u2705 Validate all inputs in command hooks\n- \u2705 Quote all bash variables\n- \u2705 Set appropriate timeouts\n- \u2705 Return structured JSON output\n- \u2705 Test hooks thoroughly\n\n**DON''''T:**\n- \u274c Use hardcoded paths\n- \u274c Trust user input without validation\n- \u274c Create long-running hooks\n- \u274c Rely on hook execution order\n- \u274c Modify global state unpredictably\n- \u274c Log sensitive information\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and advanced techniques, consult:\n\n- **`references/patterns.md`** - Common hook patterns (8+ proven patterns)\n- **`references/migration.md`** - Migrating from basic to advanced hooks\n- **`references/advanced.md`** - Advanced use cases and techniques\n\n### Example Hook Scripts\n\nWorking examples in `examples/`:\n\n- **`validate-write.sh`** - File write validation example\n- **`validate-bash.sh`** - Bash command validation example\n- **`load-context.sh`** - SessionStart context loading example\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-hook-schema.sh`** - Validate hooks.json structure and syntax\n- **`test-hook.sh`** - Test hooks with sample input before deployment\n- **`hook-linter.sh`** - Check hook scripts for common issues and best practices\n\n### External Resources\n\n- **Official Docs**: https://docs.claude.com/en/docs/claude-code/hooks\n- **Examples**: See security-guidance plugin in marketplace\n- **Testing**: Use `claude --debug` for detailed logs\n- **Validation**: Use `jq` to validate hook JSON output\n\n## Implementation Workflow\n\nTo implement hooks in a plugin:\n\n1. Identify events to hook into (PreToolUse, Stop, SessionStart, etc.)\n2. Decide between prompt-based (flexible) or command (deterministic) hooks\n3. Write hook configuration in `hooks/hooks.json`\n4. For command hooks, create hook scripts\n5. Use ${CLAUDE_PLUGIN_ROOT} for all file references\n6. Validate configuration with `scripts/validate-hook-schema.sh hooks/hooks.json`\n7. Test hooks with `scripts/test-hook.sh` before deployment\n8. Test in Claude Code with `claude --debug`\n9. Document hooks in plugin README\n\nFocus on prompt-based hooks for most use cases. Reserve command hooks for performance-critical or deterministic checks.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 15, clarity_structure: 13, error_handling: 12',
  'Score: 95, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-hook-development-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Command Development - This skill should be used when the user asks to "create a slash command", "add a command", "write a custom command", "define command arguments", "use ',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Command Development for Claude Code\n\n## Overview\n\nSlash commands are frequently-used prompts defined as Markdown files that Claude executes during interactive sessions. Understanding command structure, frontmatter options, and dynamic features enables creating powerful, reusable workflows.\n\n**Key concepts:**\n- Markdown file format for commands\n- YAML frontmatter for configuration\n- Dynamic arguments and file references\n- Bash execution for context\n- Command organization and namespacing\n\n## Command Basics\n\n### What is a Slash Command?\n\nA slash command is a Markdown file containing a prompt that Claude executes when invoked. Commands provide:\n- **Reusability**: Define once, use repeatedly\n- **Consistency**: Standardize common workflows\n- **Sharing**: Distribute across team or projects\n- **Efficiency**: Quick access to complex prompts\n\n### Critical: Commands are Instructions FOR Claude\n\n**Commands are written for agent consumption, not human consumption.**\n\nWhen a user invokes `/command-name`, the command content becomes Claude''''s instructions. Write commands as directives TO Claude about what to do, not as messages TO the user.\n\n**Correct approach (instructions for Claude):**\n```markdown\nReview this code for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication issues\n\nProvide specific line numbers and severity ratings.\n```\n\n**Incorrect approach (messages to user):**\n```markdown\nThis command will review your code for security issues.\nYou''''ll receive a report with vulnerability details.\n```\n\nThe first example tells Claude what to do. The second tells the user what will happen but doesn''''t instruct Claude. Always use the first approach.\n\n### Command Locations\n\n**Project commands** (shared with team):\n- Location: `.claude/commands/`\n- Scope: Available in specific project\n- Label: Shown as \"(project)\" in `/help`\n- Use for: Team workflows, project-specific tasks\n\n**Personal commands** (available everywhere):\n- Location: `~/.claude/commands/`\n- Scope: Available in all projects\n- Label: Shown as \"(user)\" in `/help`\n- Use for: Personal workflows, cross-project utilities\n\n**Plugin commands** (bundled with plugins):\n- Location: `plugin-name/commands/`\n- Scope: Available when plugin installed\n- Label: Shown as \"(plugin-name)\" in `/help`\n- Use for: Plugin-specific functionality\n\n## File Format\n\n### Basic Structure\n\nCommands are Markdown files with `.md` extension:\n\n```\n.claude/commands/\n\u251c\u2500\u2500 review.md           # /review command\n\u251c\u2500\u2500 test.md             # /test command\n\u2514\u2500\u2500 deploy.md           # /deploy command\n```\n\n**Simple command:**\n```markdown\nReview this code for security vulnerabilities including:\n- SQL injection\n- XSS attacks\n- Authentication bypass\n- Insecure data handling\n```\n\nNo frontmatter needed for basic commands.\n\n### With YAML Frontmatter\n\nAdd configuration using YAML frontmatter:\n\n```markdown\n---\ndescription: Review code for security issues\nallowed-tools: Read, Grep, Bash(git:*)\nmodel: sonnet\n---\n\nReview this code for security vulnerabilities...\n```\n\n## YAML Frontmatter Fields\n\n### description\n\n**Purpose:** Brief description shown in `/help`\n**Type:** String\n**Default:** First line of command prompt\n\n```yaml\n---\ndescription: Review pull request for code quality\n---\n```\n\n**Best practice:** Clear, actionable description (under 60 characters)\n\n### allowed-tools\n\n**Purpose:** Specify which tools command can use\n**Type:** String or Array\n**Default:** Inherits from conversation\n\n```yaml\n---\nallowed-tools: Read, Write, Edit, Bash(git:*)\n---\n```\n\n**Patterns:**\n- `Read, Write, Edit` - Specific tools\n- `Bash(git:*)` - Bash with git commands only\n- `*` - All tools (rarely needed)\n\n**Use when:** Command requires specific tool access\n\n### model\n\n**Purpose:** Specify model for command execution\n**Type:** String (sonnet, opus, haiku)\n**Default:** Inherits from conversation\n\n```yaml\n---\nmodel: haiku\n---\n```\n\n**Use cases:**\n- `haiku` - Fast, simple commands\n- `sonnet` - Standard workflows\n- `opus` - Complex analysis\n\n### argument-hint\n\n**Purpose:** Document expected arguments for autocomplete\n**Type:** String\n**Default:** None\n\n```yaml\n---\nargument-hint: [pr-number] [priority] [assignee]\n---\n```\n\n**Benefits:**\n- Helps users understand command arguments\n- Improves command discovery\n- Documents command interface\n\n### disable-model-invocation\n\n**Purpose:** Prevent SlashCommand tool from programmatically calling command\n**Type:** Boolean\n**Default:** false\n\n```yaml\n---\ndisable-model-invocation: true\n---\n```\n\n**Use when:** Command should only be manually invoked\n\n## Dynamic Arguments\n\n### Using $ARGUMENTS\n\nCapture all arguments as single string:\n\n```markdown\n---\ndescription: Fix issue by number\nargument-hint: [issue-number]\n---\n\nFix issue #$ARGUMENTS following our coding standards and best practices.\n```\n\n**Usage:**\n```\n> /fix-issue 123\n> /fix-issue 456\n```\n\n**Expands to:**\n```\nFix issue #123 following our coding standards...\nFix issue #456 following our coding standards...\n```\n\n### Using Positional Arguments\n\nCapture individual arguments with `$1`, `$2`, `$3`, etc.:\n\n```markdown\n---\ndescription: Review PR with priority and assignee\nargument-hint: [pr-number] [priority] [assignee]\n---\n\nReview pull request #$1 with priority level $2.\nAfter review, assign to $3 for follow-up.\n```\n\n**Usage:**\n```\n> /review-pr 123 high alice\n```\n\n**Expands to:**\n```\nReview pull request #123 with priority level high.\nAfter review, assign to alice for follow-up.\n```\n\n### Combining Arguments\n\nMix positional and remaining arguments:\n\n```markdown\nDeploy $1 to $2 environment with options: $3\n```\n\n**Usage:**\n```\n> /deploy api staging --force --skip-tests\n```\n\n**Expands to:**\n```\nDeploy api to staging environment with options: --force --skip-tests\n```\n\n## File References\n\n### Using @ Syntax\n\nInclude file contents in command:\n\n```markdown\n---\ndescription: Review specific file\nargument-hint: [file-path]\n---\n\nReview @$1 for:\n- Code quality\n- Best practices\n- Potential bugs\n```\n\n**Usage:**\n```\n> /review-file src/api/users.ts\n```\n\n**Effect:** Claude reads `src/api/users.ts` before processing command\n\n### Multiple File References\n\nReference multiple files:\n\n```markdown\nCompare @src/old-version.js with @src/new-version.js\n\nIdentify:\n- Breaking changes\n- New features\n- Bug fixes\n```\n\n### Static File References\n\nReference known files without arguments:\n\n```markdown\nReview @package.json and @tsconfig.json for consistency\n\nEnsure:\n- TypeScript version matches\n- Dependencies are aligned\n- Build configuration is correct\n```\n\n## Bash Execution in Commands\n\nCommands can execute bash commands inline to dynamically gather context before Claude processes the command. This is useful for including repository state, environment information, or project-specific context.\n\n**When to use:**\n- Include dynamic context (git status, environment vars, etc.)\n- Gather project/repository state\n- Build context-aware workflows\n\n**Implementation details:**\nFor complete syntax, examples, and best practices, see `references/plugin-features-reference.md` section on bash execution. The reference includes the exact syntax and multiple working examples to avoid execution issues\n\n## Command Organization\n\n### Flat Structure\n\nSimple organization for small command sets:\n\n```\n.claude/commands/\n\u251c\u2500\u2500 build.md\n\u251c\u2500\u2500 test.md\n\u251c\u2500\u2500 deploy.md\n\u251c\u2500\u2500 review.md\n\u2514\u2500\u2500 docs.md\n```\n\n**Use when:** 5-15 commands, no clear categories\n\n### Namespaced Structure\n\nOrganize commands in subdirectories:\n\n```\n.claude/commands/\n\u251c\u2500\u2500 ci/\n\u2502   \u251c\u2500\u2500 build.md        # /build (project:ci)\n\u2502   \u251c\u2500\u2500 test.md         # /test (project:ci)\n\u2502   \u2514\u2500\u2500 lint.md         # /lint (project:ci)\n\u251c\u2500\u2500 git/\n\u2502   \u251c\u2500\u2500 commit.md       # /commit (project:git)\n\u2502   \u2514\u2500\u2500 pr.md           # /pr (project:git)\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 generate.md     # /generate (project:docs)\n    \u2514\u2500\u2500 publish.md      # /publish (project:docs)\n```\n\n**Benefits:**\n- Logical grouping by category\n- Namespace shown in `/help`\n- Easier to find related commands\n\n**Use when:** 15+ commands, clear categories\n\n## Best Practices\n\n### Command Design\n\n1. **Single responsibility:** One command, one task\n2. **Clear descriptions:** Self-explanatory in `/help`\n3. **Explicit dependencies:** Use `allowed-tools` when needed\n4. **Document arguments:** Always provide `argument-hint`\n5. **Consistent naming:** Use verb-noun pattern (review-pr, fix-issue)\n\n### Argument Handling\n\n1. **Validate arguments:** Check for required arguments in prompt\n2. **Provide defaults:** Suggest defaults when arguments missing\n3. **Document format:** Explain expected argument format\n4. **Handle edge cases:** Consider missing or invalid arguments\n\n```markdown\n---\nargument-hint: [pr-number]\n---\n\n$IF($1,\n  Review PR #$1,\n  Please provide a PR number. Usage: /review-pr [number]\n)\n```\n\n### File References\n\n1. **Explicit paths:** Use clear file paths\n2. **Check existence:** Handle missing files gracefully\n3. **Relative paths:** Use project-relative paths\n4. **Glob support:** Consider using Glob tool for patterns\n\n### Bash Commands\n\n1. **Limit scope:** Use `Bash(git:*)` not `Bash(*)`\n2. **Safe commands:** Avoid destructive operations\n3. **Handle errors:** Consider command failures\n4. **Keep fast:** Long-running commands slow invocation\n\n### Documentation\n\n1. **Add comments:** Explain complex logic\n2. **Provide examples:** Show usage in comments\n3. **List requirements:** Document dependencies\n4. **Version commands:** Note breaking changes\n\n```markdown\n---\ndescription: Deploy application to environment\nargument-hint: [environment] [version]\n---\n\n<!--\nUsage: /deploy [staging|production] [version]\nRequires: AWS credentials configured\nExample: /deploy staging v1.2.3\n-->\n\nDeploy application to $1 environment using version $2...\n```\n\n## Common Patterns\n\n### Review Pattern\n\n```markdown\n---\ndescription: Review code changes\nallowed-tools: Read, Bash(git:*)\n---\n\nFiles changed: !`git diff --name-only`\n\nReview each file for:\n1. Code quality and style\n2. Potential bugs or issues\n3. Test coverage\n4. Documentation needs\n\nProvide specific feedback for each file.\n```\n\n### Testing Pattern\n\n```markdown\n---\ndescription: Run tests for specific file\nargument-hint: [test-file]\nallowed-tools: Bash(npm:*)\n---\n\nRun tests: !`npm test $1`\n\nAnalyze results and suggest fixes for failures.\n```\n\n### Documentation Pattern\n\n```markdown\n---\ndescription: Generate documentation for file\nargument-hint: [source-file]\n---\n\nGenerate comprehensive documentation for @$1 including:\n- Function/class descriptions\n- Parameter documentation\n- Return value descriptions\n- Usage examples\n- Edge cases and errors\n```\n\n### Workflow Pattern\n\n```markdown\n---\ndescription: Complete PR workflow\nargument-hint: [pr-number]\nallowed-tools: Bash(gh:*), Read\n---\n\nPR #$1 Workflow:\n\n1. Fetch PR: !`gh pr view $1`\n2. Review changes\n3. Run checks\n4. Approve or request changes\n```\n\n## Troubleshooting\n\n**Command not appearing:**\n- Check file is in correct directory\n- Verify `.md` extension present\n- Ensure valid Markdown format\n- Restart Claude Code\n\n**Arguments not working:**\n- Verify `$1`, `$2` syntax correct\n- Check `argument-hint` matches usage\n- Ensure no extra spaces\n\n**Bash execution failing:**\n- Check `allowed-tools` includes Bash\n- Verify command syntax in backticks\n- Test command in terminal first\n- Check for required permissions\n\n**File references not working:**\n- Verify `@` syntax correct\n- Check file path is valid\n- Ensure Read tool allowed\n- Use absolute or project-relative paths\n\n## Plugin-Specific Features\n\n### CLAUDE_PLUGIN_ROOT Variable\n\nPlugin commands have access to `${CLAUDE_PLUGIN_ROOT}`, an environment variable that resolves to the plugin''''s absolute path.\n\n**Purpose:**\n- Reference plugin files portably\n- Execute plugin scripts\n- Load plugin configuration\n- Access plugin templates\n\n**Basic usage:**\n\n```markdown\n---\ndescription: Analyze using plugin script\nallowed-tools: Bash(node:*)\n---\n\nRun analysis: !`node ${CLAUDE_PLUGIN_ROOT}/scripts/analyze.js $1`\n\nReview results and report findings.\n```\n\n**Common patterns:**\n\n```markdown\n# Execute plugin script\n!`bash ${CLAUDE_PLUGIN_ROOT}/scripts/script.sh`\n\n# Load plugin configuration\n@${CLAUDE_PLUGIN_ROOT}/config/settings.json\n\n# Use plugin template\n@${CLAUDE_PLUGIN_ROOT}/templates/report.md\n\n# Access plugin resources\n@${CLAUDE_PLUGIN_ROOT}/docs/reference.md\n```\n\n**Why use it:**\n- Works across all installations\n- Portable between systems\n- No hardcoded paths needed\n- Essential for multi-file plugins\n\n### Plugin Command Organization\n\nPlugin commands discovered automatically from `commands/` directory:\n\n```\nplugin-name/\n\u251c\u2500\u2500 commands/\n\u2502   \u251c\u2500\u2500 foo.md              # /foo (plugin:plugin-name)\n\u2502   \u251c\u2500\u2500 bar.md              # /bar (plugin:plugin-name)\n\u2502   \u2514\u2500\u2500 utils/\n\u2502       \u2514\u2500\u2500 helper.md       # /helper (plugin:plugin-name:utils)\n\u2514\u2500\u2500 plugin.json\n```\n\n**Namespace benefits:**\n- Logical command grouping\n- Shown in `/help` output\n- Avoid name conflicts\n- Organize related commands\n\n**Naming conventions:**\n- Use descriptive action names\n- Avoid generic names (test, run)\n- Consider plugin-specific prefix\n- Use hyphens for multi-word names\n\n### Plugin Command Patterns\n\n**Configuration-based pattern:**\n\n```markdown\n---\ndescription: Deploy using plugin configuration\nargument-hint: [environment]\nallowed-tools: Read, Bash(*)\n---\n\nLoad configuration: @${CLAUDE_PLUGIN_ROOT}/config/$1-deploy.json\n\nDeploy to $1 using configuration settings.\nMonitor deployment and report status.\n```\n\n**Template-based pattern:**\n\n```markdown\n---\ndescription: Generate docs from template\nargument-hint: [component]\n---\n\nTemplate: @${CLAUDE_PLUGIN_ROOT}/templates/docs.md\n\nGenerate documentation for $1 following template structure.\n```\n\n**Multi-script pattern:**\n\n```markdown\n---\ndescription: Complete build workflow\nallowed-tools: Bash(*)\n---\n\nBuild: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/build.sh`\nTest: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/test.sh`\nPackage: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/package.sh`\n\nReview outputs and report workflow status.\n```\n\n**See `references/plugin-features-reference.md` for detailed patterns.**\n\n## Integration with Plugin Components\n\nCommands can integrate with other plugin components for powerful workflows.\n\n### Agent Integration\n\nLaunch plugin agents for complex tasks:\n\n```markdown\n---\ndescription: Deep code review\nargument-hint: [file-path]\n---\n\nInitiate comprehensive review of @$1 using the code-reviewer agent.\n\nThe agent will analyze:\n- Code structure\n- Security issues\n- Performance\n- Best practices\n\nAgent uses plugin resources:\n- ${CLAUDE_PLUGIN_ROOT}/config/rules.json\n- ${CLAUDE_PLUGIN_ROOT}/checklists/review.md\n```\n\n**Key points:**\n- Agent must exist in `plugin/agents/` directory\n- Claude uses Task tool to launch agent\n- Document agent capabilities\n- Reference plugin resources agent uses\n\n### Skill Integration\n\nLeverage plugin skills for specialized knowledge:\n\n```markdown\n---\ndescription: Document API with standards\nargument-hint: [api-file]\n---\n\nDocument API in @$1 following plugin standards.\n\nUse the api-docs-standards skill to ensure:\n- Complete endpoint documentation\n- Consistent formatting\n- Example quality\n- Error documentation\n\nGenerate production-ready API docs.\n```\n\n**Key points:**\n- Skill must exist in `plugin/skills/` directory\n- Mention skill name to trigger invocation\n- Document skill purpose\n- Explain what skill provides\n\n### Hook Coordination\n\nDesign commands that work with plugin hooks:\n- Commands can prepare state for hooks to process\n- Hooks execute automatically on tool events\n- Commands should document expected hook behavior\n- Guide Claude on interpreting hook output\n\nSee `references/plugin-features-reference.md` for examples of commands that coordinate with hooks\n\n### Multi-Component Workflows\n\nCombine agents, skills, and scripts:\n\n```markdown\n---\ndescription: Comprehensive review workflow\nargument-hint: [file]\nallowed-tools: Bash(node:*), Read\n---\n\nTarget: @$1\n\nPhase 1 - Static Analysis:\n!`node ${CLAUDE_PLUGIN_ROOT}/scripts/lint.js $1`\n\nPhase 2 - Deep Review:\nLaunch code-reviewer agent for detailed analysis.\n\nPhase 3 - Standards Check:\nUse coding-standards skill for validation.\n\nPhase 4 - Report:\nTemplate: @${CLAUDE_PLUGIN_ROOT}/templates/review.md\n\nCompile findings into report following template.\n```\n\n**When to use:**\n- Complex multi-step workflows\n- Leverage multiple plugin capabilities\n- Require specialized analysis\n- Need structured outputs\n\n## Validation Patterns\n\nCommands should validate inputs and resources before processing.\n\n### Argument Validation\n\n```markdown\n---\ndescription: Deploy with validation\nargument-hint: [environment]\n---\n\nValidate environment: !`echo \"$1\" | grep -E \"^(dev|staging|prod)$\" || echo \"INVALID\"`\n\nIf $1 is valid environment:\n  Deploy to $1\nOtherwise:\n  Explain valid environments: dev, staging, prod\n  Show usage: /deploy [environment]\n```\n\n### File Existence Checks\n\n```markdown\n---\ndescription: Process configuration\nargument-hint: [config-file]\n---\n\nCheck file exists: !`test -f $1 && echo \"EXISTS\" || echo \"MISSING\"`\n\nIf file exists:\n  Process configuration: @$1\nOtherwise:\n  Explain where to place config file\n  Show expected format\n  Provide example configuration\n```\n\n### Plugin Resource Validation\n\n```markdown\n---\ndescription: Run plugin analyzer\nallowed-tools: Bash(test:*)\n---\n\nValidate plugin setup:\n- Script: !`test -x ${CLAUDE_PLUGIN_ROOT}/bin/analyze && echo \"\u2713\" || echo \"\u2717\"`\n- Config: !`test -f ${CLAUDE_PLUGIN_ROOT}/config.json && echo \"\u2713\" || echo \"\u2717\"`\n\nIf all checks pass, run analysis.\nOtherwise, report missing components.\n```\n\n### Error Handling\n\n```markdown\n---\ndescription: Build with error handling\nallowed-tools: Bash(*)\n---\n\nExecute build: !`bash ${CLAUDE_PLUGIN_ROOT}/scripts/build.sh 2>&1 || echo \"BUILD_FAILED\"`\n\nIf build succeeded:\n  Report success and output location\nIf build failed:\n  Analyze error output\n  Suggest likely causes\n  Provide troubleshooting steps\n```\n\n**Best practices:**\n- Validate early in command\n- Provide helpful error messages\n- Suggest corrective actions\n- Handle edge cases gracefully\n\n---\n\nFor detailed frontmatter field specifications, see `references/frontmatter-reference.md`.\nFor plugin-specific features and patterns, see `references/plugin-features-reference.md`.\nFor command pattern examples, see `examples/` directory.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 15, completeness: 15',
  'Score: 100, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-command-development-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Agent Development - This skill should be used when the user asks to "create an agent", "add an agent", "write a subagent", "agent frontmatter", "when to use description",',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Agent Development for Claude Code Plugins\n\n## Overview\n\nAgents are autonomous subprocesses that handle complex, multi-step tasks independently. Understanding agent structure, triggering conditions, and system prompt design enables creating powerful autonomous capabilities.\n\n**Key concepts:**\n- Agents are FOR autonomous work, commands are FOR user-initiated actions\n- Markdown file format with YAML frontmatter\n- Triggering via description field with examples\n- System prompt defines agent behavior\n- Model and color customization\n\n## Agent File Structure\n\n### Complete Format\n\n```markdown\n---\nname: agent-identifier\ndescription: Use this agent when [triggering conditions]. Examples:\n\n<example>\nContext: [Situation description]\nuser: \"[User request]\"\nassistant: \"[How assistant should respond and use this agent]\"\n<commentary>\n[Why this agent should be triggered]\n</commentary>\n</example>\n\n<example>\n[Additional example...]\n</example>\n\nmodel: inherit\ncolor: blue\ntools: [\"Read\", \"Write\", \"Grep\"]\n---\n\nYou are [agent role description]...\n\n**Your Core Responsibilities:**\n1. [Responsibility 1]\n2. [Responsibility 2]\n\n**Analysis Process:**\n[Step-by-step workflow]\n\n**Output Format:**\n[What to return]\n```\n\n## Frontmatter Fields\n\n### name (required)\n\nAgent identifier used for namespacing and invocation.\n\n**Format:** lowercase, numbers, hyphens only\n**Length:** 3-50 characters\n**Pattern:** Must start and end with alphanumeric\n\n**Good examples:**\n- `code-reviewer`\n- `test-generator`\n- `api-docs-writer`\n- `security-analyzer`\n\n**Bad examples:**\n- `helper` (too generic)\n- `-agent-` (starts/ends with hyphen)\n- `my_agent` (underscores not allowed)\n- `ag` (too short, < 3 chars)\n\n### description (required)\n\nDefines when Claude should trigger this agent. **This is the most critical field.**\n\n**Must include:**\n1. Triggering conditions (\"Use this agent when...\")\n2. Multiple `<example>` blocks showing usage\n3. Context, user request, and assistant response in each example\n4. `<commentary>` explaining why agent triggers\n\n**Format:**\n```\nUse this agent when [conditions]. Examples:\n\n<example>\nContext: [Scenario description]\nuser: \"[What user says]\"\nassistant: \"[How Claude should respond]\"\n<commentary>\n[Why this agent is appropriate]\n</commentary>\n</example>\n\n[More examples...]\n```\n\n**Best practices:**\n- Include 2-4 concrete examples\n- Show proactive and reactive triggering\n- Cover different phrasings of same intent\n- Explain reasoning in commentary\n- Be specific about when NOT to use the agent\n\n### model (required)\n\nWhich model the agent should use.\n\n**Options:**\n- `inherit` - Use same model as parent (recommended)\n- `sonnet` - Claude Sonnet (balanced)\n- `opus` - Claude Opus (most capable, expensive)\n- `haiku` - Claude Haiku (fast, cheap)\n\n**Recommendation:** Use `inherit` unless agent needs specific model capabilities.\n\n### color (required)\n\nVisual identifier for agent in UI.\n\n**Options:** `blue`, `cyan`, `green`, `yellow`, `magenta`, `red`\n\n**Guidelines:**\n- Choose distinct colors for different agents in same plugin\n- Use consistent colors for similar agent types\n- Blue/cyan: Analysis, review\n- Green: Success-oriented tasks\n- Yellow: Caution, validation\n- Red: Critical, security\n- Magenta: Creative, generation\n\n### tools (optional)\n\nRestrict agent to specific tools.\n\n**Format:** Array of tool names\n\n```yaml\ntools: [\"Read\", \"Write\", \"Grep\", \"Bash\"]\n```\n\n**Default:** If omitted, agent has access to all tools\n\n**Best practice:** Limit tools to minimum needed (principle of least privilege)\n\n**Common tool sets:**\n- Read-only analysis: `[\"Read\", \"Grep\", \"Glob\"]`\n- Code generation: `[\"Read\", \"Write\", \"Grep\"]`\n- Testing: `[\"Read\", \"Bash\", \"Grep\"]`\n- Full access: Omit field or use `[\"*\"]`\n\n## System Prompt Design\n\nThe markdown body becomes the agent''''s system prompt. Write in second person, addressing the agent directly.\n\n### Structure\n\n**Standard template:**\n```markdown\nYou are [role] specializing in [domain].\n\n**Your Core Responsibilities:**\n1. [Primary responsibility]\n2. [Secondary responsibility]\n3. [Additional responsibilities...]\n\n**Analysis Process:**\n1. [Step one]\n2. [Step two]\n3. [Step three]\n[...]\n\n**Quality Standards:**\n- [Standard 1]\n- [Standard 2]\n\n**Output Format:**\nProvide results in this format:\n- [What to include]\n- [How to structure]\n\n**Edge Cases:**\nHandle these situations:\n- [Edge case 1]: [How to handle]\n- [Edge case 2]: [How to handle]\n```\n\n### Best Practices\n\n\u2705 **DO:**\n- Write in second person (\"You are...\", \"You will...\")\n- Be specific about responsibilities\n- Provide step-by-step process\n- Define output format\n- Include quality standards\n- Address edge cases\n- Keep under 10,000 characters\n\n\u274c **DON''''T:**\n- Write in first person (\"I am...\", \"I will...\")\n- Be vague or generic\n- Omit process steps\n- Leave output format undefined\n- Skip quality guidance\n- Ignore error cases\n\n## Creating Agents\n\n### Method 1: AI-Assisted Generation\n\nUse this prompt pattern (extracted from Claude Code):\n\n```\nCreate an agent configuration based on this request: \"[YOUR DESCRIPTION]\"\n\nRequirements:\n1. Extract core intent and responsibilities\n2. Design expert persona for the domain\n3. Create comprehensive system prompt with:\n   - Clear behavioral boundaries\n   - Specific methodologies\n   - Edge case handling\n   - Output format\n4. Create identifier (lowercase, hyphens, 3-50 chars)\n5. Write description with triggering conditions\n6. Include 2-3 <example> blocks showing when to use\n\nReturn JSON with:\n{\n  \"identifier\": \"agent-name\",\n  \"whenToUse\": \"Use this agent when... Examples: <example>...</example>\",\n  \"systemPrompt\": \"You are...\"\n}\n```\n\nThen convert to agent file format with frontmatter.\n\nSee `examples/agent-creation-prompt.md` for complete template.\n\n### Method 2: Manual Creation\n\n1. Choose agent identifier (3-50 chars, lowercase, hyphens)\n2. Write description with examples\n3. Select model (usually `inherit`)\n4. Choose color for visual identification\n5. Define tools (if restricting access)\n6. Write system prompt with structure above\n7. Save as `agents/agent-name.md`\n\n## Validation Rules\n\n### Identifier Validation\n\n```\n\u2705 Valid: code-reviewer, test-gen, api-analyzer-v2\n\u274c Invalid: ag (too short), -start (starts with hyphen), my_agent (underscore)\n```\n\n**Rules:**\n- 3-50 characters\n- Lowercase letters, numbers, hyphens only\n- Must start and end with alphanumeric\n- No underscores, spaces, or special characters\n\n### Description Validation\n\n**Length:** 10-5,000 characters\n**Must include:** Triggering conditions and examples\n**Best:** 200-1,000 characters with 2-4 examples\n\n### System Prompt Validation\n\n**Length:** 20-10,000 characters\n**Best:** 500-3,000 characters\n**Structure:** Clear responsibilities, process, output format\n\n## Agent Organization\n\n### Plugin Agents Directory\n\n```\nplugin-name/\n\u2514\u2500\u2500 agents/\n    \u251c\u2500\u2500 analyzer.md\n    \u251c\u2500\u2500 reviewer.md\n    \u2514\u2500\u2500 generator.md\n```\n\nAll `.md` files in `agents/` are auto-discovered.\n\n### Namespacing\n\nAgents are namespaced automatically:\n- Single plugin: `agent-name`\n- With subdirectories: `plugin:subdir:agent-name`\n\n## Testing Agents\n\n### Test Triggering\n\nCreate test scenarios to verify agent triggers correctly:\n\n1. Write agent with specific triggering examples\n2. Use similar phrasing to examples in test\n3. Check Claude loads the agent\n4. Verify agent provides expected functionality\n\n### Test System Prompt\n\nEnsure system prompt is complete:\n\n1. Give agent typical task\n2. Check it follows process steps\n3. Verify output format is correct\n4. Test edge cases mentioned in prompt\n5. Confirm quality standards are met\n\n## Quick Reference\n\n### Minimal Agent\n\n```markdown\n---\nname: simple-agent\ndescription: Use this agent when... Examples: <example>...</example>\nmodel: inherit\ncolor: blue\n---\n\nYou are an agent that [does X].\n\nProcess:\n1. [Step 1]\n2. [Step 2]\n\nOutput: [What to provide]\n```\n\n### Frontmatter Fields Summary\n\n| Field | Required | Format | Example |\n|-------|----------|--------|---------|\n| name | Yes | lowercase-hyphens | code-reviewer |\n| description | Yes | Text + examples | Use when... <example>... |\n| model | Yes | inherit/sonnet/opus/haiku | inherit |\n| color | Yes | Color name | blue |\n| tools | No | Array of tool names | [\"Read\", \"Grep\"] |\n\n### Best Practices\n\n**DO:**\n- \u2705 Include 2-4 concrete examples in description\n- \u2705 Write specific triggering conditions\n- \u2705 Use `inherit` for model unless specific need\n- \u2705 Choose appropriate tools (least privilege)\n- \u2705 Write clear, structured system prompts\n- \u2705 Test agent triggering thoroughly\n\n**DON''''T:**\n- \u274c Use generic descriptions without examples\n- \u274c Omit triggering conditions\n- \u274c Give all agents same color\n- \u274c Grant unnecessary tool access\n- \u274c Write vague system prompts\n- \u274c Skip testing\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed guidance, consult:\n\n- **`references/system-prompt-design.md`** - Complete system prompt patterns\n- **`references/triggering-examples.md`** - Example formats and best practices\n- **`references/agent-creation-system-prompt.md`** - The exact prompt from Claude Code\n\n### Example Files\n\nWorking examples in `examples/`:\n\n- **`agent-creation-prompt.md`** - AI-assisted agent generation template\n- **`complete-agent-examples.md`** - Full agent examples for different use cases\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-agent.sh`** - Validate agent file structure\n- **`test-agent-trigger.sh`** - Test if agent triggers correctly\n\n## Implementation Workflow\n\nTo create an agent for a plugin:\n\n1. Define agent purpose and triggering conditions\n2. Choose creation method (AI-assisted or manual)\n3. Create `agents/agent-name.md` file\n4. Write frontmatter with all required fields\n5. Write system prompt following best practices\n6. Include 2-4 triggering examples in description\n7. Validate with `scripts/validate-agent.sh`\n8. Test triggering with real scenarios\n9. Document agent in plugin README\n\nFocus on clear triggering conditions and comprehensive system prompts for autonomous operation.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 15, completeness: 15',
  'Score: 97, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-agent-development-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Writing Hookify Rules - This skill should be used when the user asks to "create a hookify rule", "write a hook rule", "configure hookify", "add a hookify rule", or needs guid',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Writing Hookify Rules\n\n## Overview\n\nHookify rules are markdown files with YAML frontmatter that define patterns to watch for and messages to show when those patterns match. Rules are stored in `.claude/hookify.{rule-name}.local.md` files.\n\n## Rule File Format\n\n### Basic Structure\n\n```markdown\n---\nname: rule-identifier\nenabled: true\nevent: bash|file|stop|prompt|all\npattern: regex-pattern-here\n---\n\nMessage to show Claude when this rule triggers.\nCan include markdown formatting, warnings, suggestions, etc.\n```\n\n### Frontmatter Fields\n\n**name** (required): Unique identifier for the rule\n- Use kebab-case: `warn-dangerous-rm`, `block-console-log`\n- Be descriptive and action-oriented\n- Start with verb: warn, prevent, block, require, check\n\n**enabled** (required): Boolean to activate/deactivate\n- `true`: Rule is active\n- `false`: Rule is disabled (won''''t trigger)\n- Can toggle without deleting rule\n\n**event** (required): Which hook event to trigger on\n- `bash`: Bash tool commands\n- `file`: Edit, Write, MultiEdit tools\n- `stop`: When agent wants to stop\n- `prompt`: When user submits a prompt\n- `all`: All events\n\n**action** (optional): What to do when rule matches\n- `warn`: Show message but allow operation (default)\n- `block`: Prevent operation (PreToolUse) or stop session (Stop events)\n- If omitted, defaults to `warn`\n\n**pattern** (simple format): Regex pattern to match\n- Used for simple single-condition rules\n- Matches against command (bash) or new_text (file)\n- Python regex syntax\n\n**Example:**\n```yaml\nevent: bash\npattern: rm\\s+-rf\n```\n\n### Advanced Format (Multiple Conditions)\n\nFor complex rules with multiple conditions:\n\n```markdown\n---\nname: warn-env-file-edits\nenabled: true\nevent: file\nconditions:\n  - field: file_path\n    operator: regex_match\n    pattern: \\.env$\n  - field: new_text\n    operator: contains\n    pattern: API_KEY\n---\n\nYou''''re adding an API key to a .env file. Ensure this file is in .gitignore!\n```\n\n**Condition fields:**\n- `field`: Which field to check\n  - For bash: `command`\n  - For file: `file_path`, `new_text`, `old_text`, `content`\n- `operator`: How to match\n  - `regex_match`: Regex pattern matching\n  - `contains`: Substring check\n  - `equals`: Exact match\n  - `not_contains`: Substring must NOT be present\n  - `starts_with`: Prefix check\n  - `ends_with`: Suffix check\n- `pattern`: Pattern or string to match\n\n**All conditions must match for rule to trigger.**\n\n## Message Body\n\nThe markdown content after frontmatter is shown to Claude when the rule triggers.\n\n**Good messages:**\n- Explain what was detected\n- Explain why it''''s problematic\n- Suggest alternatives or best practices\n- Use formatting for clarity (bold, lists, etc.)\n\n**Example:**\n```markdown\n\u26a0\ufe0f **Console.log detected!**\n\nYou''''re adding console.log to production code.\n\n**Why this matters:**\n- Debug logs shouldn''''t ship to production\n- Console.log can expose sensitive data\n- Impacts browser performance\n\n**Alternatives:**\n- Use a proper logging library\n- Remove before committing\n- Use conditional debug builds\n```\n\n## Event Type Guide\n\n### bash Events\n\nMatch Bash command patterns:\n\n```markdown\n---\nevent: bash\npattern: sudo\\s+|rm\\s+-rf|chmod\\s+777\n---\n\nDangerous command detected!\n```\n\n**Common patterns:**\n- Dangerous commands: `rm\\s+-rf`, `dd\\s+if=`, `mkfs`\n- Privilege escalation: `sudo\\s+`, `su\\s+`\n- Permission issues: `chmod\\s+777`, `chown\\s+root`\n\n### file Events\n\nMatch Edit/Write/MultiEdit operations:\n\n```markdown\n---\nevent: file\npattern: console\\.log\\(|eval\\(|innerHTML\\s*=\n---\n\nPotentially problematic code pattern detected!\n```\n\n**Match on different fields:**\n```markdown\n---\nevent: file\nconditions:\n  - field: file_path\n    operator: regex_match\n    pattern: \\.tsx?$\n  - field: new_text\n    operator: regex_match\n    pattern: console\\.log\\(\n---\n\nConsole.log in TypeScript file!\n```\n\n**Common patterns:**\n- Debug code: `console\\.log\\(`, `debugger`, `print\\(`\n- Security risks: `eval\\(`, `innerHTML\\s*=`, `dangerouslySetInnerHTML`\n- Sensitive files: `\\.env$`, `credentials`, `\\.pem$`\n- Generated files: `node_modules/`, `dist/`, `build/`\n\n### stop Events\n\nMatch when agent wants to stop (completion checks):\n\n```markdown\n---\nevent: stop\npattern: .*\n---\n\nBefore stopping, verify:\n- [ ] Tests were run\n- [ ] Build succeeded\n- [ ] Documentation updated\n```\n\n**Use for:**\n- Reminders about required steps\n- Completion checklists\n- Process enforcement\n\n### prompt Events\n\nMatch user prompt content (advanced):\n\n```markdown\n---\nevent: prompt\nconditions:\n  - field: user_prompt\n    operator: contains\n    pattern: deploy to production\n---\n\nProduction deployment checklist:\n- [ ] Tests passing?\n- [ ] Reviewed by team?\n- [ ] Monitoring ready?\n```\n\n## Pattern Writing Tips\n\n### Regex Basics\n\n**Literal characters:** Most characters match themselves\n- `rm` matches \"rm\"\n- `console.log` matches \"console.log\"\n\n**Special characters need escaping:**\n- `.` (any char) \u2192 `\\.` (literal dot)\n- `(` `)` \u2192 `\\(` `\\)` (literal parens)\n- `[` `]` \u2192 `\\[` `\\]` (literal brackets)\n\n**Common metacharacters:**\n- `\\s` - whitespace (space, tab, newline)\n- `\\d` - digit (0-9)\n- `\\w` - word character (a-z, A-Z, 0-9, _)\n- `.` - any character\n- `+` - one or more\n- `*` - zero or more\n- `?` - zero or one\n- `|` - OR\n\n**Examples:**\n```\nrm\\s+-rf         Matches: rm -rf, rm  -rf\nconsole\\.log\\(   Matches: console.log(\n(eval|exec)\\(    Matches: eval( or exec(\nchmod\\s+777      Matches: chmod 777, chmod  777\nAPI_KEY\\s*=      Matches: API_KEY=, API_KEY =\n```\n\n### Testing Patterns\n\nTest regex patterns before using:\n\n```bash\npython3 -c \"import re; print(re.search(r''''your_pattern'''', ''''test text''''))\"\n```\n\nOr use online regex testers (regex101.com with Python flavor).\n\n### Common Pitfalls\n\n**Too broad:**\n```yaml\npattern: log    # Matches \"log\", \"login\", \"dialog\", \"catalog\"\n```\nBetter: `console\\.log\\(|logger\\.`\n\n**Too specific:**\n```yaml\npattern: rm -rf /tmp  # Only matches exact path\n```\nBetter: `rm\\s+-rf`\n\n**Escaping issues:**\n- YAML quoted strings: `\"pattern\"` requires double backslashes `\\\\s`\n- YAML unquoted: `pattern: \\s` works as-is\n- **Recommendation**: Use unquoted patterns in YAML\n\n## File Organization\n\n**Location:** All rules in `.claude/` directory\n**Naming:** `.claude/hookify.{descriptive-name}.local.md`\n**Gitignore:** Add `.claude/*.local.md` to `.gitignore`\n\n**Good names:**\n- `hookify.dangerous-rm.local.md`\n- `hookify.console-log.local.md`\n- `hookify.require-tests.local.md`\n- `hookify.sensitive-files.local.md`\n\n**Bad names:**\n- `hookify.rule1.local.md` (not descriptive)\n- `hookify.md` (missing .local)\n- `danger.local.md` (missing hookify prefix)\n\n## Workflow\n\n### Creating a Rule\n\n1. Identify unwanted behavior\n2. Determine which tool is involved (Bash, Edit, etc.)\n3. Choose event type (bash, file, stop, etc.)\n4. Write regex pattern\n5. Create `.claude/hookify.{name}.local.md` file in project root\n6. Test immediately - rules are read dynamically on next tool use\n\n### Refining a Rule\n\n1. Edit the `.local.md` file\n2. Adjust pattern or message\n3. Test immediately - changes take effect on next tool use\n\n### Disabling a Rule\n\n**Temporary:** Set `enabled: false` in frontmatter\n**Permanent:** Delete the `.local.md` file\n\n## Examples\n\nSee `${CLAUDE_PLUGIN_ROOT}/examples/` for complete examples:\n- `dangerous-rm.local.md` - Block dangerous rm commands\n- `console-log-warning.local.md` - Warn about console.log\n- `sensitive-files-warning.local.md` - Warn about editing .env files\n\n## Quick Reference\n\n**Minimum viable rule:**\n```markdown\n---\nname: my-rule\nenabled: true\nevent: bash\npattern: dangerous_command\n---\n\nWarning message here\n```\n\n**Rule with conditions:**\n```markdown\n---\nname: my-rule\nenabled: true\nevent: file\nconditions:\n  - field: file_path\n    operator: regex_match\n    pattern: \\.ts$\n  - field: new_text\n    operator: contains\n    pattern: any\n---\n\nWarning message\n```\n\n**Event types:**\n- `bash` - Bash commands\n- `file` - File edits\n- `stop` - Completion checks\n- `prompt` - User input\n- `all` - All events\n\n**Field options:**\n- Bash: `command`\n- File: `file_path`, `new_text`, `old_text`, `content`\n- Prompt: `user_prompt`\n\n**Operators:**\n- `regex_match`, `contains`, `equals`, `not_contains`, `starts_with`, `ends_with`\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 5',
  'Score: 87, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-hookify-skills-writing-rules-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'MCP Integration - This skill should be used when the user asks to "add MCP server", "integrate MCP", "configure MCP in plugin", "use .mcp.json", "set up Model Context P',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# MCP Integration for Claude Code Plugins\n\n## Overview\n\nModel Context Protocol (MCP) enables Claude Code plugins to integrate with external services and APIs by providing structured tool access. Use MCP integration to expose external service capabilities as tools within Claude Code.\n\n**Key capabilities:**\n- Connect to external services (databases, APIs, file systems)\n- Provide 10+ related tools from a single service\n- Handle OAuth and complex authentication flows\n- Bundle MCP servers with plugins for automatic setup\n\n## MCP Server Configuration Methods\n\nPlugins can bundle MCP servers in two ways:\n\n### Method 1: Dedicated .mcp.json (Recommended)\n\nCreate `.mcp.json` at plugin root:\n\n```json\n{\n  \"database-tools\": {\n    \"command\": \"${CLAUDE_PLUGIN_ROOT}/servers/db-server\",\n    \"args\": [\"--config\", \"${CLAUDE_PLUGIN_ROOT}/config.json\"],\n    \"env\": {\n      \"DB_URL\": \"${DB_URL}\"\n    }\n  }\n}\n```\n\n**Benefits:**\n- Clear separation of concerns\n- Easier to maintain\n- Better for multiple servers\n\n### Method 2: Inline in plugin.json\n\nAdd `mcpServers` field to plugin.json:\n\n```json\n{\n  \"name\": \"my-plugin\",\n  \"version\": \"1.0.0\",\n  \"mcpServers\": {\n    \"plugin-api\": {\n      \"command\": \"${CLAUDE_PLUGIN_ROOT}/servers/api-server\",\n      \"args\": [\"--port\", \"8080\"]\n    }\n  }\n}\n```\n\n**Benefits:**\n- Single configuration file\n- Good for simple single-server plugins\n\n## MCP Server Types\n\n### stdio (Local Process)\n\nExecute local MCP servers as child processes. Best for local tools and custom servers.\n\n**Configuration:**\n```json\n{\n  \"filesystem\": {\n    \"command\": \"npx\",\n    \"args\": [\"-y\", \"@modelcontextprotocol/server-filesystem\", \"/allowed/path\"],\n    \"env\": {\n      \"LOG_LEVEL\": \"debug\"\n    }\n  }\n}\n```\n\n**Use cases:**\n- File system access\n- Local database connections\n- Custom MCP servers\n- NPM-packaged MCP servers\n\n**Process management:**\n- Claude Code spawns and manages the process\n- Communicates via stdin/stdout\n- Terminates when Claude Code exits\n\n### SSE (Server-Sent Events)\n\nConnect to hosted MCP servers with OAuth support. Best for cloud services.\n\n**Configuration:**\n```json\n{\n  \"asana\": {\n    \"type\": \"sse\",\n    \"url\": \"https://mcp.asana.com/sse\"\n  }\n}\n```\n\n**Use cases:**\n- Official hosted MCP servers (Asana, GitHub, etc.)\n- Cloud services with MCP endpoints\n- OAuth-based authentication\n- No local installation needed\n\n**Authentication:**\n- OAuth flows handled automatically\n- User prompted on first use\n- Tokens managed by Claude Code\n\n### HTTP (REST API)\n\nConnect to RESTful MCP servers with token authentication.\n\n**Configuration:**\n```json\n{\n  \"api-service\": {\n    \"type\": \"http\",\n    \"url\": \"https://api.example.com/mcp\",\n    \"headers\": {\n      \"Authorization\": \"Bearer ${API_TOKEN}\",\n      \"X-Custom-Header\": \"value\"\n    }\n  }\n}\n```\n\n**Use cases:**\n- REST API-based MCP servers\n- Token-based authentication\n- Custom API backends\n- Stateless interactions\n\n### WebSocket (Real-time)\n\nConnect to WebSocket MCP servers for real-time bidirectional communication.\n\n**Configuration:**\n```json\n{\n  \"realtime-service\": {\n    \"type\": \"ws\",\n    \"url\": \"wss://mcp.example.com/ws\",\n    \"headers\": {\n      \"Authorization\": \"Bearer ${TOKEN}\"\n    }\n  }\n}\n```\n\n**Use cases:**\n- Real-time data streaming\n- Persistent connections\n- Push notifications from server\n- Low-latency requirements\n\n## Environment Variable Expansion\n\nAll MCP configurations support environment variable substitution:\n\n**${CLAUDE_PLUGIN_ROOT}** - Plugin directory (always use for portability):\n```json\n{\n  \"command\": \"${CLAUDE_PLUGIN_ROOT}/servers/my-server\"\n}\n```\n\n**User environment variables** - From user''''s shell:\n```json\n{\n  \"env\": {\n    \"API_KEY\": \"${MY_API_KEY}\",\n    \"DATABASE_URL\": \"${DB_URL}\"\n  }\n}\n```\n\n**Best practice:** Document all required environment variables in plugin README.\n\n## MCP Tool Naming\n\nWhen MCP servers provide tools, they''''re automatically prefixed:\n\n**Format:** `mcp__plugin_<plugin-name>_<server-name>__<tool-name>`\n\n**Example:**\n- Plugin: `asana`\n- Server: `asana`\n- Tool: `create_task`\n- **Full name:** `mcp__plugin_asana_asana__asana_create_task`\n\n### Using MCP Tools in Commands\n\nPre-allow specific MCP tools in command frontmatter:\n\n```markdown\n---\nallowed-tools: [\n  \"mcp__plugin_asana_asana__asana_create_task\",\n  \"mcp__plugin_asana_asana__asana_search_tasks\"\n]\n---\n```\n\n**Wildcard (use sparingly):**\n```markdown\n---\nallowed-tools: [\"mcp__plugin_asana_asana__*\"]\n---\n```\n\n**Best practice:** Pre-allow specific tools, not wildcards, for security.\n\n## Lifecycle Management\n\n**Automatic startup:**\n- MCP servers start when plugin enables\n- Connection established before first tool use\n- Restart required for configuration changes\n\n**Lifecycle:**\n1. Plugin loads\n2. MCP configuration parsed\n3. Server process started (stdio) or connection established (SSE/HTTP/WS)\n4. Tools discovered and registered\n5. Tools available as `mcp__plugin_...__...`\n\n**Viewing servers:**\nUse `/mcp` command to see all servers including plugin-provided ones.\n\n## Authentication Patterns\n\n### OAuth (SSE/HTTP)\n\nOAuth handled automatically by Claude Code:\n\n```json\n{\n  \"type\": \"sse\",\n  \"url\": \"https://mcp.example.com/sse\"\n}\n```\n\nUser authenticates in browser on first use. No additional configuration needed.\n\n### Token-Based (Headers)\n\nStatic or environment variable tokens:\n\n```json\n{\n  \"type\": \"http\",\n  \"url\": \"https://api.example.com\",\n  \"headers\": {\n    \"Authorization\": \"Bearer ${API_TOKEN}\"\n  }\n}\n```\n\nDocument required environment variables in README.\n\n### Environment Variables (stdio)\n\nPass configuration to MCP server:\n\n```json\n{\n  \"command\": \"python\",\n  \"args\": [\"-m\", \"my_mcp_server\"],\n  \"env\": {\n    \"DATABASE_URL\": \"${DB_URL}\",\n    \"API_KEY\": \"${API_KEY}\",\n    \"LOG_LEVEL\": \"info\"\n  }\n}\n```\n\n## Integration Patterns\n\n### Pattern 1: Simple Tool Wrapper\n\nCommands use MCP tools with user interaction:\n\n```markdown\n# Command: create-item.md\n---\nallowed-tools: [\"mcp__plugin_name_server__create_item\"]\n---\n\nSteps:\n1. Gather item details from user\n2. Use mcp__plugin_name_server__create_item\n3. Confirm creation\n```\n\n**Use for:** Adding validation or preprocessing before MCP calls.\n\n### Pattern 2: Autonomous Agent\n\nAgents use MCP tools autonomously:\n\n```markdown\n# Agent: data-analyzer.md\n\nAnalysis Process:\n1. Query data via mcp__plugin_db_server__query\n2. Process and analyze results\n3. Generate insights report\n```\n\n**Use for:** Multi-step MCP workflows without user interaction.\n\n### Pattern 3: Multi-Server Plugin\n\nIntegrate multiple MCP servers:\n\n```json\n{\n  \"github\": {\n    \"type\": \"sse\",\n    \"url\": \"https://mcp.github.com/sse\"\n  },\n  \"jira\": {\n    \"type\": \"sse\",\n    \"url\": \"https://mcp.jira.com/sse\"\n  }\n}\n```\n\n**Use for:** Workflows spanning multiple services.\n\n## Security Best Practices\n\n### Use HTTPS/WSS\n\nAlways use secure connections:\n\n```json\n\u2705 \"url\": \"https://mcp.example.com/sse\"\n\u274c \"url\": \"http://mcp.example.com/sse\"\n```\n\n### Token Management\n\n**DO:**\n- \u2705 Use environment variables for tokens\n- \u2705 Document required env vars in README\n- \u2705 Let OAuth flow handle authentication\n\n**DON''''T:**\n- \u274c Hardcode tokens in configuration\n- \u274c Commit tokens to git\n- \u274c Share tokens in documentation\n\n### Permission Scoping\n\nPre-allow only necessary MCP tools:\n\n```markdown\n\u2705 allowed-tools: [\n  \"mcp__plugin_api_server__read_data\",\n  \"mcp__plugin_api_server__create_item\"\n]\n\n\u274c allowed-tools: [\"mcp__plugin_api_server__*\"]\n```\n\n## Error Handling\n\n### Connection Failures\n\nHandle MCP server unavailability:\n- Provide fallback behavior in commands\n- Inform user of connection issues\n- Check server URL and configuration\n\n### Tool Call Errors\n\nHandle failed MCP operations:\n- Validate inputs before calling MCP tools\n- Provide clear error messages\n- Check rate limiting and quotas\n\n### Configuration Errors\n\nValidate MCP configuration:\n- Test server connectivity during development\n- Validate JSON syntax\n- Check required environment variables\n\n## Performance Considerations\n\n### Lazy Loading\n\nMCP servers connect on-demand:\n- Not all servers connect at startup\n- First tool use triggers connection\n- Connection pooling managed automatically\n\n### Batching\n\nBatch similar requests when possible:\n\n```\n# Good: Single query with filters\ntasks = search_tasks(project=\"X\", assignee=\"me\", limit=50)\n\n# Avoid: Many individual queries\nfor id in task_ids:\n    task = get_task(id)\n```\n\n## Testing MCP Integration\n\n### Local Testing\n\n1. Configure MCP server in `.mcp.json`\n2. Install plugin locally (`.claude-plugin/`)\n3. Run `/mcp` to verify server appears\n4. Test tool calls in commands\n5. Check `claude --debug` logs for connection issues\n\n### Validation Checklist\n\n- [ ] MCP configuration is valid JSON\n- [ ] Server URL is correct and accessible\n- [ ] Required environment variables documented\n- [ ] Tools appear in `/mcp` output\n- [ ] Authentication works (OAuth or tokens)\n- [ ] Tool calls succeed from commands\n- [ ] Error cases handled gracefully\n\n## Debugging\n\n### Enable Debug Logging\n\n```bash\nclaude --debug\n```\n\nLook for:\n- MCP server connection attempts\n- Tool discovery logs\n- Authentication flows\n- Tool call errors\n\n### Common Issues\n\n**Server not connecting:**\n- Check URL is correct\n- Verify server is running (stdio)\n- Check network connectivity\n- Review authentication configuration\n\n**Tools not available:**\n- Verify server connected successfully\n- Check tool names match exactly\n- Run `/mcp` to see available tools\n- Restart Claude Code after config changes\n\n**Authentication failing:**\n- Clear cached auth tokens\n- Re-authenticate\n- Check token scopes and permissions\n- Verify environment variables set\n\n## Quick Reference\n\n### MCP Server Types\n\n| Type | Transport | Best For | Auth |\n|------|-----------|----------|------|\n| stdio | Process | Local tools, custom servers | Env vars |\n| SSE | HTTP | Hosted services, cloud APIs | OAuth |\n| HTTP | REST | API backends, token auth | Tokens |\n| ws | WebSocket | Real-time, streaming | Tokens |\n\n### Configuration Checklist\n\n- [ ] Server type specified (stdio/SSE/HTTP/ws)\n- [ ] Type-specific fields complete (command or url)\n- [ ] Authentication configured\n- [ ] Environment variables documented\n- [ ] HTTPS/WSS used (not HTTP/WS)\n- [ ] ${CLAUDE_PLUGIN_ROOT} used for paths\n\n### Best Practices\n\n**DO:**\n- \u2705 Use ${CLAUDE_PLUGIN_ROOT} for portable paths\n- \u2705 Document required environment variables\n- \u2705 Use secure connections (HTTPS/WSS)\n- \u2705 Pre-allow specific MCP tools in commands\n- \u2705 Test MCP integration before publishing\n- \u2705 Handle connection and tool errors gracefully\n\n**DON''''T:**\n- \u274c Hardcode absolute paths\n- \u274c Commit credentials to git\n- \u274c Use HTTP instead of HTTPS\n- \u274c Pre-allow all tools with wildcards\n- \u274c Skip error handling\n- \u274c Forget to document setup\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed information, consult:\n\n- **`references/server-types.md`** - Deep dive on each server type\n- **`references/authentication.md`** - Authentication patterns and OAuth\n- **`references/tool-usage.md`** - Using MCP tools in commands and agents\n\n### Example Configurations\n\nWorking examples in `examples/`:\n\n- **`stdio-server.json`** - Local stdio MCP server\n- **`sse-server.json`** - Hosted SSE server with OAuth\n- **`http-server.json`** - REST API with token auth\n\n### External Resources\n\n- **Official MCP Docs**: https://modelcontextprotocol.io/\n- **Claude Code MCP Docs**: https://docs.claude.com/en/docs/claude-code/mcp\n- **MCP SDK**: @modelcontextprotocol/sdk\n- **Testing**: Use `claude --debug` and `/mcp` command\n\n## Implementation Workflow\n\nTo add MCP integration to a plugin:\n\n1. Choose MCP server type (stdio, SSE, HTTP, ws)\n2. Create `.mcp.json` at plugin root with configuration\n3. Use ${CLAUDE_PLUGIN_ROOT} for all file references\n4. Document required environment variables in README\n5. Test locally with `/mcp` command\n6. Pre-allow MCP tools in relevant commands\n7. Handle authentication (OAuth or tokens)\n8. Test error cases (connection failures, auth errors)\n9. Document MCP integration in plugin README\n\nFocus on stdio for custom/local servers, SSE for hosted services with OAuth.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 10',
  'Score: 95, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-mcp-integration-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Skill Development - This skill should be used when the user wants to "create a skill", "add a skill to plugin", "write a new skill", "improve skill description", "organiz',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Skill Development for Claude Code Plugins\n\nThis skill provides guidance for creating effective skills for Claude Code plugins.\n\n## About Skills\n\nSkills are modular, self-contained packages that extend Claude''''s capabilities by providing\nspecialized knowledge, workflows, and tools. Think of them as \"onboarding guides\" for specific\ndomains or tasks\u2014they transform Claude from a general-purpose agent into a specialized agent\nequipped with procedural knowledge that no model can fully possess.\n\n### What Skills Provide\n\n1. Specialized workflows - Multi-step procedures for specific domains\n2. Tool integrations - Instructions for working with specific file formats or APIs\n3. Domain expertise - Company-specific knowledge, schemas, business logic\n4. Bundled resources - Scripts, references, and assets for complex and repetitive tasks\n\n### Anatomy of a Skill\n\nEvery skill consists of a required SKILL.md file and optional bundled resources:\n\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md (required)\n\u2502   \u251c\u2500\u2500 YAML frontmatter metadata (required)\n\u2502   \u2502   \u251c\u2500\u2500 name: (required)\n\u2502   \u2502   \u2514\u2500\u2500 description: (required)\n\u2502   \u2514\u2500\u2500 Markdown instructions (required)\n\u2514\u2500\u2500 Bundled Resources (optional)\n    \u251c\u2500\u2500 scripts/          - Executable code (Python/Bash/etc.)\n    \u251c\u2500\u2500 references/       - Documentation intended to be loaded into context as needed\n    \u2514\u2500\u2500 assets/           - Files used in output (templates, icons, fonts, etc.)\n```\n\n#### SKILL.md (required)\n\n**Metadata Quality:** The `name` and `description` in YAML frontmatter determine when Claude will use the skill. Be specific about what the skill does and when to use it. Use the third-person (e.g. \"This skill should be used when...\" instead of \"Use this skill when...\").\n\n#### Bundled Resources (optional)\n\n##### Scripts (`scripts/`)\n\nExecutable code (Python/Bash/etc.) for tasks that require deterministic reliability or are repeatedly rewritten.\n\n- **When to include**: When the same code is being rewritten repeatedly or deterministic reliability is needed\n- **Example**: `scripts/rotate_pdf.py` for PDF rotation tasks\n- **Benefits**: Token efficient, deterministic, may be executed without loading into context\n- **Note**: Scripts may still need to be read by Claude for patching or environment-specific adjustments\n\n##### References (`references/`)\n\nDocumentation and reference material intended to be loaded as needed into context to inform Claude''''s process and thinking.\n\n- **When to include**: For documentation that Claude should reference while working\n- **Examples**: `references/finance.md` for financial schemas, `references/mnda.md` for company NDA template, `references/policies.md` for company policies, `references/api_docs.md` for API specifications\n- **Use cases**: Database schemas, API documentation, domain knowledge, company policies, detailed workflow guides\n- **Benefits**: Keeps SKILL.md lean, loaded only when Claude determines it''''s needed\n- **Best practice**: If files are large (>10k words), include grep search patterns in SKILL.md\n- **Avoid duplication**: Information should live in either SKILL.md or references files, not both. Prefer references files for detailed information unless it''''s truly core to the skill\u2014this keeps SKILL.md lean while making information discoverable without hogging the context window. Keep only essential procedural instructions and workflow guidance in SKILL.md; move detailed reference material, schemas, and examples to references files.\n\n##### Assets (`assets/`)\n\nFiles not intended to be loaded into context, but rather used within the output Claude produces.\n\n- **When to include**: When the skill needs files that will be used in the final output\n- **Examples**: `assets/logo.png` for brand assets, `assets/slides.pptx` for PowerPoint templates, `assets/frontend-template/` for HTML/React boilerplate, `assets/font.ttf` for typography\n- **Use cases**: Templates, images, icons, boilerplate code, fonts, sample documents that get copied or modified\n- **Benefits**: Separates output resources from documentation, enables Claude to use files without loading them into context\n\n### Progressive Disclosure Design Principle\n\nSkills use a three-level loading system to manage context efficiently:\n\n1. **Metadata (name + description)** - Always in context (~100 words)\n2. **SKILL.md body** - When skill triggers (<5k words)\n3. **Bundled resources** - As needed by Claude (Unlimited*)\n\n*Unlimited because scripts can be executed without reading into context window.\n\n## Skill Creation Process\n\nTo create a skill, follow the \"Skill Creation Process\" in order, skipping steps only if there is a clear reason why they are not applicable.\n\n### Step 1: Understanding the Skill with Concrete Examples\n\nSkip this step only when the skill''''s usage patterns are already clearly understood. It remains valuable even when working with an existing skill.\n\nTo create an effective skill, clearly understand concrete examples of how the skill will be used. This understanding can come from either direct user examples or generated examples that are validated with user feedback.\n\nFor example, when building an image-editor skill, relevant questions include:\n\n- \"What functionality should the image-editor skill support? Editing, rotating, anything else?\"\n- \"Can you give some examples of how this skill would be used?\"\n- \"I can imagine users asking for things like ''''Remove the red-eye from this image'''' or ''''Rotate this image''''. Are there other ways you imagine this skill being used?\"\n- \"What would a user say that should trigger this skill?\"\n\nTo avoid overwhelming users, avoid asking too many questions in a single message. Start with the most important questions and follow up as needed for better effectiveness.\n\nConclude this step when there is a clear sense of the functionality the skill should support.\n\n### Step 2: Planning the Reusable Skill Contents\n\nTo turn concrete examples into an effective skill, analyze each example by:\n\n1. Considering how to execute on the example from scratch\n2. Identifying what scripts, references, and assets would be helpful when executing these workflows repeatedly\n\nExample: When building a `pdf-editor` skill to handle queries like \"Help me rotate this PDF,\" the analysis shows:\n\n1. Rotating a PDF requires re-writing the same code each time\n2. A `scripts/rotate_pdf.py` script would be helpful to store in the skill\n\nExample: When designing a `frontend-webapp-builder` skill for queries like \"Build me a todo app\" or \"Build me a dashboard to track my steps,\" the analysis shows:\n\n1. Writing a frontend webapp requires the same boilerplate HTML/React each time\n2. An `assets/hello-world/` template containing the boilerplate HTML/React project files would be helpful to store in the skill\n\nExample: When building a `big-query` skill to handle queries like \"How many users have logged in today?\" the analysis shows:\n\n1. Querying BigQuery requires re-discovering the table schemas and relationships each time\n2. A `references/schema.md` file documenting the table schemas would be helpful to store in the skill\n\n**For Claude Code plugins:** When building a hooks skill, the analysis shows:\n1. Developers repeatedly need to validate hooks.json and test hook scripts\n2. `scripts/validate-hook-schema.sh` and `scripts/test-hook.sh` utilities would be helpful\n3. `references/patterns.md` for detailed hook patterns to avoid bloating SKILL.md\n\nTo establish the skill''''s contents, analyze each concrete example to create a list of the reusable resources to include: scripts, references, and assets.\n\n### Step 3: Create Skill Structure\n\nFor Claude Code plugins, create the skill directory structure:\n\n```bash\nmkdir -p plugin-name/skills/skill-name/{references,examples,scripts}\ntouch plugin-name/skills/skill-name/SKILL.md\n```\n\n**Note:** Unlike the generic skill-creator which uses `init_skill.py`, plugin skills are created directly in the plugin''''s `skills/` directory with a simpler manual structure.\n\n### Step 4: Edit the Skill\n\nWhen editing the (newly-created or existing) skill, remember that the skill is being created for another instance of Claude to use. Focus on including information that would be beneficial and non-obvious to Claude. Consider what procedural knowledge, domain-specific details, or reusable assets would help another Claude instance execute these tasks more effectively.\n\n#### Start with Reusable Skill Contents\n\nTo begin implementation, start with the reusable resources identified above: `scripts/`, `references/`, and `assets/` files. Note that this step may require user input. For example, when implementing a `brand-guidelines` skill, the user may need to provide brand assets or templates to store in `assets/`, or documentation to store in `references/`.\n\nAlso, delete any example files and directories not needed for the skill. Create only the directories you actually need (references/, examples/, scripts/).\n\n#### Update SKILL.md\n\n**Writing Style:** Write the entire skill using **imperative/infinitive form** (verb-first instructions), not second person. Use objective, instructional language (e.g., \"To accomplish X, do Y\" rather than \"You should do X\" or \"If you need to do X\"). This maintains consistency and clarity for AI consumption.\n\n**Description (Frontmatter):** Use third-person format with specific trigger phrases:\n\n```yaml\n---\nname: Skill Name\ndescription: This skill should be used when the user asks to \"specific phrase 1\", \"specific phrase 2\", \"specific phrase 3\". Include exact phrases users would say that should trigger this skill. Be concrete and specific.\nversion: 0.1.0\n---\n```\n\n**Good description examples:**\n```yaml\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse hook\", \"validate tool use\", \"implement prompt-based hooks\", or mentions hook events (PreToolUse, PostToolUse, Stop).\n```\n\n**Bad description examples:**\n```yaml\ndescription: Use this skill when working with hooks.  # Wrong person, vague\ndescription: Load when user needs hook help.  # Not third person\ndescription: Provides hook guidance.  # No trigger phrases\n```\n\nTo complete SKILL.md body, answer the following questions:\n\n1. What is the purpose of the skill, in a few sentences?\n2. When should the skill be used? (Include this in frontmatter description with specific triggers)\n3. In practice, how should Claude use the skill? All reusable skill contents developed above should be referenced so that Claude knows how to use them.\n\n**Keep SKILL.md lean:** Target 1,500-2,000 words for the body. Move detailed content to references/:\n- Detailed patterns \u2192 `references/patterns.md`\n- Advanced techniques \u2192 `references/advanced.md`\n- Migration guides \u2192 `references/migration.md`\n- API references \u2192 `references/api-reference.md`\n\n**Reference resources in SKILL.md:**\n```markdown\n## Additional Resources\n\n### Reference Files\n\nFor detailed patterns and techniques, consult:\n- **`references/patterns.md`** - Common patterns\n- **`references/advanced.md`** - Advanced use cases\n\n### Example Files\n\nWorking examples in `examples/`:\n- **`example-script.sh`** - Working example\n```\n\n### Step 5: Validate and Test\n\n**For plugin skills, validation is different from generic skills:**\n\n1. **Check structure**: Skill directory in `plugin-name/skills/skill-name/`\n2. **Validate SKILL.md**: Has frontmatter with name and description\n3. **Check trigger phrases**: Description includes specific user queries\n4. **Verify writing style**: Body uses imperative/infinitive form, not second person\n5. **Test progressive disclosure**: SKILL.md is lean (~1,500-2,000 words), detailed content in references/\n6. **Check references**: All referenced files exist\n7. **Validate examples**: Examples are complete and correct\n8. **Test scripts**: Scripts are executable and work correctly\n\n**Use the skill-reviewer agent:**\n```\nAsk: \"Review my skill and check if it follows best practices\"\n```\n\nThe skill-reviewer agent will check description quality, content organization, and progressive disclosure.\n\n### Step 6: Iterate\n\nAfter testing the skill, users may request improvements. Often this happens right after using the skill, with fresh context of how the skill performed.\n\n**Iteration workflow:**\n1. Use the skill on real tasks\n2. Notice struggles or inefficiencies\n3. Identify how SKILL.md or bundled resources should be updated\n4. Implement changes and test again\n\n**Common improvements:**\n- Strengthen trigger phrases in description\n- Move long sections from SKILL.md to references/\n- Add missing examples or scripts\n- Clarify ambiguous instructions\n- Add edge case handling\n\n## Plugin-Specific Considerations\n\n### Skill Location in Plugins\n\nPlugin skills live in the plugin''''s `skills/` directory:\n\n```\nmy-plugin/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json\n\u251c\u2500\u2500 commands/\n\u251c\u2500\u2500 agents/\n\u2514\u2500\u2500 skills/\n    \u2514\u2500\u2500 my-skill/\n        \u251c\u2500\u2500 SKILL.md\n        \u251c\u2500\u2500 references/\n        \u251c\u2500\u2500 examples/\n        \u2514\u2500\u2500 scripts/\n```\n\n### Auto-Discovery\n\nClaude Code automatically discovers skills:\n- Scans `skills/` directory\n- Finds subdirectories containing `SKILL.md`\n- Loads skill metadata (name + description) always\n- Loads SKILL.md body when skill triggers\n- Loads references/examples when needed\n\n### No Packaging Needed\n\nPlugin skills are distributed as part of the plugin, not as separate ZIP files. Users get skills when they install the plugin.\n\n### Testing in Plugins\n\nTest skills by installing plugin locally:\n\n```bash\n# Test with --plugin-dir\ncc --plugin-dir /path/to/plugin\n\n# Ask questions that should trigger the skill\n# Verify skill loads correctly\n```\n\n## Examples from Plugin-Dev\n\nStudy the skills in this plugin as examples of best practices:\n\n**hook-development skill:**\n- Excellent trigger phrases: \"create a hook\", \"add a PreToolUse hook\", etc.\n- Lean SKILL.md (1,651 words)\n- 3 references/ files for detailed content\n- 3 examples/ of working hooks\n- 3 scripts/ utilities\n\n**agent-development skill:**\n- Strong triggers: \"create an agent\", \"agent frontmatter\", etc.\n- Focused SKILL.md (1,438 words)\n- References include the AI generation prompt from Claude Code\n- Complete agent examples\n\n**plugin-settings skill:**\n- Specific triggers: \"plugin settings\", \".local.md files\", \"YAML frontmatter\"\n- References show real implementations (multi-agent-swarm, ralph-wiggum)\n- Working parsing scripts\n\nEach demonstrates progressive disclosure and strong triggering.\n\n## Progressive Disclosure in Practice\n\n### What Goes in SKILL.md\n\n**Include (always loaded when skill triggers):**\n- Core concepts and overview\n- Essential procedures and workflows\n- Quick reference tables\n- Pointers to references/examples/scripts\n- Most common use cases\n\n**Keep under 3,000 words, ideally 1,500-2,000 words**\n\n### What Goes in references/\n\n**Move to references/ (loaded as needed):**\n- Detailed patterns and advanced techniques\n- Comprehensive API documentation\n- Migration guides\n- Edge cases and troubleshooting\n- Extensive examples and walkthroughs\n\n**Each reference file can be large (2,000-5,000+ words)**\n\n### What Goes in examples/\n\n**Working code examples:**\n- Complete, runnable scripts\n- Configuration files\n- Template files\n- Real-world usage examples\n\n**Users can copy and adapt these directly**\n\n### What Goes in scripts/\n\n**Utility scripts:**\n- Validation tools\n- Testing helpers\n- Parsing utilities\n- Automation scripts\n\n**Should be executable and documented**\n\n## Writing Style Requirements\n\n### Imperative/Infinitive Form\n\nWrite using verb-first instructions, not second person:\n\n**Correct (imperative):**\n```\nTo create a hook, define the event type.\nConfigure the MCP server with authentication.\nValidate settings before use.\n```\n\n**Incorrect (second person):**\n```\nYou should create a hook by defining the event type.\nYou need to configure the MCP server.\nYou must validate settings before use.\n```\n\n### Third-Person in Description\n\nThe frontmatter description must use third person:\n\n**Correct:**\n```yaml\ndescription: This skill should be used when the user asks to \"create X\", \"configure Y\"...\n```\n\n**Incorrect:**\n```yaml\ndescription: Use this skill when you want to create X...\ndescription: Load this skill when user asks...\n```\n\n### Objective, Instructional Language\n\nFocus on what to do, not who should do it:\n\n**Correct:**\n```\nParse the frontmatter using sed.\nExtract fields with grep.\nValidate values before use.\n```\n\n**Incorrect:**\n```\nYou can parse the frontmatter...\nClaude should extract fields...\nThe user might validate values...\n```\n\n## Validation Checklist\n\nBefore finalizing a skill:\n\n**Structure:**\n- [ ] SKILL.md file exists with valid YAML frontmatter\n- [ ] Frontmatter has `name` and `description` fields\n- [ ] Markdown body is present and substantial\n- [ ] Referenced files actually exist\n\n**Description Quality:**\n- [ ] Uses third person (\"This skill should be used when...\")\n- [ ] Includes specific trigger phrases users would say\n- [ ] Lists concrete scenarios (\"create X\", \"configure Y\")\n- [ ] Not vague or generic\n\n**Content Quality:**\n- [ ] SKILL.md body uses imperative/infinitive form\n- [ ] Body is focused and lean (1,500-2,000 words ideal, <5k max)\n- [ ] Detailed content moved to references/\n- [ ] Examples are complete and working\n- [ ] Scripts are executable and documented\n\n**Progressive Disclosure:**\n- [ ] Core concepts in SKILL.md\n- [ ] Detailed docs in references/\n- [ ] Working code in examples/\n- [ ] Utilities in scripts/\n- [ ] SKILL.md references these resources\n\n**Testing:**\n- [ ] Skill triggers on expected user queries\n- [ ] Content is helpful for intended tasks\n- [ ] No duplicated information across files\n- [ ] References load when needed\n\n## Common Mistakes to Avoid\n\n### Mistake 1: Weak Trigger Description\n\n\u274c **Bad:**\n```yaml\ndescription: Provides guidance for working with hooks.\n```\n\n**Why bad:** Vague, no specific trigger phrases, not third person\n\n\u2705 **Good:**\n```yaml\ndescription: This skill should be used when the user asks to \"create a hook\", \"add a PreToolUse hook\", \"validate tool use\", or mentions hook events. Provides comprehensive hooks API guidance.\n```\n\n**Why good:** Third person, specific phrases, concrete scenarios\n\n### Mistake 2: Too Much in SKILL.md\n\n\u274c **Bad:**\n```\nskill-name/\n\u2514\u2500\u2500 SKILL.md  (8,000 words - everything in one file)\n```\n\n**Why bad:** Bloats context when skill loads, detailed content always loaded\n\n\u2705 **Good:**\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md  (1,800 words - core essentials)\n\u2514\u2500\u2500 references/\n    \u251c\u2500\u2500 patterns.md (2,500 words)\n    \u2514\u2500\u2500 advanced.md (3,700 words)\n```\n\n**Why good:** Progressive disclosure, detailed content loaded only when needed\n\n### Mistake 3: Second Person Writing\n\n\u274c **Bad:**\n```markdown\nYou should start by reading the configuration file.\nYou need to validate the input.\nYou can use the grep tool to search.\n```\n\n**Why bad:** Second person, not imperative form\n\n\u2705 **Good:**\n```markdown\nStart by reading the configuration file.\nValidate the input before processing.\nUse the grep tool to search for patterns.\n```\n\n**Why good:** Imperative form, direct instructions\n\n### Mistake 4: Missing Resource References\n\n\u274c **Bad:**\n```markdown\n# SKILL.md\n\n[Core content]\n\n[No mention of references/ or examples/]\n```\n\n**Why bad:** Claude doesn''''t know references exist\n\n\u2705 **Good:**\n```markdown\n# SKILL.md\n\n[Core content]\n\n## Additional Resources\n\n### Reference Files\n- **`references/patterns.md`** - Detailed patterns\n- **`references/advanced.md`** - Advanced techniques\n\n### Examples\n- **`examples/script.sh`** - Working example\n```\n\n**Why good:** Claude knows where to find additional information\n\n## Quick Reference\n\n### Minimal Skill\n\n```\nskill-name/\n\u2514\u2500\u2500 SKILL.md\n```\n\nGood for: Simple knowledge, no complex resources needed\n\n### Standard Skill (Recommended)\n\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md\n\u251c\u2500\u2500 references/\n\u2502   \u2514\u2500\u2500 detailed-guide.md\n\u2514\u2500\u2500 examples/\n    \u2514\u2500\u2500 working-example.sh\n```\n\nGood for: Most plugin skills with detailed documentation\n\n### Complete Skill\n\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md\n\u251c\u2500\u2500 references/\n\u2502   \u251c\u2500\u2500 patterns.md\n\u2502   \u2514\u2500\u2500 advanced.md\n\u251c\u2500\u2500 examples/\n\u2502   \u251c\u2500\u2500 example1.sh\n\u2502   \u2514\u2500\u2500 example2.json\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 validate.sh\n```\n\nGood for: Complex domains with validation utilities\n\n## Best Practices Summary\n\n\u2705 **DO:**\n- Use third-person in description (\"This skill should be used when...\")\n- Include specific trigger phrases (\"create X\", \"configure Y\")\n- Keep SKILL.md lean (1,500-2,000 words)\n- Use progressive disclosure (move details to references/)\n- Write in imperative/infinitive form\n- Reference supporting files clearly\n- Provide working examples\n- Create utility scripts for common operations\n- Study plugin-dev''''s skills as templates\n\n\u274c **DON''''T:**\n- Use second person anywhere\n- Have vague trigger conditions\n- Put everything in SKILL.md (>3,000 words without references/)\n- Write in second person (\"You should...\")\n- Leave resources unreferenced\n- Include broken or incomplete examples\n- Skip validation\n\n## Additional Resources\n\n### Study These Skills\n\nPlugin-dev''''s skills demonstrate best practices:\n- `../hook-development/` - Progressive disclosure, utilities\n- `../agent-development/` - AI-assisted creation, references\n- `../mcp-integration/` - Comprehensive references\n- `../plugin-settings/` - Real-world examples\n- `../command-development/` - Clear critical concepts\n- `../plugin-structure/` - Good organization\n\n### Reference Files\n\nFor complete skill-creator methodology:\n- **`references/skill-creator-original.md`** - Full original skill-creator content\n\n## Implementation Workflow\n\nTo create a skill for your plugin:\n\n1. **Understand use cases**: Identify concrete examples of skill usage\n2. **Plan resources**: Determine what scripts/references/examples needed\n3. **Create structure**: `mkdir -p skills/skill-name/{references,examples,scripts}`\n4. **Write SKILL.md**:\n   - Frontmatter with third-person description and trigger phrases\n   - Lean body (1,500-2,000 words) in imperative form\n   - Reference supporting files\n5. **Add resources**: Create references/, examples/, scripts/ as needed\n6. **Validate**: Check description, writing style, organization\n7. **Test**: Verify skill loads on expected triggers\n8. **Iterate**: Improve based on usage\n\nFocus on strong trigger descriptions, progressive disclosure, and imperative writing style for effective skills that load when needed and provide targeted guidance.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 15, clarity_structure: 12, error_handling: 9',
  'Score: 91, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-skill-development-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Plugin Structure - This skill should be used when the user asks to "create a plugin", "scaffold a plugin", "understand plugin structure", "organize plugin components", "',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Plugin Structure for Claude Code\n\n## Overview\n\nClaude Code plugins follow a standardized directory structure with automatic component discovery. Understanding this structure enables creating well-organized, maintainable plugins that integrate seamlessly with Claude Code.\n\n**Key concepts:**\n- Conventional directory layout for automatic discovery\n- Manifest-driven configuration in `.claude-plugin/plugin.json`\n- Component-based organization (commands, agents, skills, hooks)\n- Portable path references using `${CLAUDE_PLUGIN_ROOT}`\n- Explicit vs. auto-discovered component loading\n\n## Directory Structure\n\nEvery Claude Code plugin follows this organizational pattern:\n\n```\nplugin-name/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json          # Required: Plugin manifest\n\u251c\u2500\u2500 commands/                 # Slash commands (.md files)\n\u251c\u2500\u2500 agents/                   # Subagent definitions (.md files)\n\u251c\u2500\u2500 skills/                   # Agent skills (subdirectories)\n\u2502   \u2514\u2500\u2500 skill-name/\n\u2502       \u2514\u2500\u2500 SKILL.md         # Required for each skill\n\u251c\u2500\u2500 hooks/\n\u2502   \u2514\u2500\u2500 hooks.json           # Event handler configuration\n\u251c\u2500\u2500 .mcp.json                # MCP server definitions\n\u2514\u2500\u2500 scripts/                 # Helper scripts and utilities\n```\n\n**Critical rules:**\n\n1. **Manifest location**: The `plugin.json` manifest MUST be in `.claude-plugin/` directory\n2. **Component locations**: All component directories (commands, agents, skills, hooks) MUST be at plugin root level, NOT nested inside `.claude-plugin/`\n3. **Optional components**: Only create directories for components the plugin actually uses\n4. **Naming convention**: Use kebab-case for all directory and file names\n\n## Plugin Manifest (plugin.json)\n\nThe manifest defines plugin metadata and configuration. Located at `.claude-plugin/plugin.json`:\n\n### Required Fields\n\n```json\n{\n  \"name\": \"plugin-name\"\n}\n```\n\n**Name requirements:**\n- Use kebab-case format (lowercase with hyphens)\n- Must be unique across installed plugins\n- No spaces or special characters\n- Example: `code-review-assistant`, `test-runner`, `api-docs`\n\n### Recommended Metadata\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"version\": \"1.0.0\",\n  \"description\": \"Brief explanation of plugin purpose\",\n  \"author\": {\n    \"name\": \"Author Name\",\n    \"email\": \"author@example.com\",\n    \"url\": \"https://example.com\"\n  },\n  \"homepage\": \"https://docs.example.com\",\n  \"repository\": \"https://github.com/user/plugin-name\",\n  \"license\": \"MIT\",\n  \"keywords\": [\"testing\", \"automation\", \"ci-cd\"]\n}\n```\n\n**Version format**: Follow semantic versioning (MAJOR.MINOR.PATCH)\n**Keywords**: Use for plugin discovery and categorization\n\n### Component Path Configuration\n\nSpecify custom paths for components (supplements default directories):\n\n```json\n{\n  \"name\": \"plugin-name\",\n  \"commands\": \"./custom-commands\",\n  \"agents\": [\"./agents\", \"./specialized-agents\"],\n  \"hooks\": \"./config/hooks.json\",\n  \"mcpServers\": \"./.mcp.json\"\n}\n```\n\n**Important**: Custom paths supplement defaults\u2014they don''''t replace them. Components in both default directories and custom paths will load.\n\n**Path rules:**\n- Must be relative to plugin root\n- Must start with `./`\n- Cannot use absolute paths\n- Support arrays for multiple locations\n\n## Component Organization\n\n### Commands\n\n**Location**: `commands/` directory\n**Format**: Markdown files with YAML frontmatter\n**Auto-discovery**: All `.md` files in `commands/` load automatically\n\n**Example structure**:\n```\ncommands/\n\u251c\u2500\u2500 review.md        # /review command\n\u251c\u2500\u2500 test.md          # /test command\n\u2514\u2500\u2500 deploy.md        # /deploy command\n```\n\n**File format**:\n```markdown\n---\nname: command-name\ndescription: Command description\n---\n\nCommand implementation instructions...\n```\n\n**Usage**: Commands integrate as native slash commands in Claude Code\n\n### Agents\n\n**Location**: `agents/` directory\n**Format**: Markdown files with YAML frontmatter\n**Auto-discovery**: All `.md` files in `agents/` load automatically\n\n**Example structure**:\n```\nagents/\n\u251c\u2500\u2500 code-reviewer.md\n\u251c\u2500\u2500 test-generator.md\n\u2514\u2500\u2500 refactorer.md\n```\n\n**File format**:\n```markdown\n---\ndescription: Agent role and expertise\ncapabilities:\n  - Specific task 1\n  - Specific task 2\n---\n\nDetailed agent instructions and knowledge...\n```\n\n**Usage**: Users can invoke agents manually, or Claude Code selects them automatically based on task context\n\n### Skills\n\n**Location**: `skills/` directory with subdirectories per skill\n**Format**: Each skill in its own directory with `SKILL.md` file\n**Auto-discovery**: All `SKILL.md` files in skill subdirectories load automatically\n\n**Example structure**:\n```\nskills/\n\u251c\u2500\u2500 api-testing/\n\u2502   \u251c\u2500\u2500 SKILL.md\n\u2502   \u251c\u2500\u2500 scripts/\n\u2502   \u2502   \u2514\u2500\u2500 test-runner.py\n\u2502   \u2514\u2500\u2500 references/\n\u2502       \u2514\u2500\u2500 api-spec.md\n\u2514\u2500\u2500 database-migrations/\n    \u251c\u2500\u2500 SKILL.md\n    \u2514\u2500\u2500 examples/\n        \u2514\u2500\u2500 migration-template.sql\n```\n\n**SKILL.md format**:\n```markdown\n---\nname: Skill Name\ndescription: When to use this skill\nversion: 1.0.0\n---\n\nSkill instructions and guidance...\n```\n\n**Supporting files**: Skills can include scripts, references, examples, or assets in subdirectories\n\n**Usage**: Claude Code autonomously activates skills based on task context matching the description\n\n### Hooks\n\n**Location**: `hooks/hooks.json` or inline in `plugin.json`\n**Format**: JSON configuration defining event handlers\n**Registration**: Hooks register automatically when plugin enables\n\n**Example structure**:\n```\nhooks/\n\u251c\u2500\u2500 hooks.json           # Hook configuration\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 validate.sh      # Hook script\n    \u2514\u2500\u2500 check-style.sh   # Hook script\n```\n\n**Configuration format**:\n```json\n{\n  \"PreToolUse\": [{\n    \"matcher\": \"Write|Edit\",\n    \"hooks\": [{\n      \"type\": \"command\",\n      \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/hooks/scripts/validate.sh\",\n      \"timeout\": 30\n    }]\n  }]\n}\n```\n\n**Available events**: PreToolUse, PostToolUse, Stop, SubagentStop, SessionStart, SessionEnd, UserPromptSubmit, PreCompact, Notification\n\n**Usage**: Hooks execute automatically in response to Claude Code events\n\n### MCP Servers\n\n**Location**: `.mcp.json` at plugin root or inline in `plugin.json`\n**Format**: JSON configuration for MCP server definitions\n**Auto-start**: Servers start automatically when plugin enables\n\n**Example format**:\n```json\n{\n  \"mcpServers\": {\n    \"server-name\": {\n      \"command\": \"node\",\n      \"args\": [\"${CLAUDE_PLUGIN_ROOT}/servers/server.js\"],\n      \"env\": {\n        \"API_KEY\": \"${API_KEY}\"\n      }\n    }\n  }\n}\n```\n\n**Usage**: MCP servers integrate seamlessly with Claude Code''''s tool system\n\n## Portable Path References\n\n### ${CLAUDE_PLUGIN_ROOT}\n\nUse `${CLAUDE_PLUGIN_ROOT}` environment variable for all intra-plugin path references:\n\n```json\n{\n  \"command\": \"bash ${CLAUDE_PLUGIN_ROOT}/scripts/run.sh\"\n}\n```\n\n**Why it matters**: Plugins install in different locations depending on:\n- User installation method (marketplace, local, npm)\n- Operating system conventions\n- User preferences\n\n**Where to use it**:\n- Hook command paths\n- MCP server command arguments\n- Script execution references\n- Resource file paths\n\n**Never use**:\n- Hardcoded absolute paths (`/Users/name/plugins/...`)\n- Relative paths from working directory (`./scripts/...` in commands)\n- Home directory shortcuts (`~/plugins/...`)\n\n### Path Resolution Rules\n\n**In manifest JSON fields** (hooks, MCP servers):\n```json\n\"command\": \"${CLAUDE_PLUGIN_ROOT}/scripts/tool.sh\"\n```\n\n**In component files** (commands, agents, skills):\n```markdown\nReference scripts at: ${CLAUDE_PLUGIN_ROOT}/scripts/helper.py\n```\n\n**In executed scripts**:\n```bash\n#!/bin/bash\n# ${CLAUDE_PLUGIN_ROOT} available as environment variable\nsource \"${CLAUDE_PLUGIN_ROOT}/lib/common.sh\"\n```\n\n## File Naming Conventions\n\n### Component Files\n\n**Commands**: Use kebab-case `.md` files\n- `code-review.md` \u2192 `/code-review`\n- `run-tests.md` \u2192 `/run-tests`\n- `api-docs.md` \u2192 `/api-docs`\n\n**Agents**: Use kebab-case `.md` files describing role\n- `test-generator.md`\n- `code-reviewer.md`\n- `performance-analyzer.md`\n\n**Skills**: Use kebab-case directory names\n- `api-testing/`\n- `database-migrations/`\n- `error-handling/`\n\n### Supporting Files\n\n**Scripts**: Use descriptive kebab-case names with appropriate extensions\n- `validate-input.sh`\n- `generate-report.py`\n- `process-data.js`\n\n**Documentation**: Use kebab-case markdown files\n- `api-reference.md`\n- `migration-guide.md`\n- `best-practices.md`\n\n**Configuration**: Use standard names\n- `hooks.json`\n- `.mcp.json`\n- `plugin.json`\n\n## Auto-Discovery Mechanism\n\nClaude Code automatically discovers and loads components:\n\n1. **Plugin manifest**: Reads `.claude-plugin/plugin.json` when plugin enables\n2. **Commands**: Scans `commands/` directory for `.md` files\n3. **Agents**: Scans `agents/` directory for `.md` files\n4. **Skills**: Scans `skills/` for subdirectories containing `SKILL.md`\n5. **Hooks**: Loads configuration from `hooks/hooks.json` or manifest\n6. **MCP servers**: Loads configuration from `.mcp.json` or manifest\n\n**Discovery timing**:\n- Plugin installation: Components register with Claude Code\n- Plugin enable: Components become available for use\n- No restart required: Changes take effect on next Claude Code session\n\n**Override behavior**: Custom paths in `plugin.json` supplement (not replace) default directories\n\n## Best Practices\n\n### Organization\n\n1. **Logical grouping**: Group related components together\n   - Put test-related commands, agents, and skills together\n   - Create subdirectories in `scripts/` for different purposes\n\n2. **Minimal manifest**: Keep `plugin.json` lean\n   - Only specify custom paths when necessary\n   - Rely on auto-discovery for standard layouts\n   - Use inline configuration only for simple cases\n\n3. **Documentation**: Include README files\n   - Plugin root: Overall purpose and usage\n   - Component directories: Specific guidance\n   - Script directories: Usage and requirements\n\n### Naming\n\n1. **Consistency**: Use consistent naming across components\n   - If command is `test-runner`, name related agent `test-runner-agent`\n   - Match skill directory names to their purpose\n\n2. **Clarity**: Use descriptive names that indicate purpose\n   - Good: `api-integration-testing/`, `code-quality-checker.md`\n   - Avoid: `utils/`, `misc.md`, `temp.sh`\n\n3. **Length**: Balance brevity with clarity\n   - Commands: 2-3 words (`review-pr`, `run-ci`)\n   - Agents: Describe role clearly (`code-reviewer`, `test-generator`)\n   - Skills: Topic-focused (`error-handling`, `api-design`)\n\n### Portability\n\n1. **Always use ${CLAUDE_PLUGIN_ROOT}**: Never hardcode paths\n2. **Test on multiple systems**: Verify on macOS, Linux, Windows\n3. **Document dependencies**: List required tools and versions\n4. **Avoid system-specific features**: Use portable bash/Python constructs\n\n### Maintenance\n\n1. **Version consistently**: Update version in plugin.json for releases\n2. **Deprecate gracefully**: Mark old components clearly before removal\n3. **Document breaking changes**: Note changes affecting existing users\n4. **Test thoroughly**: Verify all components work after changes\n\n## Common Patterns\n\n### Minimal Plugin\n\nSingle command with no dependencies:\n```\nmy-plugin/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json    # Just name field\n\u2514\u2500\u2500 commands/\n    \u2514\u2500\u2500 hello.md       # Single command\n```\n\n### Full-Featured Plugin\n\nComplete plugin with all component types:\n```\nmy-plugin/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json\n\u251c\u2500\u2500 commands/          # User-facing commands\n\u251c\u2500\u2500 agents/            # Specialized subagents\n\u251c\u2500\u2500 skills/            # Auto-activating skills\n\u251c\u2500\u2500 hooks/             # Event handlers\n\u2502   \u251c\u2500\u2500 hooks.json\n\u2502   \u2514\u2500\u2500 scripts/\n\u251c\u2500\u2500 .mcp.json          # External integrations\n\u2514\u2500\u2500 scripts/           # Shared utilities\n```\n\n### Skill-Focused Plugin\n\nPlugin providing only skills:\n```\nmy-plugin/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json\n\u2514\u2500\u2500 skills/\n    \u251c\u2500\u2500 skill-one/\n    \u2502   \u2514\u2500\u2500 SKILL.md\n    \u2514\u2500\u2500 skill-two/\n        \u2514\u2500\u2500 SKILL.md\n```\n\n## Troubleshooting\n\n**Component not loading**:\n- Verify file is in correct directory with correct extension\n- Check YAML frontmatter syntax (commands, agents, skills)\n- Ensure skill has `SKILL.md` (not `README.md` or other name)\n- Confirm plugin is enabled in Claude Code settings\n\n**Path resolution errors**:\n- Replace all hardcoded paths with `${CLAUDE_PLUGIN_ROOT}`\n- Verify paths are relative and start with `./` in manifest\n- Check that referenced files exist at specified paths\n- Test with `echo $CLAUDE_PLUGIN_ROOT` in hook scripts\n\n**Auto-discovery not working**:\n- Confirm directories are at plugin root (not in `.claude-plugin/`)\n- Check file naming follows conventions (kebab-case, correct extensions)\n- Verify custom paths in manifest are correct\n- Restart Claude Code to reload plugin configuration\n\n**Conflicts between plugins**:\n- Use unique, descriptive component names\n- Namespace commands with plugin name if needed\n- Document potential conflicts in plugin README\n- Consider command prefixes for related functionality\n\n---\n\nFor detailed examples and advanced patterns, see files in `references/` and `examples/` directories.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 11',
  'Score: 93, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-plugin-structure-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'Plugin Settings - This skill should be used when the user asks about "plugin settings", "store plugin configuration", "user-configurable plugin", ".local.md files", "pl',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Plugin Settings Pattern for Claude Code Plugins\n\n## Overview\n\nPlugins can store user-configurable settings and state in `.claude/plugin-name.local.md` files within the project directory. This pattern uses YAML frontmatter for structured configuration and markdown content for prompts or additional context.\n\n**Key characteristics:**\n- File location: `.claude/plugin-name.local.md` in project root\n- Structure: YAML frontmatter + markdown body\n- Purpose: Per-project plugin configuration and state\n- Usage: Read from hooks, commands, and agents\n- Lifecycle: User-managed (not in git, should be in `.gitignore`)\n\n## File Structure\n\n### Basic Template\n\n```markdown\n---\nenabled: true\nsetting1: value1\nsetting2: value2\nnumeric_setting: 42\nlist_setting: [\"item1\", \"item2\"]\n---\n\n# Additional Context\n\nThis markdown body can contain:\n- Task descriptions\n- Additional instructions\n- Prompts to feed back to Claude\n- Documentation or notes\n```\n\n### Example: Plugin State File\n\n**.claude/my-plugin.local.md:**\n```markdown\n---\nenabled: true\nstrict_mode: false\nmax_retries: 3\nnotification_level: info\ncoordinator_session: team-leader\n---\n\n# Plugin Configuration\n\nThis plugin is configured for standard validation mode.\nContact @team-lead with questions.\n```\n\n## Reading Settings Files\n\n### From Hooks (Bash Scripts)\n\n**Pattern: Check existence and parse frontmatter**\n\n```bash\n#!/bin/bash\nset -euo pipefail\n\n# Define state file path\nSTATE_FILE=\".claude/my-plugin.local.md\"\n\n# Quick exit if file doesn''''t exist\nif [[ ! -f \"$STATE_FILE\" ]]; then\n  exit 0  # Plugin not configured, skip\nfi\n\n# Parse YAML frontmatter (between --- markers)\nFRONTMATTER=$(sed -n ''''/^---$/,/^---$/{ /^---$/d; p; }'''' \"$STATE_FILE\")\n\n# Extract individual fields\nENABLED=$(echo \"$FRONTMATTER\" | grep ''''^enabled:'''' | sed ''''s/enabled: *//'''' | sed ''''s/^\"\\(.*\\)\"$/\\1/'''')\nSTRICT_MODE=$(echo \"$FRONTMATTER\" | grep ''''^strict_mode:'''' | sed ''''s/strict_mode: *//'''' | sed ''''s/^\"\\(.*\\)\"$/\\1/'''')\n\n# Check if enabled\nif [[ \"$ENABLED\" != \"true\" ]]; then\n  exit 0  # Disabled\nfi\n\n# Use configuration in hook logic\nif [[ \"$STRICT_MODE\" == \"true\" ]]; then\n  # Apply strict validation\n  # ...\nfi\n```\n\nSee `examples/read-settings-hook.sh` for complete working example.\n\n### From Commands\n\nCommands can read settings files to customize behavior:\n\n```markdown\n---\ndescription: Process data with plugin\nallowed-tools: [\"Read\", \"Bash\"]\n---\n\n# Process Command\n\nSteps:\n1. Check if settings exist at `.claude/my-plugin.local.md`\n2. Read configuration using Read tool\n3. Parse YAML frontmatter to extract settings\n4. Apply settings to processing logic\n5. Execute with configured behavior\n```\n\n### From Agents\n\nAgents can reference settings in their instructions:\n\n```markdown\n---\nname: configured-agent\ndescription: Agent that adapts to project settings\n---\n\nCheck for plugin settings at `.claude/my-plugin.local.md`.\nIf present, parse YAML frontmatter and adapt behavior according to:\n- enabled: Whether plugin is active\n- mode: Processing mode (strict, standard, lenient)\n- Additional configuration fields\n```\n\n## Parsing Techniques\n\n### Extract Frontmatter\n\n```bash\n# Extract everything between --- markers\nFRONTMATTER=$(sed -n ''''/^---$/,/^---$/{ /^---$/d; p; }'''' \"$FILE\")\n```\n\n### Read Individual Fields\n\n**String fields:**\n```bash\nVALUE=$(echo \"$FRONTMATTER\" | grep ''''^field_name:'''' | sed ''''s/field_name: *//'''' | sed ''''s/^\"\\(.*\\)\"$/\\1/'''')\n```\n\n**Boolean fields:**\n```bash\nENABLED=$(echo \"$FRONTMATTER\" | grep ''''^enabled:'''' | sed ''''s/enabled: *//'''')\n# Compare: if [[ \"$ENABLED\" == \"true\" ]]; then\n```\n\n**Numeric fields:**\n```bash\nMAX=$(echo \"$FRONTMATTER\" | grep ''''^max_value:'''' | sed ''''s/max_value: *//'''')\n# Use: if [[ $MAX -gt 100 ]]; then\n```\n\n### Read Markdown Body\n\nExtract content after second `---`:\n\n```bash\n# Get everything after closing ---\nBODY=$(awk ''''/^---$/{i++; next} i>=2'''' \"$FILE\")\n```\n\n## Common Patterns\n\n### Pattern 1: Temporarily Active Hooks\n\nUse settings file to control hook activation:\n\n```bash\n#!/bin/bash\nSTATE_FILE=\".claude/security-scan.local.md\"\n\n# Quick exit if not configured\nif [[ ! -f \"$STATE_FILE\" ]]; then\n  exit 0\nfi\n\n# Read enabled flag\nFRONTMATTER=$(sed -n ''''/^---$/,/^---$/{ /^---$/d; p; }'''' \"$STATE_FILE\")\nENABLED=$(echo \"$FRONTMATTER\" | grep ''''^enabled:'''' | sed ''''s/enabled: *//'''')\n\nif [[ \"$ENABLED\" != \"true\" ]]; then\n  exit 0  # Disabled\nfi\n\n# Run hook logic\n# ...\n```\n\n**Use case:** Enable/disable hooks without editing hooks.json (requires restart).\n\n### Pattern 2: Agent State Management\n\nStore agent-specific state and configuration:\n\n**.claude/multi-agent-swarm.local.md:**\n```markdown\n---\nagent_name: auth-agent\ntask_number: 3.5\npr_number: 1234\ncoordinator_session: team-leader\nenabled: true\ndependencies: [\"Task 3.4\"]\n---\n\n# Task Assignment\n\nImplement JWT authentication for the API.\n\n**Success Criteria:**\n- Authentication endpoints created\n- Tests passing\n- PR created and CI green\n```\n\nRead from hooks to coordinate agents:\n\n```bash\nAGENT_NAME=$(echo \"$FRONTMATTER\" | grep ''''^agent_name:'''' | sed ''''s/agent_name: *//'''')\nCOORDINATOR=$(echo \"$FRONTMATTER\" | grep ''''^coordinator_session:'''' | sed ''''s/coordinator_session: *//'''')\n\n# Send notification to coordinator\ntmux send-keys -t \"$COORDINATOR\" \"Agent $AGENT_NAME completed task\" Enter\n```\n\n### Pattern 3: Configuration-Driven Behavior\n\n**.claude/my-plugin.local.md:**\n```markdown\n---\nvalidation_level: strict\nmax_file_size: 1000000\nallowed_extensions: [\".js\", \".ts\", \".tsx\"]\nenable_logging: true\n---\n\n# Validation Configuration\n\nStrict mode enabled for this project.\nAll writes validated against security policies.\n```\n\nUse in hooks or commands:\n\n```bash\nLEVEL=$(echo \"$FRONTMATTER\" | grep ''''^validation_level:'''' | sed ''''s/validation_level: *//'''')\n\ncase \"$LEVEL\" in\n  strict)\n    # Apply strict validation\n    ;;\n  standard)\n    # Apply standard validation\n    ;;\n  lenient)\n    # Apply lenient validation\n    ;;\nesac\n```\n\n## Creating Settings Files\n\n### From Commands\n\nCommands can create settings files:\n\n```markdown\n# Setup Command\n\nSteps:\n1. Ask user for configuration preferences\n2. Create `.claude/my-plugin.local.md` with YAML frontmatter\n3. Set appropriate values based on user input\n4. Inform user that settings are saved\n5. Remind user to restart Claude Code for hooks to recognize changes\n```\n\n### Template Generation\n\nProvide template in plugin README:\n\n```markdown\n## Configuration\n\nCreate `.claude/my-plugin.local.md` in your project:\n\n\\`\\`\\`markdown\n---\nenabled: true\nmode: standard\nmax_retries: 3\n---\n\n# Plugin Configuration\n\nYour settings are active.\n\\`\\`\\`\n\nAfter creating or editing, restart Claude Code for changes to take effect.\n```\n\n## Best Practices\n\n### File Naming\n\n\u2705 **DO:**\n- Use `.claude/plugin-name.local.md` format\n- Match plugin name exactly\n- Use `.local.md` suffix for user-local files\n\n\u274c **DON''''T:**\n- Use different directory (not `.claude/`)\n- Use inconsistent naming\n- Use `.md` without `.local` (might be committed)\n\n### Gitignore\n\nAlways add to `.gitignore`:\n\n```gitignore\n.claude/*.local.md\n.claude/*.local.json\n```\n\nDocument this in plugin README.\n\n### Defaults\n\nProvide sensible defaults when settings file doesn''''t exist:\n\n```bash\nif [[ ! -f \"$STATE_FILE\" ]]; then\n  # Use defaults\n  ENABLED=true\n  MODE=standard\nelse\n  # Read from file\n  # ...\nfi\n```\n\n### Validation\n\nValidate settings values:\n\n```bash\nMAX=$(echo \"$FRONTMATTER\" | grep ''''^max_value:'''' | sed ''''s/max_value: *//'''')\n\n# Validate numeric range\nif ! [[ \"$MAX\" =~ ^[0-9]+$ ]] || [[ $MAX -lt 1 ]] || [[ $MAX -gt 100 ]]; then\n  echo \"\u26a0\ufe0f  Invalid max_value in settings (must be 1-100)\" >&2\n  MAX=10  # Use default\nfi\n```\n\n### Restart Requirement\n\n**Important:** Settings changes require Claude Code restart.\n\nDocument in your README:\n\n```markdown\n## Changing Settings\n\nAfter editing `.claude/my-plugin.local.md`:\n1. Save the file\n2. Exit Claude Code\n3. Restart: `claude` or `cc`\n4. New settings will be loaded\n```\n\nHooks cannot be hot-swapped within a session.\n\n## Security Considerations\n\n### Sanitize User Input\n\nWhen writing settings files from user input:\n\n```bash\n# Escape quotes in user input\nSAFE_VALUE=$(echo \"$USER_INPUT\" | sed ''''s/\"/\\\\\"/g'''')\n\n# Write to file\ncat > \"$STATE_FILE\" <<EOF\n---\nuser_setting: \"$SAFE_VALUE\"\n---\nEOF\n```\n\n### Validate File Paths\n\nIf settings contain file paths:\n\n```bash\nFILE_PATH=$(echo \"$FRONTMATTER\" | grep ''''^data_file:'''' | sed ''''s/data_file: *//'''')\n\n# Check for path traversal\nif [[ \"$FILE_PATH\" == *\"..\"* ]]; then\n  echo \"\u26a0\ufe0f  Invalid path in settings (path traversal)\" >&2\n  exit 2\nfi\n```\n\n### Permissions\n\nSettings files should be:\n- Readable by user only (`chmod 600`)\n- Not committed to git\n- Not shared between users\n\n## Real-World Examples\n\n### multi-agent-swarm Plugin\n\n**.claude/multi-agent-swarm.local.md:**\n```markdown\n---\nagent_name: auth-implementation\ntask_number: 3.5\npr_number: 1234\ncoordinator_session: team-leader\nenabled: true\ndependencies: [\"Task 3.4\"]\nadditional_instructions: Use JWT tokens, not sessions\n---\n\n# Task: Implement Authentication\n\nBuild JWT-based authentication for the REST API.\nCoordinate with auth-agent on shared types.\n```\n\n**Hook usage (agent-stop-notification.sh):**\n- Checks if file exists (line 15-18: quick exit if not)\n- Parses frontmatter to get coordinator_session, agent_name, enabled\n- Sends notifications to coordinator if enabled\n- Allows quick activation/deactivation via `enabled: true/false`\n\n### ralph-wiggum Plugin\n\n**.claude/ralph-loop.local.md:**\n```markdown\n---\niteration: 1\nmax_iterations: 10\ncompletion_promise: \"All tests passing and build successful\"\n---\n\nFix all the linting errors in the project.\nMake sure tests pass after each fix.\n```\n\n**Hook usage (stop-hook.sh):**\n- Checks if file exists (line 15-18: quick exit if not active)\n- Reads iteration count and max_iterations\n- Extracts completion_promise for loop termination\n- Reads body as the prompt to feed back\n- Updates iteration count on each loop\n\n## Quick Reference\n\n### File Location\n\n```\nproject-root/\n\u2514\u2500\u2500 .claude/\n    \u2514\u2500\u2500 plugin-name.local.md\n```\n\n### Frontmatter Parsing\n\n```bash\n# Extract frontmatter\nFRONTMATTER=$(sed -n ''''/^---$/,/^---$/{ /^---$/d; p; }'''' \"$FILE\")\n\n# Read field\nVALUE=$(echo \"$FRONTMATTER\" | grep ''''^field:'''' | sed ''''s/field: *//'''' | sed ''''s/^\"\\(.*\\)\"$/\\1/'''')\n```\n\n### Body Parsing\n\n```bash\n# Extract body (after second ---)\nBODY=$(awk ''''/^---$/{i++; next} i>=2'''' \"$FILE\")\n```\n\n### Quick Exit Pattern\n\n```bash\nif [[ ! -f \".claude/my-plugin.local.md\" ]]; then\n  exit 0  # Not configured\nfi\n```\n\n## Additional Resources\n\n### Reference Files\n\nFor detailed implementation patterns:\n\n- **`references/parsing-techniques.md`** - Complete guide to parsing YAML frontmatter and markdown bodies\n- **`references/real-world-examples.md`** - Deep dive into multi-agent-swarm and ralph-wiggum implementations\n\n### Example Files\n\nWorking examples in `examples/`:\n\n- **`read-settings-hook.sh`** - Hook that reads and uses settings\n- **`create-settings-command.md`** - Command that creates settings file\n- **`example-settings.md`** - Template settings file\n\n### Utility Scripts\n\nDevelopment tools in `scripts/`:\n\n- **`validate-settings.sh`** - Validate settings file structure\n- **`parse-frontmatter.sh`** - Extract frontmatter fields\n\n## Implementation Workflow\n\nTo add settings to a plugin:\n\n1. Design settings schema (which fields, types, defaults)\n2. Create template file in plugin documentation\n3. Add gitignore entry for `.claude/*.local.md`\n4. Implement settings parsing in hooks/commands\n5. Use quick-exit pattern (check file exists, check enabled field)\n6. Document settings in plugin README with template\n7. Remind users that changes require Claude Code restart\n\nFocus on keeping settings simple and providing good defaults when settings file doesn''''t exist.\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 8',
  'Score: 90, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-plugin-dev-skills-plugin-settings-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'claude-opus-4-5-migration - Migrate prompts and code from Claude Sonnet 4.0, Sonnet 4.5, or Opus 4.1 to Opus 4.5. Use when the user wants to update their codebase, prompts, or AP',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Opus 4.5 Migration Guide\n\nOne-shot migration from Sonnet 4.0, Sonnet 4.5, or Opus 4.1 to Opus 4.5.\n\n## Migration Workflow\n\n1. Search codebase for model strings and API calls\n2. Update model strings to Opus 4.5 (see platform-specific strings below)\n3. Remove unsupported beta headers\n4. Add effort parameter set to `\"high\"` (see `references/effort.md`)\n5. Summarize all changes made\n6. Tell the user: \"If you encounter any issues with Opus 4.5, let me know and I can help adjust your prompts.\"\n\n## Model String Updates\n\nIdentify which platform the codebase uses, then replace model strings accordingly.\n\n### Unsupported Beta Headers\n\nRemove the `context-1m-2025-08-07` beta header if present\u2014it is not yet supported with Opus 4.5. Leave a comment noting this:\n\n```python\n# Note: 1M context beta (context-1m-2025-08-07) not yet supported with Opus 4.5\n```\n\n### Target Model Strings (Opus 4.5)\n\n| Platform | Opus 4.5 Model String |\n|----------|----------------------|\n| Anthropic API (1P) | `claude-opus-4-5-20251101` |\n| AWS Bedrock | `anthropic.claude-opus-4-5-20251101-v1:0` |\n| Google Vertex AI | `claude-opus-4-5@20251101` |\n| Azure AI Foundry | `claude-opus-4-5-20251101` |\n\n### Source Model Strings to Replace\n\n| Source Model | Anthropic API (1P) | AWS Bedrock | Google Vertex AI |\n|--------------|-------------------|-------------|------------------|\n| Sonnet 4.0 | `claude-sonnet-4-20250514` | `anthropic.claude-sonnet-4-20250514-v1:0` | `claude-sonnet-4@20250514` |\n| Sonnet 4.5 | `claude-sonnet-4-5-20250929` | `anthropic.claude-sonnet-4-5-20250929-v1:0` | `claude-sonnet-4-5@20250929` |\n| Opus 4.1 | `claude-opus-4-1-20250422` | `anthropic.claude-opus-4-1-20250422-v1:0` | `claude-opus-4-1@20250422` |\n\n**Do NOT migrate**: Any Haiku models (e.g., `claude-haiku-4-5-20251001`).\n\n## Prompt Adjustments\n\nOpus 4.5 has known behavioral differences from previous models. **Only apply these fixes if the user explicitly requests them or reports a specific issue.** By default, just update model strings.\n\n**Integration guidelines**: When adding snippets, don''''t just append them to prompts. Integrate them thoughtfully:\n- Use XML tags (e.g., `<code_guidelines>`, `<tool_usage>`) to organize additions\n- Match the style and structure of the existing prompt\n- Place snippets in logical locations (e.g., coding guidelines near other coding instructions)\n- If the prompt already uses XML tags, add new content within appropriate existing tags or create consistent new ones\n\n### 1. Tool Overtriggering\n\nOpus 4.5 is more responsive to system prompts. Aggressive language that prevented undertriggering on previous models may now cause overtriggering.\n\n**Apply if**: User reports tools being called too frequently or unnecessarily.\n\n**Find and soften**:\n- `CRITICAL:` \u2192 remove or soften\n- `You MUST...` \u2192 `You should...`\n- `ALWAYS do X` \u2192 `Do X`\n- `NEVER skip...` \u2192 `Don''''t skip...`\n- `REQUIRED` \u2192 remove or soften\n\nOnly apply to tool-triggering instructions. Leave other uses of emphasis alone.\n\n### 2. Over-Engineering Prevention\n\nOpus 4.5 tends to create extra files, add unnecessary abstractions, or build unrequested flexibility.\n\n**Apply if**: User reports unwanted files, excessive abstraction, or unrequested features. Add the snippet from `references/prompt-snippets.md`.\n\n### 3. Code Exploration\n\nOpus 4.5 can be overly conservative about exploring code, proposing solutions without reading files.\n\n**Apply if**: User reports the model proposing fixes without inspecting relevant code. Add the snippet from `references/prompt-snippets.md`.\n\n### 4. Frontend Design\n\n**Apply if**: User requests improved frontend design quality or reports generic-looking outputs.\n\nAdd the frontend aesthetics snippet from `references/prompt-snippets.md`.\n\n### 5. Thinking Sensitivity\n\nWhen extended thinking is not enabled (the default), Opus 4.5 is particularly sensitive to the word \"think\" and its variants. Extended thinking is enabled only if the API request contains a `thinking` parameter.\n\n**Apply if**: User reports issues related to \"thinking\" while extended thinking is not enabled (no `thinking` parameter in request).\n\nReplace \"think\" with alternatives like \"consider,\" \"believe,\" or \"evaluate.\"\n\n## Reference\n\nSee `references/prompt-snippets.md` for the full text of each snippet to add.\n\nSee `references/effort.md` for configuring the effort parameter (only if user requests it).\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 15, error_handling: 5, example_quality: 3',
  'Score: 73, Tier: TIER_2_GOOD, Strengths: workflow_structure: 20, problem_definition: 15, clarity_structure: 15',
  'https://skillsmp.com/skills/anthropics-claude-code-plugins-claude-opus-4-5-migration-skills-claude-opus-4-5-migration-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'payload - Use when working with Payload CMS projects (payload.config.ts, collections, fields, hooks, access control, Payload API). Use when debugging validation',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Payload CMS Application Development\n\nPayload is a Next.js native CMS with TypeScript-first architecture, providing admin panel, database management, REST/GraphQL APIs, authentication, and file storage.\n\n## Quick Reference\n\n| Task                     | Solution                                  | Details                                                                                                                          |\n| ------------------------ | ----------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------- |\n| Auto-generate slugs      | `slugField()`                             | [FIELDS.md#slug-field-helper](reference/FIELDS.md#slug-field-helper)                                                             |\n| Restrict content by user | Access control with query                 | [ACCESS-CONTROL.md#row-level-security-with-complex-queries](reference/ACCESS-CONTROL.md#row-level-security-with-complex-queries) |\n| Local API user ops       | `user` + `overrideAccess: false`          | [QUERIES.md#access-control-in-local-api](reference/QUERIES.md#access-control-in-local-api)                                       |\n| Draft/publish workflow   | `versions: { drafts: true }`              | [COLLECTIONS.md#versioning--drafts](reference/COLLECTIONS.md#versioning--drafts)                                                 |\n| Computed fields          | `virtual: true` with afterRead            | [FIELDS.md#virtual-fields](reference/FIELDS.md#virtual-fields)                                                                   |\n| Conditional fields       | `admin.condition`                         | [FIELDS.md#conditional-fields](reference/FIELDS.md#conditional-fields)                                                           |\n| Custom field validation  | `validate` function                       | [FIELDS.md#validation](reference/FIELDS.md#validation)                                                                           |\n| Filter relationship list | `filterOptions` on field                  | [FIELDS.md#relationship](reference/FIELDS.md#relationship)                                                                       |\n| Select specific fields   | `select` parameter                        | [QUERIES.md#field-selection](reference/QUERIES.md#field-selection)                                                               |\n| Auto-set author/dates    | beforeChange hook                         | [HOOKS.md#collection-hooks](reference/HOOKS.md#collection-hooks)                                                                 |\n| Prevent hook loops       | `req.context` check                       | [HOOKS.md#context](reference/HOOKS.md#context)                                                                                   |\n| Cascading deletes        | beforeDelete hook                         | [HOOKS.md#collection-hooks](reference/HOOKS.md#collection-hooks)                                                                 |\n| Geospatial queries       | `point` field with `near`/`within`        | [FIELDS.md#point-geolocation](reference/FIELDS.md#point-geolocation)                                                             |\n| Reverse relationships    | `join` field type                         | [FIELDS.md#join-fields](reference/FIELDS.md#join-fields)                                                                         |\n| Next.js revalidation     | Context control in afterChange            | [HOOKS.md#nextjs-revalidation-with-context-control](reference/HOOKS.md#nextjs-revalidation-with-context-control)                 |\n| Query by relationship    | Nested property syntax                    | [QUERIES.md#nested-properties](reference/QUERIES.md#nested-properties)                                                           |\n| Complex queries          | AND/OR logic                              | [QUERIES.md#andor-logic](reference/QUERIES.md#andor-logic)                                                                       |\n| Transactions             | Pass `req` to operations                  | [ADAPTERS.md#threading-req-through-operations](reference/ADAPTERS.md#threading-req-through-operations)                           |\n| Background jobs          | Jobs queue with tasks                     | [ADVANCED.md#jobs-queue](reference/ADVANCED.md#jobs-queue)                                                                       |\n| Custom API routes        | Collection custom endpoints               | [ADVANCED.md#custom-endpoints](reference/ADVANCED.md#custom-endpoints)                                                           |\n| Cloud storage            | Storage adapter plugins                   | [ADAPTERS.md#storage-adapters](reference/ADAPTERS.md#storage-adapters)                                                           |\n| Multi-language           | `localization` config + `localized: true` | [ADVANCED.md#localization](reference/ADVANCED.md#localization)                                                                   |\n| Create plugin            | `(options) => (config) => Config`         | [PLUGIN-DEVELOPMENT.md#plugin-architecture](reference/PLUGIN-DEVELOPMENT.md#plugin-architecture)                                 |\n| Plugin package setup     | Package structure with SWC                | [PLUGIN-DEVELOPMENT.md#plugin-package-structure](reference/PLUGIN-DEVELOPMENT.md#plugin-package-structure)                       |\n| Add fields to collection | Map collections, spread fields            | [PLUGIN-DEVELOPMENT.md#adding-fields-to-collections](reference/PLUGIN-DEVELOPMENT.md#adding-fields-to-collections)               |\n| Plugin hooks             | Preserve existing hooks in array          | [PLUGIN-DEVELOPMENT.md#adding-hooks](reference/PLUGIN-DEVELOPMENT.md#adding-hooks)                                               |\n| Check field type         | Type guard functions                      | [FIELD-TYPE-GUARDS.md](reference/FIELD-TYPE-GUARDS.md)                                                                           |\n\n## Quick Start\n\n```bash\nnpx create-payload-app@latest my-app\ncd my-app\npnpm dev\n```\n\n### Minimal Config\n\n```ts\nimport { buildConfig } from ''''payload''''\nimport { mongooseAdapter } from ''''@payloadcms/db-mongodb''''\nimport { lexicalEditor } from ''''@payloadcms/richtext-lexical''''\nimport path from ''''path''''\nimport { fileURLToPath } from ''''url''''\n\nconst filename = fileURLToPath(import.meta.url)\nconst dirname = path.dirname(filename)\n\nexport default buildConfig({\n  admin: {\n    user: ''''users'''',\n    importMap: {\n      baseDir: path.resolve(dirname),\n    },\n  },\n  collections: [Users, Media],\n  editor: lexicalEditor(),\n  secret: process.env.PAYLOAD_SECRET,\n  typescript: {\n    outputFile: path.resolve(dirname, ''''payload-types.ts''''),\n  },\n  db: mongooseAdapter({\n    url: process.env.DATABASE_URI,\n  }),\n})\n```\n\n## Essential Patterns\n\n### Basic Collection\n\n```ts\nimport type { CollectionConfig } from ''''payload''''\n\nexport const Posts: CollectionConfig = {\n  slug: ''''posts'''',\n  admin: {\n    useAsTitle: ''''title'''',\n    defaultColumns: [''''title'''', ''''author'''', ''''status'''', ''''createdAt''''],\n  },\n  fields: [\n    { name: ''''title'''', type: ''''text'''', required: true },\n    { name: ''''slug'''', type: ''''text'''', unique: true, index: true },\n    { name: ''''content'''', type: ''''richText'''' },\n    { name: ''''author'''', type: ''''relationship'''', relationTo: ''''users'''' },\n  ],\n  timestamps: true,\n}\n```\n\nFor more collection patterns (auth, upload, drafts, live preview), see [COLLECTIONS.md](reference/COLLECTIONS.md).\n\n### Common Fields\n\n```ts\n// Text field\n{ name: ''''title'''', type: ''''text'''', required: true }\n\n// Relationship\n{ name: ''''author'''', type: ''''relationship'''', relationTo: ''''users'''', required: true }\n\n// Rich text\n{ name: ''''content'''', type: ''''richText'''', required: true }\n\n// Select\n{ name: ''''status'''', type: ''''select'''', options: [''''draft'''', ''''published''''], defaultValue: ''''draft'''' }\n\n// Upload\n{ name: ''''image'''', type: ''''upload'''', relationTo: ''''media'''' }\n```\n\nFor all field types (array, blocks, point, join, virtual, conditional, etc.), see [FIELDS.md](reference/FIELDS.md).\n\n### Hook Example\n\n```ts\nexport const Posts: CollectionConfig = {\n  slug: ''''posts'''',\n  hooks: {\n    beforeChange: [\n      async ({ data, operation }) => {\n        if (operation === ''''create'''') {\n          data.slug = slugify(data.title)\n        }\n        return data\n      },\n    ],\n  },\n  fields: [{ name: ''''title'''', type: ''''text'''' }],\n}\n```\n\nFor all hook patterns, see [HOOKS.md](reference/HOOKS.md). For access control, see [ACCESS-CONTROL.md](reference/ACCESS-CONTROL.md).\n\n### Access Control with Type Safety\n\n```ts\nimport type { Access } from ''''payload''''\nimport type { User } from ''''@/payload-types''''\n\n// Type-safe access control\nexport const adminOnly: Access = ({ req }) => {\n  const user = req.user as User\n  return user?.roles?.includes(''''admin'''') || false\n}\n\n// Row-level access control\nexport const ownPostsOnly: Access = ({ req }) => {\n  const user = req.user as User\n  if (!user) return false\n  if (user.roles?.includes(''''admin'''')) return true\n\n  return {\n    author: { equals: user.id },\n  }\n}\n```\n\n### Query Example\n\n```ts\n// Local API\nconst posts = await payload.find({\n  collection: ''''posts'''',\n  where: {\n    status: { equals: ''''published'''' },\n    ''''author.name'''': { contains: ''''john'''' },\n  },\n  depth: 2,\n  limit: 10,\n  sort: ''''-createdAt'''',\n})\n\n// Query with populated relationships\nconst post = await payload.findByID({\n  collection: ''''posts'''',\n  id: ''''123'''',\n  depth: 2, // Populates relationships (default is 2)\n})\n// Returns: { author: { id: \"user123\", name: \"John\" } }\n\n// Without depth, relationships return IDs only\nconst post = await payload.findByID({\n  collection: ''''posts'''',\n  id: ''''123'''',\n  depth: 0,\n})\n// Returns: { author: \"user123\" }\n```\n\nFor all query operators and REST/GraphQL examples, see [QUERIES.md](reference/QUERIES.md).\n\n### Getting Payload Instance\n\n```ts\n// In API routes (Next.js)\nimport { getPayload } from ''''payload''''\nimport config from ''''@payload-config''''\n\nexport async function GET() {\n  const payload = await getPayload({ config })\n\n  const posts = await payload.find({\n    collection: ''''posts'''',\n  })\n\n  return Response.json(posts)\n}\n\n// In Server Components\nimport { getPayload } from ''''payload''''\nimport config from ''''@payload-config''''\n\nexport default async function Page() {\n  const payload = await getPayload({ config })\n  const { docs } = await payload.find({ collection: ''''posts'''' })\n\n  return <div>{docs.map(post => <h1 key={post.id}>{post.title}</h1>)}</div>\n}\n```\n\n## Security Pitfalls\n\n### 1. Local API Access Control (CRITICAL)\n\n**By default, Local API operations bypass ALL access control**, even when passing a user.\n\n```ts\n// \u274c SECURITY BUG: Passes user but ignores their permissions\nawait payload.find({\n  collection: ''''posts'''',\n  user: someUser, // Access control is BYPASSED!\n})\n\n// \u2705 SECURE: Actually enforces the user''''s permissions\nawait payload.find({\n  collection: ''''posts'''',\n  user: someUser,\n  overrideAccess: false, // REQUIRED for access control\n})\n```\n\n**When to use each:**\n\n- `overrideAccess: true` (default) - Server-side operations you trust (cron jobs, system tasks)\n- `overrideAccess: false` - When operating on behalf of a user (API routes, webhooks)\n\nSee [QUERIES.md#access-control-in-local-api](reference/QUERIES.md#access-control-in-local-api).\n\n### 2. Transaction Failures in Hooks\n\n**Nested operations in hooks without `req` break transaction atomicity.**\n\n```ts\n// \u274c DATA CORRUPTION RISK: Separate transaction\nhooks: {\n  afterChange: [\n    async ({ doc, req }) => {\n      await req.payload.create({\n        collection: ''''audit-log'''',\n        data: { docId: doc.id },\n        // Missing req - runs in separate transaction!\n      })\n    },\n  ]\n}\n\n// \u2705 ATOMIC: Same transaction\nhooks: {\n  afterChange: [\n    async ({ doc, req }) => {\n      await req.payload.create({\n        collection: ''''audit-log'''',\n        data: { docId: doc.id },\n        req, // Maintains atomicity\n      })\n    },\n  ]\n}\n```\n\nSee [ADAPTERS.md#threading-req-through-operations](reference/ADAPTERS.md#threading-req-through-operations).\n\n### 3. Infinite Hook Loops\n\n**Hooks triggering operations that trigger the same hooks create infinite loops.**\n\n```ts\n// \u274c INFINITE LOOP\nhooks: {\n  afterChange: [\n    async ({ doc, req }) => {\n      await req.payload.update({\n        collection: ''''posts'''',\n        id: doc.id,\n        data: { views: doc.views + 1 },\n        req,\n      }) // Triggers afterChange again!\n    },\n  ]\n}\n\n// \u2705 SAFE: Use context flag\nhooks: {\n  afterChange: [\n    async ({ doc, req, context }) => {\n      if (context.skipHooks) return\n\n      await req.payload.update({\n        collection: ''''posts'''',\n        id: doc.id,\n        data: { views: doc.views + 1 },\n        context: { skipHooks: true },\n        req,\n      })\n    },\n  ]\n}\n```\n\nSee [HOOKS.md#context](reference/HOOKS.md#context).\n\n## Project Structure\n\n```txt\nsrc/\n\u251c\u2500\u2500 app/\n\u2502   \u251c\u2500\u2500 (frontend)/\n\u2502   \u2502   \u2514\u2500\u2500 page.tsx\n\u2502   \u2514\u2500\u2500 (payload)/\n\u2502       \u2514\u2500\u2500 admin/[[...segments]]/page.tsx\n\u251c\u2500\u2500 collections/\n\u2502   \u251c\u2500\u2500 Posts.ts\n\u2502   \u251c\u2500\u2500 Media.ts\n\u2502   \u2514\u2500\u2500 Users.ts\n\u251c\u2500\u2500 globals/\n\u2502   \u2514\u2500\u2500 Header.ts\n\u251c\u2500\u2500 components/\n\u2502   \u2514\u2500\u2500 CustomField.tsx\n\u251c\u2500\u2500 hooks/\n\u2502   \u2514\u2500\u2500 slugify.ts\n\u2514\u2500\u2500 payload.config.ts\n```\n\n## Type Generation\n\n```ts\n// payload.config.ts\nexport default buildConfig({\n  typescript: {\n    outputFile: path.resolve(dirname, ''''payload-types.ts''''),\n  },\n  // ...\n})\n\n// Usage\nimport type { Post, User } from ''''@/payload-types''''\n```\n\n## Reference Documentation\n\n- **[FIELDS.md](reference/FIELDS.md)** - All field types, validation, admin options\n- **[FIELD-TYPE-GUARDS.md](reference/FIELD-TYPE-GUARDS.md)** - Type guards for runtime field type checking and narrowing\n- **[COLLECTIONS.md](reference/COLLECTIONS.md)** - Collection configs, auth, upload, drafts, live preview\n- **[HOOKS.md](reference/HOOKS.md)** - Collection hooks, field hooks, context patterns\n- **[ACCESS-CONTROL.md](reference/ACCESS-CONTROL.md)** - Collection, field, global access control, RBAC, multi-tenant\n- **[ACCESS-CONTROL-ADVANCED.md](reference/ACCESS-CONTROL-ADVANCED.md)** - Context-aware, time-based, subscription-based access, factory functions, templates\n- **[QUERIES.md](reference/QUERIES.md)** - Query operators, Local/REST/GraphQL APIs\n- **[ENDPOINTS.md](reference/ENDPOINTS.md)** - Custom API endpoints: authentication, helpers, request/response patterns\n- **[ADAPTERS.md](reference/ADAPTERS.md)** - Database, storage, email adapters, transactions\n- **[ADVANCED.md](reference/ADVANCED.md)** - Authentication, jobs, endpoints, components, plugins, localization\n- **[PLUGIN-DEVELOPMENT.md](reference/PLUGIN-DEVELOPMENT.md)** - Plugin architecture, monorepo structure, patterns, best practices\n\n## Resources\n\n- llms-full.txt: <https://payloadcms.com/llms-full.txt>\n- Docs: <https://payloadcms.com/docs>\n- GitHub: <https://github.com/payloadcms/payload>\n- Examples: <https://github.com/payloadcms/payload/tree/main/examples>\n- Templates: <https://github.com/payloadcms/payload/tree/main/templates>\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 15, workflow_structure: 10, error_handling: 6',
  'Score: 78, Tier: TIER_1_EXCELLENT, Strengths: example_quality: 17, problem_definition: 15, clarity_structure: 15',
  'https://skillsmp.com/skills/payloadcms-payload-tools-claude-plugin-skills-payload-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'creating-financial-models - This skill provides an advanced financial modeling suite with DCF analysis, sensitivity testing, Monte Carlo simulations, and scenario planning for in',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Financial Modeling Suite\n\nA comprehensive financial modeling toolkit for investment analysis, valuation, and risk assessment using industry-standard methodologies.\n\n## Core Capabilities\n\n### 1. Discounted Cash Flow (DCF) Analysis\n- Build complete DCF models with multiple growth scenarios\n- Calculate terminal values using perpetuity growth and exit multiple methods\n- Determine weighted average cost of capital (WACC)\n- Generate enterprise and equity valuations\n\n### 2. Sensitivity Analysis\n- Test key assumptions impact on valuation\n- Create data tables for multiple variables\n- Generate tornado charts for sensitivity ranking\n- Identify critical value drivers\n\n### 3. Monte Carlo Simulation\n- Run thousands of scenarios with probability distributions\n- Model uncertainty in key inputs\n- Generate confidence intervals for valuations\n- Calculate probability of achieving targets\n\n### 4. Scenario Planning\n- Build best/base/worst case scenarios\n- Model different economic environments\n- Test strategic alternatives\n- Compare outcome probabilities\n\n## Input Requirements\n\n### For DCF Analysis\n- Historical financial statements (3-5 years)\n- Revenue growth assumptions\n- Operating margin projections\n- Capital expenditure forecasts\n- Working capital requirements\n- Terminal growth rate or exit multiple\n- Discount rate components (risk-free rate, beta, market premium)\n\n### For Sensitivity Analysis\n- Base case model\n- Variable ranges to test\n- Key metrics to track\n\n### For Monte Carlo Simulation\n- Probability distributions for uncertain variables\n- Correlation assumptions between variables\n- Number of iterations (typically 1,000-10,000)\n\n### For Scenario Planning\n- Scenario definitions and assumptions\n- Probability weights for scenarios\n- Key performance indicators to track\n\n## Output Formats\n\n### DCF Model Output\n- Complete financial projections\n- Free cash flow calculations\n- Terminal value computation\n- Enterprise and equity value summary\n- Valuation multiples implied\n- Excel workbook with full model\n\n### Sensitivity Analysis Output\n- Sensitivity tables showing value ranges\n- Tornado chart of key drivers\n- Break-even analysis\n- Charts showing relationships\n\n### Monte Carlo Output\n- Probability distribution of valuations\n- Confidence intervals (e.g., 90%, 95%)\n- Statistical summary (mean, median, std dev)\n- Risk metrics (VaR, probability of loss)\n\n### Scenario Planning Output\n- Scenario comparison table\n- Probability-weighted expected values\n- Decision tree visualization\n- Risk-return profiles\n\n## Model Types Supported\n\n1. **Corporate Valuation**\n   - Mature companies with stable cash flows\n   - Growth companies with J-curve projections\n   - Turnaround situations\n\n2. **Project Finance**\n   - Infrastructure projects\n   - Real estate developments\n   - Energy projects\n\n3. **M&A Analysis**\n   - Acquisition valuations\n   - Synergy modeling\n   - Accretion/dilution analysis\n\n4. **LBO Models**\n   - Leveraged buyout analysis\n   - Returns analysis (IRR, MOIC)\n   - Debt capacity assessment\n\n## Best Practices Applied\n\n### Modeling Standards\n- Consistent formatting and structure\n- Clear assumption documentation\n- Separation of inputs, calculations, outputs\n- Error checking and validation\n- Version control and change tracking\n\n### Valuation Principles\n- Use multiple valuation methods for triangulation\n- Apply appropriate risk adjustments\n- Consider market comparables\n- Validate against trading multiples\n- Document key assumptions clearly\n\n### Risk Management\n- Identify and quantify key risks\n- Use probability-weighted scenarios\n- Stress test extreme cases\n- Consider correlation effects\n- Provide confidence intervals\n\n## Example Usage\n\n\"Build a DCF model for this technology company using the attached financials\"\n\n\"Run a Monte Carlo simulation on this acquisition model with 5,000 iterations\"\n\n\"Create sensitivity analysis showing impact of growth rate and WACC on valuation\"\n\n\"Develop three scenarios for this expansion project with probability weights\"\n\n## Scripts Included\n\n- `dcf_model.py`: Complete DCF valuation engine\n- `sensitivity_analysis.py`: Sensitivity testing framework\n\n## Limitations and Disclaimers\n\n- Models are only as good as their assumptions\n- Past performance doesn''''t guarantee future results\n- Market conditions can change rapidly\n- Regulatory and tax changes may impact results\n- Professional judgment required for interpretation\n- Not a substitute for professional financial advice\n\n## Quality Checks\n\nThe model automatically performs:\n1. Balance sheet balancing checks\n2. Cash flow reconciliation\n3. Circular reference resolution\n4. Sensitivity bound checking\n5. Statistical validation of Monte Carlo results\n\n## Updates and Maintenance\n\n- Models use latest financial theory and practices\n- Regular updates for market parameter defaults\n- Incorporation of regulatory changes\n- Continuous improvement based on usage patterns\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'problem_definition: 10, example_quality: 5, error_handling: 2',
  'Score: 59, Tier: TIER_2_GOOD, Strengths: workflow_structure: 15, completeness: 15, clarity_structure: 12',
  'https://skillsmp.com/skills/anthropics-claude-cookbooks-skills-custom-skills-creating-financial-models-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'cookbook-audit - Audit an Anthropic Cookbook notebook based on a rubric. Use whenever a notebook review or audit is requested.',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Cookbook Audit\n\n## Instructions\n\nReview the requested Cookbook notebook using the guidelines and rubrics in `style_guide.md`. Provide a score based on scoring guidelines and recommendations on improving the cookbook.\n\nThe style guide provides detailed templates and examples for:\n- Problem-focused introductions with Terminal Learning Objectives (TLOs) and Enabling Learning Objectives (ELOs)\n- Prerequisites and setup patterns\n- Core content structure\n- Conclusions that map back to learning objectives\n\n**IMPORTANT**: Always read `style_guide.md` first before conducting an audit. The style guide contains the canonical templates and good/bad examples to reference.\n\n## Workflow\n\nFollow these steps for a comprehensive audit:\n\n1. **Read the style guide**: First review `style_guide.md` to understand current best practices\n2. **Identify the notebook**: Ask user for path if not provided\n3. **Run automated checks**: Use `python3 validate_notebook.py <path>` to catch technical issues and generate markdown\n   - The script automatically runs detect-secrets to scan for hardcoded API keys and credentials\n   - Uses custom patterns defined in `scripts/detect-secrets/plugins.py`\n   - Checks against baseline at `scripts/detect-secrets/.secrets.baseline`\n4. **Review markdown output**: The script generates a markdown file in the `tmp/` folder for easier review (saves context vs raw .ipynb)\n   - The tmp/ folder is gitignored to avoid committing review artifacts\n   - Markdown includes code cells but excludes outputs for cleaner review\n5. **Manual review**: Read through the markdown version evaluating against style guide and rubric\n6. **Score each dimension**: Apply scoring guidelines objectively\n7. **Generate report**: Follow the audit report format below\n8. **Provide specific examples**: Show concrete improvements with line references using the style guide templates\n\n## Audit Report Format\n\nPresent your audit using this structure:\n\n### Executive Summary\n- **Overall Score**: X/20\n- **Key Strengths** (2-3 bullet points)\n- **Critical Issues** (2-3 bullet points)\n\n### Detailed Scoring\n\n#### 1. Narrative Quality: X/5\n[Brief justification with specific examples]\n\n#### 2. Code Quality: X/5\n[Brief justification with specific examples]\n\n#### 3. Technical Accuracy: X/5\n[Brief justification with specific examples]\n\n#### 4. Actionability & Understanding: X/5\n[Brief justification with specific examples]\n\n### Specific Recommendations\n\n[Prioritized, actionable list of improvements with references to specific sections]\n\n### Examples & Suggestions\n\n[Show specific excerpts from the notebook with concrete suggestions for improvement]\n\n## Quick Reference Checklist\n\nUse this to ensure comprehensive coverage:\n\n**Introduction** (See style_guide.md Section 1)\n- [ ] Hooks with the problem being solved (1-2 sentences)\n- [ ] Explains why it matters (1-2 sentences)\n- [ ] Lists learning objectives as bullet points (2-4 TLOs/ELOs)\n- [ ] Focuses on value delivered, not machinery built\n- [ ] Optional: mentions broader applications (1 sentence)\n\n**Prerequisites & Setup** (See style_guide.md Section 2)\n- [ ] Lists required knowledge clearly\n- [ ] Lists required tools (Python version, API keys)\n- [ ] Mentions recommended background if applicable\n- [ ] Uses %%capture for pip install to suppress output\n- [ ] Uses dotenv.load_dotenv() not os.environ\n- [ ] Defines MODEL constant at top\n- [ ] Groups related installs in single command\n\n**Structure & Organization**\n- [ ] Has logical section progression\n- [ ] Each section teaches through demonstration\n- [ ] Code blocks have explanatory text before them\n- [ ] Includes what we learned after code blocks\n- [ ] Uses headers to break up sections\n\n**Conclusion** (See style_guide.md Section 4)\n- [ ] Maps back to learning objectives\n- [ ] Summarizes what was accomplished\n- [ ] Suggests ways to apply lessons to user''''s context\n- [ ] Points to next steps or related resources\n\n**Code Quality**\n- [ ] All code blocks have explanatory text before them\n- [ ] No hardcoded API keys (automatically checked by detect-secrets)\n- [ ] Meaningful variable names\n- [ ] Comments explain \"why\" not \"what\"\n- [ ] Follows language best practices\n- [ ] Model name defined as constant at top of notebook\n\n**Output Management**\n- [ ] pip install logs suppressed with %%capture\n- [ ] No verbose debug output\n- [ ] Shows relevant API responses\n- [ ] Stack traces only when demonstrating error handling\n\n**Content Quality**\n- [ ] Explains why approaches work\n- [ ] Discusses when to use this approach\n- [ ] Mentions limitations/considerations\n- [ ] Provides transferable knowledge\n- [ ] Appropriate model selection\n\n**Technical Requirements**\n- [ ] Executable without modification (except API keys)\n- [ ] Uses non-deprecated API patterns\n- [ ] Uses valid model names (claude-sonnet-4-5, claude-haiku-4-5, claude-opus-4-1)\n- [ ] Model name defined as constant at top of notebook\n- [ ] Includes dependency specifications\n- [ ] Assigned to primary category\n- [ ] Has relevant tags\n\n### Content Philosophy: Action + Understanding\n\nCookbooks are primarily action-oriented but strategically incorporate understanding and informed by Diataxis framework.\n\n**Core Principles:**\n- **Practical focus**: Show users how to accomplish specific tasks with working code\n- **Problem-first framing**: Lead with the problem being solved and value delivered, not the machinery\n- **Builder''''s perspective**: Written from the user''''s point of view, solving real problems\n- **Agency-building**: Help users understand why approaches work, not just how\n- **Transferable knowledge**: Teach patterns and principles that apply beyond the specific example\n- **Critical thinking**: Encourage users to question outputs, recognize limitations, make informed choices\n- **Learning contracts**: State learning objectives upfront, then map back to them in conclusions\n\n### What Makes a Good Cookbook\n\nA good cookbook doesn''''t just help users solve today''''s problem, it also helps them understand the underlying principles behind the solutions, encouraging them to recognize when and how to adapt approaches. Users will be able to make more informed decisions about AI system design, develop judgement about model outputs, and build skills that transfer to future AI systems.\n\n### What Cookbooks Are NOT\n\nCookbooks are not pure tutorials: We assume users have basic technical skills and API familiarity. We clearly state prerequisites in our cookbooks, and direct users to the Academy to learn more on topics.\nThey are not comprehensive explanations: We don''''t teach transformer architecture or probability theory. We need to understand that our users are following our cookbooks to solve problems they are facing today. They are busy, in the midst of learning or building, and want to be able to use what they learn to solve their immediate needs.\nCookbooks are not reference docs: We don''''t exhaustively document every parameter, we link to appropriate resources in our documentation as needed.\nCookbooks are not simple tips and tricks: We don''''t teach \"hacks\" that only work for the current model generation. We don''''t over-promise and under-deliver.\nCookbooks are not production-ready code: They showcase use cases and capabilities, not production patterns. Excessive error handling is not required.\n\n### Style Guidelines\n\n#### Voice & Tone\n- Educational and agency-building\n- Professional but approachable\n- Respectful of user intelligence and time\n- Either second person (\"you\") or first person plural (\"we\") - be consistent within a notebook\n\n#### Writing Quality\n- Clear, concise explanations\n- Active voice preferred\n- Short paragraphs (3-5 sentences)\n- Avoid jargon without definition\n- Use headers to break up sections\n\n#### Code Presentation\n- **Always explain before showing**: Every code block should be preceded by explanatory text\n- **Explain after running**: Include what we learned after code blocks execute\n- **Comments explain why, not what**: Use meaningful variable names\n- **Use constants**: Define MODEL as a constant at the top\n- **Good habits**: Use `dotenv.load_dotenv()` instead of `os.environ`\n\n#### Output Handling\n**Remove extraneous output** with %%capture:\n- pip install logs (always suppress these)\n- Verbose debug statements\n- Lengthy stack traces (unless demonstrating error handling)\n\n**Show relevant output**:\n- API responses that demonstrate functionality\n- Examples of successful execution\n\n### Structural Requirements\n\n**See style_guide.md for detailed templates and examples**\n\n#### 1. Introduction (Required)\nMust include:\n- **Problem hook** (1-2 sentences): What problem are we solving?\n- **Why it matters** (1-2 sentences): Why is this important?\n- **Learning objectives** (2-4 bullet points): \"By the end of this cookbook, you''''ll be able to...\"\n  - Use action verbs (Build, Implement, Deploy, etc.)\n  - Be specific about capabilities\n  - Include context/constraints\n- **Optional**: Broader applications (1 sentence)\n\n\u274c **Avoid**: Leading with machinery (\"We will build a research agent...\")\n\u2705 **Do**: Lead with problem/value (\"Your team spends hours triaging CI failures...\")\n\n#### 2. Prerequisites & Setup (Required)\nMust include:\n- **Required Knowledge**: Technical skills needed\n- **Required Tools**: Python version, API keys with links\n- **Recommended**: Optional background that helps\n- **Setup**: Step-by-step with explanations\n  - Use `%%capture` for pip installs\n  - Use `dotenv.load_dotenv()` not `os.environ`\n  - Define `MODEL` constant at top\n\n#### 3. Main Content (Required)\nOrganized by logical steps or phases, each with:\n- Clear section headers\n- **Explanatory text before code blocks** (what we''''re about to do)\n- Code examples\n- **Explanatory text after code blocks** (what we learned)\n- Expected outputs (where relevant)\n- Optional: Understanding callouts (why it works, when to use, limitations)\n\n#### 4. Conclusion (Recommended)\nMust include:\n- **Recap**: Map back to learning objectives\n- **What was accomplished**: Summary of key points\n- **Application guidance**: How to apply lessons to user''''s context\n- **Next steps**: Related resources or ideas to pursue\n\n\u274c **Avoid**: Generic summaries (\"We''''ve demonstrated how the SDK enables...\")\n\u2705 **Do**: Actionable guidance (\"Consider applying this to X... Next, try Y...\")\n\n#### Optional Sections\n- **How It Works**: Brief explanation of underlying mechanism\n- **When to Use This**: Appropriate use cases and contexts\n- **Limitations & Considerations**: Caveats, failure modes, constraints\n- **Troubleshooting**: Common issues and solutions\n- **Variations**: Alternative approaches or extensions\n- **Performance Notes**: Optimization considerations\n- **Further Reading**: Links to relevant docs, papers, or deeper explanations\n\n### Common Anti-Patterns to Flag\n\nRefer to style_guide.md for detailed good/bad examples. Watch for these issues:\n\n#### Introduction Anti-Patterns\n\u274c Leading with machinery: \"We will build a research agent using the Claude SDK...\"\n\u274c Feature dumps: Listing SDK methods or tool capabilities\n\u274c Vague learning objectives: \"Learn about agents\" or \"Understand the API\"\n\u2705 Problem-first framing with specific, actionable learning objectives\n\n#### Setup Anti-Patterns\n\u274c Noisy pip install output without `%%capture`\n\u274c Multiple separate pip install commands\n\u274c Using `os.environ[\"API_KEY\"] = \"your_key\"` instead of dotenv\n\u274c Hardcoding model names throughout instead of using a MODEL constant\n\u2705 Clean setup with grouped installs, dotenv, and constants\n\n#### Code Presentation Anti-Patterns\n\u274c Code blocks without explanatory text before them\n\u274c No explanation of what we learned after running code\n\u274c Comments that explain \"what\" the code does (code should be self-documenting)\n\u274c Over-explaining obvious code\n\u2705 Context before code, insights after code, comments explain \"why\"\n\n#### Conclusion Anti-Patterns\n\u274c Generic summaries: \"We''''ve demonstrated how the SDK enables...\"\n\u274c Simply restating what the notebook did without guidance\n\u274c Not mapping back to the stated learning objectives\n\u2705 Actionable guidance on applying lessons to user''''s specific context"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 12, clarity_structure: 12, example_quality: 2',
  'Score: 76, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, completeness: 15',
  'https://skillsmp.com/skills/anthropics-claude-cookbooks-claude-skills-cookbook-audit-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'analyzing-financial-statements - This skill calculates key financial ratios and metrics from financial statement data for investment analysis',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Financial Ratio Calculator Skill\n\nThis skill provides comprehensive financial ratio analysis for evaluating company performance, profitability, liquidity, and valuation.\n\n## Capabilities\n\nCalculate and interpret:\n- **Profitability Ratios**: ROE, ROA, Gross Margin, Operating Margin, Net Margin\n- **Liquidity Ratios**: Current Ratio, Quick Ratio, Cash Ratio\n- **Leverage Ratios**: Debt-to-Equity, Interest Coverage, Debt Service Coverage\n- **Efficiency Ratios**: Asset Turnover, Inventory Turnover, Receivables Turnover\n- **Valuation Ratios**: P/E, P/B, P/S, EV/EBITDA, PEG\n- **Per-Share Metrics**: EPS, Book Value per Share, Dividend per Share\n\n## How to Use\n\n1. **Input Data**: Provide financial statement data (income statement, balance sheet, cash flow)\n2. **Select Ratios**: Specify which ratios to calculate or use \"all\" for comprehensive analysis\n3. **Interpretation**: The skill will calculate ratios and provide industry-standard interpretations\n\n## Input Format\n\nFinancial data can be provided as:\n- CSV with financial line items\n- JSON with structured financial statements\n- Text description of key financial figures\n- Excel files with financial statements\n\n## Output Format\n\nResults include:\n- Calculated ratios with values\n- Industry benchmark comparisons (when available)\n- Trend analysis (if multiple periods provided)\n- Interpretation and insights\n- Excel report with formatted results\n\n## Example Usage\n\n\"Calculate key financial ratios for this company based on the attached financial statements\"\n\n\"What''''s the P/E ratio if the stock price is $50 and annual earnings are $2.50 per share?\"\n\n\"Analyze the liquidity position using the balance sheet data\"\n\n## Scripts\n\n- `calculate_ratios.py`: Main calculation engine for all financial ratios\n- `interpret_ratios.py`: Provides interpretation and benchmarking\n\n## Best Practices\n\n1. Always validate data completeness before calculations\n2. Handle missing values appropriately (use industry averages or exclude)\n3. Consider industry context when interpreting ratios\n4. Include period comparisons for trend analysis\n5. Flag unusual or concerning ratios\n\n## Limitations\n\n- Requires accurate financial data\n- Industry benchmarks are general guidelines\n- Some ratios may not apply to all industries\n- Historical data doesn''''t guarantee future performance\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 11, problem_definition: 10, example_quality: 3',
  'Score: 66, Tier: TIER_2_GOOD, Strengths: workflow_structure: 15, completeness: 15, clarity_structure: 12',
  'https://skillsmp.com/skills/anthropics-claude-cookbooks-skills-custom-skills-analyzing-financial-statements-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'applying-brand-guidelines - This skill applies consistent corporate branding and styling to all generated documents including colors, fonts, layouts, and messaging',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Corporate Brand Guidelines Skill\n\nThis skill ensures all generated documents adhere to corporate brand standards for consistent, professional communication.\n\n## Brand Identity\n\n### Company: Acme Corporation\n**Tagline**: \"Innovation Through Excellence\"\n**Industry**: Technology Solutions\n\n## Visual Standards\n\n### Color Palette\n\n**Primary Colors**:\n- **Acme Blue**: #0066CC (RGB: 0, 102, 204) - Headers, primary buttons\n- **Acme Navy**: #003366 (RGB: 0, 51, 102) - Text, accents\n- **White**: #FFFFFF - Backgrounds, reverse text\n\n**Secondary Colors**:\n- **Success Green**: #28A745 (RGB: 40, 167, 69) - Positive metrics\n- **Warning Amber**: #FFC107 (RGB: 255, 193, 7) - Cautions\n- **Error Red**: #DC3545 (RGB: 220, 53, 69) - Negative values\n- **Neutral Gray**: #6C757D (RGB: 108, 117, 125) - Secondary text\n\n### Typography\n\n**Primary Font Family**: Segoe UI, system-ui, -apple-system, sans-serif\n\n**Font Hierarchy**:\n- **H1**: 32pt, Bold, Acme Blue\n- **H2**: 24pt, Semibold, Acme Navy\n- **H3**: 18pt, Semibold, Acme Navy\n- **Body**: 11pt, Regular, Acme Navy\n- **Caption**: 9pt, Regular, Neutral Gray\n\n### Logo Usage\n\n- Position: Top-left corner on first page/slide\n- Size: 120px width (maintain aspect ratio)\n- Clear space: Minimum 20px padding on all sides\n- Never distort, rotate, or apply effects\n\n## Document Standards\n\n### PowerPoint Presentations\n\n**Slide Templates**:\n1. **Title Slide**: Company logo, presentation title, date, presenter\n2. **Section Divider**: Section title with blue background\n3. **Content Slide**: Title bar with blue background, white content area\n4. **Data Slide**: For charts/graphs, maintain color palette\n\n**Layout Rules**:\n- Margins: 0.5 inches all sides\n- Title position: Top 15% of slide\n- Bullet indentation: 0.25 inches per level\n- Maximum 6 bullet points per slide\n- Charts use brand colors exclusively\n\n### Excel Spreadsheets\n\n**Formatting Standards**:\n- **Headers**: Row 1, Bold, White text on Acme Blue background\n- **Subheaders**: Bold, Acme Navy text\n- **Data cells**: Regular, Acme Navy text\n- **Borders**: Thin, Neutral Gray\n- **Alternating rows**: Light gray (#F8F9FA) for readability\n\n**Chart Defaults**:\n- Primary series: Acme Blue\n- Secondary series: Success Green\n- Gridlines: Neutral Gray, 0.5pt\n- No 3D effects or gradients\n\n### PDF Documents\n\n**Page Layout**:\n- **Header**: Company logo left, document title center, page number right\n- **Footer**: Copyright notice left, date center, classification right\n- **Margins**: 1 inch all sides\n- **Line spacing**: 1.15\n- **Paragraph spacing**: 12pt after\n\n**Section Formatting**:\n- Main headings: Acme Blue, 16pt, bold\n- Subheadings: Acme Navy, 14pt, semibold\n- Body text: Acme Navy, 11pt, regular\n\n## Content Guidelines\n\n### Tone of Voice\n\n- **Professional**: Formal but approachable\n- **Clear**: Avoid jargon, use simple language\n- **Active**: Use active voice, action-oriented\n- **Positive**: Focus on solutions and benefits\n\n### Standard Phrases\n\n**Opening Statements**:\n- \"At Acme Corporation, we...\"\n- \"Our commitment to innovation...\"\n- \"Delivering excellence through...\"\n\n**Closing Statements**:\n- \"Thank you for your continued partnership.\"\n- \"We look forward to serving your needs.\"\n- \"Together, we achieve excellence.\"\n\n### Data Presentation\n\n**Numbers**:\n- Use comma separators for thousands\n- Currency: $X,XXX.XX format\n- Percentages: XX.X% (one decimal)\n- Dates: Month DD, YYYY\n\n**Tables**:\n- Headers in brand blue\n- Alternating row colors\n- Right-align numbers\n- Left-align text\n\n## Quality Standards\n\n### Before Finalizing\n\nAlways ensure:\n1. Logo is properly placed and sized\n2. All colors match brand palette exactly\n3. Fonts are consistent throughout\n4. No typos or grammatical errors\n5. Data is accurately presented\n6. Professional tone maintained\n\n### Prohibited Elements\n\nNever use:\n- Clip art or stock photos without approval\n- Comic Sans, Papyrus, or decorative fonts\n- Rainbow colors or gradients\n- Animations or transitions (unless specified)\n- Competitor branding or references\n\n## Application Instructions\n\nWhen creating any document:\n1. Start with brand colors and fonts\n2. Apply appropriate template structure\n3. Include logo on first page/slide\n4. Use consistent formatting throughout\n5. Review against brand standards\n6. Ensure professional appearance\n\n## Scripts\n\n- `apply_brand.py`: Automatically applies brand formatting to documents\n- `validate_brand.py`: Checks documents for brand compliance\n\n## Notes\n\n- These guidelines apply to all external communications\n- Internal documents may use simplified formatting\n- Special projects may have exceptions (request approval)\n- Brand guidelines updated quarterly - check for latest version\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 5, error_handling: 4, example_quality: 0',
  'Score: 51, Tier: TIER_2_GOOD, Strengths: workflow_structure: 20, clarity_structure: 12, problem_definition: 10',
  'https://skillsmp.com/skills/anthropics-claude-cookbooks-skills-custom-skills-applying-brand-guidelines-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'create-skill-file - Guides Claude in creating well-structured SKILL.md files following best practices. Provides clear guidelines for naming, structure, and content organi',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Claude Agent Skill \u7f16\u5199\u89c4\u8303\n\n> \u5982\u4f55\u521b\u5efa\u9ad8\u8d28\u91cf\u7684 SKILL.md \u6587\u4ef6\n\n## \u76ee\u5f55\n\n- [\u5feb\u901f\u5f00\u59cb](#\u5feb\u901f\u5f00\u59cb)\n- [\u6838\u5fc3\u539f\u5219](#\u6838\u5fc3\u539f\u5219)\n- [\u6587\u4ef6\u7ed3\u6784\u89c4\u8303](#\u6587\u4ef6\u7ed3\u6784\u89c4\u8303)\n- [\u547d\u540d\u548c\u63cf\u8ff0\u89c4\u8303](#\u547d\u540d\u548c\u63cf\u8ff0\u89c4\u8303)\n- [\u5185\u5bb9\u7f16\u5199\u6307\u5357](#\u5185\u5bb9\u7f16\u5199\u6307\u5357)\n- [\u8d28\u91cf\u68c0\u67e5\u6e05\u5355](#\u8d28\u91cf\u68c0\u67e5\u6e05\u5355)\n\n---\n\n## \u5feb\u901f\u5f00\u59cb\n\n### 3\u6b65\u521b\u5efa Skill\n\n**\u7b2c1\u6b65: \u521b\u5efa\u76ee\u5f55**\n\n```bash\nmkdir -p .claude/skill/your-skill-name\ncd .claude/skill/your-skill-name\n```\n\n**\u7b2c2\u6b65: \u521b\u5efa SKILL.md**\n\n```markdown\n---\nname: your-skill-name\ndescription: Brief description with trigger keywords and scenarios\n---\n\n# Your Skill Title\n\n## When to Use This Skill\n\n- User asks to [specific scenario]\n- User mentions \"[keyword]\"\n\n## How It Works\n\n1. Step 1: [Action]\n2. Step 2: [Action]\n\n## Examples\n\n**Input**: User request\n**Output**: Expected result\n```\n\n**\u7b2c3\u6b65: \u6d4b\u8bd5**\n- \u5728\u5bf9\u8bdd\u4e2d\u4f7f\u7528 description \u4e2d\u7684\u5173\u952e\u8bcd\u89e6\u53d1\n- \u89c2\u5bdf Claude \u662f\u5426\u6b63\u786e\u6267\u884c\n- \u6839\u636e\u6548\u679c\u8c03\u6574\n\n---\n\n## \u6838\u5fc3\u539f\u5219\n\n### 1. \u4fdd\u6301\u7b80\u6d01\n\n\u53ea\u6dfb\u52a0 Claude **\u4e0d\u77e5\u9053**\u7684\u65b0\u77e5\u8bc6:\n- \u2705 \u9879\u76ee\u7279\u5b9a\u7684\u5de5\u4f5c\u6d41\u7a0b\n- \u2705 \u7279\u6b8a\u7684\u547d\u540d\u89c4\u8303\u6216\u683c\u5f0f\u8981\u6c42\n- \u2705 \u81ea\u5b9a\u4e49\u5de5\u5177\u548c\u811a\u672c\u7684\u4f7f\u7528\u65b9\u6cd5\n- \u274c \u901a\u7528\u7f16\u7a0b\u77e5\u8bc6\n- \u274c \u663e\u800c\u6613\u89c1\u7684\u6b65\u9aa4\n\n**\u793a\u4f8b\u5bf9\u6bd4**:\n\n```markdown\n# \u274c \u8fc7\u5ea6\u8be6\u7ec6\n1. \u521b\u5efa Python \u6587\u4ef6\n2. \u5bfc\u5165\u5fc5\u8981\u7684\u5e93\n3. \u5b9a\u4e49\u51fd\u6570\n4. \u7f16\u5199\u4e3b\u7a0b\u5e8f\u903b\u8f91\n\n# \u2705 \u7b80\u6d01\u6709\u6548\n\u4f7f\u7528 `scripts/api_client.py` \u8c03\u7528\u5185\u90e8 API\u3002\n\u8bf7\u6c42\u5934\u5fc5\u987b\u5305\u542b `X-Internal-Token`(\u4ece\u73af\u5883\u53d8\u91cf `INTERNAL_API_KEY` \u83b7\u53d6)\u3002\n```\n\n### 2. \u8bbe\u5b9a\u5408\u9002\u7684\u81ea\u7531\u5ea6\n\n| \u81ea\u7531\u5ea6 | \u9002\u7528\u573a\u666f | \u7f16\u5199\u65b9\u5f0f |\n|--------|---------|---------|\n| **\u9ad8** | \u9700\u8981\u521b\u9020\u6027\u3001\u591a\u79cd\u89e3\u51b3\u65b9\u6848 | \u63d0\u4f9b\u6307\u5bfc\u539f\u5219,\u4e0d\u9650\u5b9a\u5177\u4f53\u6b65\u9aa4 |\n| **\u4e2d** | \u6709\u63a8\u8350\u6a21\u5f0f\u4f46\u5141\u8bb8\u53d8\u5316 | \u63d0\u4f9b\u53c2\u6570\u5316\u793a\u4f8b\u548c\u9ed8\u8ba4\u6d41\u7a0b |\n| **\u4f4e** | \u5bb9\u6613\u51fa\u9519\u3001\u9700\u4e25\u683c\u6267\u884c | \u63d0\u4f9b\u8be6\u7ec6\u7684\u5206\u6b65\u6307\u4ee4\u6216\u811a\u672c |\n\n**\u5224\u65ad\u6807\u51c6**:\n- \u4efb\u52a1\u662f\u5426\u6709\u660e\u786e\u7684\"\u6b63\u786e\u7b54\u6848\"? \u2192 \u4f4e\u81ea\u7531\u5ea6\n- \u662f\u5426\u9700\u8981\u9002\u5e94\u4e0d\u540c\u573a\u666f? \u2192 \u9ad8\u81ea\u7531\u5ea6\n- \u9519\u8bef\u7684\u4ee3\u4ef7\u6709\u591a\u5927? \u2192 \u4ee3\u4ef7\u9ad8\u5219\u7528\u4f4e\u81ea\u7531\u5ea6\n\n### 3. \u6e10\u8fdb\u5f0f\u62ab\u9732\n\n\u5c06\u590d\u6742\u5185\u5bb9\u5206\u5c42\u7ec4\u7ec7:\n\n```\nSKILL.md (\u4e3b\u6587\u6863, 200-500\u884c)\n\u251c\u2500\u2500 reference.md (\u8be6\u7ec6\u6587\u6863)\n\u251c\u2500\u2500 examples.md (\u5b8c\u6574\u793a\u4f8b)\n\u2514\u2500\u2500 scripts/ (\u53ef\u6267\u884c\u811a\u672c)\n```\n\n**\u89c4\u5219**:\n- SKILL.md \u8d85\u8fc7 500\u884c \u2192 \u62c6\u5206\u5b50\u6587\u4ef6\n- \u5b50\u6587\u4ef6\u8d85\u8fc7 100\u884c \u2192 \u6dfb\u52a0\u76ee\u5f55\n- \u5f15\u7528\u6df1\u5ea6 \u2264 1\u5c42\n\n---\n\n## \u6587\u4ef6\u7ed3\u6784\u89c4\u8303\n\n### YAML Frontmatter\n\n```yaml\n---\nname: skill-name-here\ndescription: Clear description of what this skill does and when to activate it\n---\n```\n\n**\u5b57\u6bb5\u89c4\u8303**:\n\n| \u5b57\u6bb5 | \u8981\u6c42 | \u8bf4\u660e |\n|------|------|------|\n| `name` | \u5c0f\u5199\u5b57\u6bcd\u3001\u6570\u5b57\u3001\u77ed\u6a2a\u7ebf,\u226464\u5b57\u7b26 | \u5fc5\u987b\u4e0e\u76ee\u5f55\u540d\u4e00\u81f4 |\n| `description` | \u7eaf\u6587\u672c,\u22641024\u5b57\u7b26 | \u7528\u4e8e\u68c0\u7d22\u548c\u6fc0\u6d3b |\n\n**\u547d\u540d\u7981\u5fcc**:\n- \u274c XML \u6807\u7b7e\u3001\u4fdd\u7559\u5b57(`anthropic`, `claude`)\n- \u274c \u6a21\u7cca\u8bcd\u6c47(`helper`, `utility`, `manager`)\n- \u274c \u7a7a\u683c\u6216\u4e0b\u5212\u7ebf(\u7528\u77ed\u6a2a\u7ebf `-`)\n\n**Description \u6280\u5de7**:\n\n```yaml\n# \u274c \u8fc7\u4e8e\u6cdb\u5316\ndescription: Helps with code tasks\n\n# \u2705 \u5177\u4f53\u4e14\u5305\u542b\u5173\u952e\u8bcd\ndescription: Processes CSV files and generates Excel reports with charts. Use when user asks to convert data formats or create visual reports.\n\n# \u2705 \u8bf4\u660e\u89e6\u53d1\u573a\u666f\ndescription: Analyzes Python code for security vulnerabilities using bandit. Activates when user mentions \"security audit\" or \"vulnerability scan\".\n```\n\n### \u76ee\u5f55\u7ec4\u7ec7\n\n**\u57fa\u7840\u7ed3\u6784**(\u7b80\u5355 Skill):\n```\nskill-name/\n\u2514\u2500\u2500 SKILL.md\n```\n\n**\u6807\u51c6\u7ed3\u6784**(\u63a8\u8350):\n```\nskill-name/\n\u251c\u2500\u2500 SKILL.md\n\u251c\u2500\u2500 templates/\n\u2502   \u2514\u2500\u2500 template.md\n\u2514\u2500\u2500 scripts/\n    \u2514\u2500\u2500 script.py\n```\n\n---\n\n## \u547d\u540d\u548c\u63cf\u8ff0\u89c4\u8303\n\n### Skill \u547d\u540d\n\n**\u63a8\u8350\u683c\u5f0f**: \u52a8\u540d\u8bcd\u5f62\u5f0f (verb-ing + noun)\n\n```\n\u2705 \u597d\u7684\u547d\u540d:\n- processing-csv-files\n- generating-api-docs\n- managing-database-migrations\n\n\u274c \u4e0d\u597d\u7684\u547d\u540d:\n- csv (\u8fc7\u4e8e\u7b80\u77ed)\n- data_processor (\u4f7f\u7528\u4e0b\u5212\u7ebf)\n- helper (\u8fc7\u4e8e\u6a21\u7cca)\n```\n\n### Description \u7f16\u5199\n\n**\u5fc5\u987b\u4f7f\u7528\u7b2c\u4e09\u4eba\u79f0**:\n\n```yaml\n# \u274c \u9519\u8bef\ndescription: I help you process PDFs\n\n# \u2705 \u6b63\u786e\ndescription: Processes PDF documents and extracts structured data\n```\n\n**4C \u539f\u5219**:\n- **Clear** (\u6e05\u6670): \u907f\u514d\u672f\u8bed\u548c\u6a21\u7cca\u8bcd\u6c47\n- **Concise** (\u7b80\u6d01): 1-2\u53e5\u8bdd\u8bf4\u660e\u6838\u5fc3\u529f\u80fd\n- **Contextual** (\u4e0a\u4e0b\u6587): \u8bf4\u660e\u9002\u7528\u573a\u666f\n- **Complete** (\u5b8c\u6574): \u529f\u80fd + \u89e6\u53d1\u6761\u4ef6\n\n---\n\n## \u5185\u5bb9\u7f16\u5199\u6307\u5357\n\n### \"When to Use\" \u7ae0\u8282\n\n\u660e\u786e\u8bf4\u660e\u89e6\u53d1\u573a\u666f:\n\n```markdown\n## When to Use This Skill\n\n- User asks to analyze Python code for type errors\n- User mentions \"mypy\" or \"type checking\"\n- User is working in a Python project with type hints\n- User needs to add type annotations\n```\n\n**\u6a21\u5f0f**:\n- \u76f4\u63a5\u8bf7\u6c42: \"User asks to X\"\n- \u5173\u952e\u8bcd: \"User mentions ''''keyword''''\"\n- \u4e0a\u4e0b\u6587: \"User is working with X\"\n- \u4efb\u52a1\u7c7b\u578b: \"User needs to X\"\n\n### \u5de5\u4f5c\u6d41\u8bbe\u8ba1\n\n**\u7b80\u5355\u7ebf\u6027\u6d41\u7a0b**:\n\n```markdown\n## How It Works\n\n1. Scan the project for all `.py` files\n2. Run `mypy --strict` on each file\n3. Parse error output and categorize by severity\n4. Generate summary report with fix suggestions\n```\n\n**\u6761\u4ef6\u5206\u652f\u6d41\u7a0b**:\n\n```markdown\n## Workflow\n\n1. **Check project type**\n   - If Django \u2192 Use `django-stubs` config\n   - If Flask \u2192 Use `flask-stubs` config\n   - Otherwise \u2192 Use default mypy config\n\n2. **Run type checking**\n   - If errors found \u2192 Proceed to step 3\n   - If no errors \u2192 Report success and exit\n```\n\n**Checklist \u6a21\u5f0f**(\u9a8c\u8bc1\u578b\u4efb\u52a1):\n\n```markdown\n## Pre-deployment Checklist\n\nExecute in order. Stop if any step fails.\n\n- [ ] Run tests: `npm test` (must pass)\n- [ ] Build: `npm run build` (no errors)\n- [ ] Check deps: `npm audit` (no critical vulnerabilities)\n```\n\n### \u793a\u4f8b\u548c\u6a21\u677f\n\n**\u8f93\u5165-\u8f93\u51fa\u793a\u4f8b**:\n\n```markdown\n## Examples\n\n### Example 1: Basic Check\n\n**User Request**: \"Check my code for type errors\"\n\n**Action**:\n1. Scan for `.py` files\n2. Run `mypy` on all files\n\n**Output**:\n   \n   Found 3 type errors in 2 files:\n   src/main.py:15: error: Missing return type\n   src/utils.py:42: error: Incompatible types\n   \n```\n\n### \u811a\u672c\u96c6\u6210\n\n**\u4f55\u65f6\u4f7f\u7528\u811a\u672c**:\n- \u7b80\u5355\u547d\u4ee4 \u2192 \u76f4\u63a5\u5728 SKILL.md \u4e2d\u8bf4\u660e\n- \u590d\u6742\u6d41\u7a0b \u2192 \u63d0\u4f9b\u72ec\u7acb\u811a\u672c\n\n**\u811a\u672c\u7f16\u5199\u89c4\u8303**:\n\n```python\n#!/usr/bin/env python3\n\"\"\"\nBrief description of what this script does.\n\nUsage:\n    python script.py <arg> [--option value]\n\"\"\"\n\nimport argparse\n\nDEFAULT_VALUE = 80  # Use constants, not magic numbers\n\ndef main():\n    parser = argparse.ArgumentParser(description=__doc__)\n    parser.add_argument(\"directory\", help=\"Directory to process\")\n    parser.add_argument(\"--threshold\", type=int, default=DEFAULT_VALUE)\n\n    args = parser.parse_args()\n\n    # Validate inputs\n    if not Path(args.directory).is_dir():\n        print(f\"Error: {args.directory} not found\")\n        return 1\n\n    # Execute\n    result = process(args.directory, args.threshold)\n\n    # Report\n    print(f\"Processed {result[''''count'''']} files\")\n    return 0\n\nif __name__ == \"__main__\":\n    exit(main())\n```\n\n**\u5173\u952e\u89c4\u8303**:\n- \u2705 Shebang \u884c\u548c docstring\n- \u2705 \u7c7b\u578b\u6ce8\u89e3\u548c\u5e38\u91cf\n- \u2705 \u53c2\u6570\u9a8c\u8bc1\u548c\u9519\u8bef\u5904\u7406\n- \u2705 \u6e05\u6670\u7684\u8fd4\u56de\u503c(0=\u6210\u529f, 1=\u5931\u8d25)\n\n### \u6700\u4f73\u5b9e\u8df5\n\n**Do**:\n- \u2705 \u63d0\u4f9b\u53ef\u6267\u884c\u7684\u547d\u4ee4\u548c\u811a\u672c\n- \u2705 \u5305\u542b\u8f93\u5165-\u8f93\u51fa\u793a\u4f8b\n- \u2705 \u8bf4\u660e\u9a8c\u8bc1\u6807\u51c6\u548c\u6210\u529f\u6761\u4ef6\n- \u2705 \u5305\u542b Do/Don''''t \u6e05\u5355\n\n**Don''''t**:\n- \u274c \u5305\u542b Claude \u5df2\u77e5\u7684\u901a\u7528\u77e5\u8bc6\n- \u274c \u4f7f\u7528\u62bd\u8c61\u63cf\u8ff0\u800c\u975e\u5177\u4f53\u6b65\u9aa4\n- \u274c \u9057\u6f0f\u9519\u8bef\u5904\u7406\u6307\u5bfc\n- \u274c \u793a\u4f8b\u4f7f\u7528\u4f2a\u4ee3\u7801\u800c\u975e\u771f\u5b9e\u4ee3\u7801\n\n---\n\n## \u8d28\u91cf\u68c0\u67e5\u6e05\u5355\n\n### \u6838\u5fc3\u8d28\u91cf\n\n- [ ] `name` \u7b26\u5408\u547d\u540d\u89c4\u8303(\u5c0f\u5199\u3001\u77ed\u6a2a\u7ebf\u3001\u226464\u5b57\u7b26)\n- [ ] `description` \u5305\u542b\u89e6\u53d1\u5173\u952e\u8bcd\u548c\u573a\u666f(\u22641024\u5b57\u7b26)\n- [ ] \u540d\u79f0\u4e0e\u76ee\u5f55\u540d\u4e00\u81f4\n- [ ] \u53ea\u5305\u542b Claude \u4e0d\u77e5\u9053\u7684\u4fe1\u606f\n- [ ] \u6ca1\u6709\u5197\u4f59\u6216\u91cd\u590d\u5185\u5bb9\n\n### \u529f\u80fd\u5b8c\u6574\u6027\n\n- [ ] \u6709\"When to Use\"\u7ae0\u8282,\u5217\u51fa 3-5 \u4e2a\u89e6\u53d1\u573a\u666f\n- [ ] \u6709\u6e05\u6670\u7684\u6267\u884c\u6d41\u7a0b\u6216\u6b65\u9aa4\n- [ ] \u81f3\u5c11 2-3 \u4e2a\u5b8c\u6574\u793a\u4f8b\n- [ ] \u5305\u542b\u8f93\u5165\u548c\u9884\u671f\u8f93\u51fa\n- [ ] \u9519\u8bef\u5904\u7406\u6709\u6307\u5bfc\n\n### \u7ed3\u6784\u89c4\u8303\n\n- [ ] \u7ae0\u8282\u7ec4\u7ec7\u6e05\u6670\n- [ ] \u8d85\u8fc7 200\u884c\u6709\u76ee\u5f55\u5bfc\u822a\n- [ ] \u5f15\u7528\u5c42\u7ea7 \u2264 1\u5c42\n- [ ] \u6240\u6709\u8def\u5f84\u4f7f\u7528\u6b63\u659c\u6760 `/`\n- [ ] \u672f\u8bed\u4f7f\u7528\u4e00\u81f4\n\n### \u811a\u672c\u548c\u6a21\u677f\n\n- [ ] \u811a\u672c\u5305\u542b\u4f7f\u7528\u8bf4\u660e\u548c\u53c2\u6570\u6587\u6863\n- [ ] \u811a\u672c\u6709\u9519\u8bef\u5904\u7406\n- [ ] \u907f\u514d\u9b54\u6cd5\u6570\u5b57,\u4f7f\u7528\u914d\u7f6e\n- [ ] \u6a21\u677f\u683c\u5f0f\u6e05\u6670\u6613\u7528\n\n### \u6700\u7ec8\u68c0\u67e5\n\n- [ ] \u901a\u8bfb\u5168\u6587,\u786e\u4fdd\u6d41\u7545\u6613\u8bfb\n- [ ] \u4f7f\u7528\u5b9e\u9645\u573a\u666f\u6d4b\u8bd5\u89e6\u53d1\n- [ ] \u957f\u5ea6\u9002\u4e2d(200-500\u884c,\u6216\u5df2\u62c6\u5206)\n\n---\n\n## \u5e38\u89c1\u95ee\u9898\n\n**Q: Skill \u591a\u957f\u624d\u5408\u9002?**\n- \u6700\u5c0f: 50-100\u884c\n- \u7406\u60f3: 200-500\u884c\n- \u6700\u5927: 500\u884c(\u8d85\u8fc7\u5219\u62c6\u5206)\n\n**Q: \u5982\u4f55\u8ba9 Skill \u66f4\u5bb9\u6613\u6fc0\u6d3b?**\n- \u5728 `description` \u4e2d\u4f7f\u7528\u7528\u6237\u4f1a\u8bf4\u7684\u5173\u952e\u8bcd\n- \u8bf4\u660e\u5177\u4f53\u573a\u666f(\"when user asks to X\")\n- \u63d0\u53ca\u76f8\u5173\u5de5\u5177\u540d\u79f0\n\n**Q: \u591a\u4e2a Skill \u529f\u80fd\u91cd\u53e0\u600e\u4e48\u529e?**\n- \u4f7f\u7528\u66f4\u5177\u4f53\u7684 `description` \u533a\u5206\n- \u5728\"When to Use\"\u4e2d\u8bf4\u660e\u5173\u7cfb\n- \u8003\u8651\u5408\u5e76\u4e3a\u4e00\u4e2a Skill\n\n**Q: Skill \u9700\u8981\u7ef4\u62a4\u5417?**\n- \u6bcf\u5b63\u5ea6\u5ba1\u67e5\u4e00\u6b21,\u66f4\u65b0\u8fc7\u65f6\u4fe1\u606f\n- \u6839\u636e\u4f7f\u7528\u53cd\u9988\u8fed\u4ee3\n- \u5de5\u5177\u6216 API \u53d8\u66f4\u65f6\u53ca\u65f6\u66f4\u65b0\n\n---\n\n## \u5feb\u901f\u53c2\u8003\n\n### Frontmatter \u6a21\u677f\n\n```yaml\n---\nname: skill-name\ndescription: Brief description with trigger keywords\n---\n```\n\n### \u57fa\u7840\u7ed3\u6784\u6a21\u677f\n\n```markdown\n# Skill Title\n\n## When to Use This Skill\n- Scenario 1\n- Scenario 2\n\n## How It Works\n1. Step 1\n2. Step 2\n\n## Examples\n### Example 1\n...\n\n## References\n- [Link](url)\n```\n\n---\n\n## \u76f8\u5173\u8d44\u6e90\n\n- [Claude Agent Skills \u5b98\u65b9\u6587\u6863](https://docs.claude.com/en/docs/agents-and-tools/agent-skills)\n- [Best Practices Checklist](https://docs.claude.com/en/docs/agents-and-tools/agent-skills/best-practices)\n- [\u6a21\u677f\u6587\u4ef6](templates/) - \u5f00\u7bb1\u5373\u7528\u7684\u6a21\u677f \n  - [\u57fa\u7840 skill \u7684\u6a21\u677f](templates/basic-skill-template.md)\n  - [\u5de5\u4f5c\u6d41 skill \u7684\u6a21\u677f](templates/workflow-skill-template.md)\n- [\u793a\u4f8b\u5e93](examples/) - \u5b8c\u6574\u7684 Skill \u793a\u4f8b\n  - [\u4f18\u79c0\u793a\u4f8b](examples/good-example.md)\n  - [\u5e38\u89c1\u9519\u8bef\u793a\u4f8b](examples/bad-example.md)\n\n---\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, problem_definition: 10, completeness: 5',
  'Score: 85, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, error_handling: 15',
  'https://skillsmp.com/skills/labring-fastgpt-claude-skills-create-skill-file-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'workflow-interactive-dev -  FastGPT ',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# \u4ea4\u4e92\u8282\u70b9\u5f00\u53d1\u6307\u5357\n\n## \u6982\u8ff0\n\nFastGPT \u5de5\u4f5c\u6d41\u652f\u6301\u591a\u79cd\u4ea4\u4e92\u8282\u70b9\u7c7b\u578b,\u5141\u8bb8\u5728\u5de5\u4f5c\u6d41\u6267\u884c\u8fc7\u7a0b\u4e2d\u6682\u505c\u5e76\u7b49\u5f85\u7528\u6237\u8f93\u5165\u3002\u672c\u6307\u5357\u8be6\u7ec6\u8bf4\u660e\u4e86\u5982\u4f55\u5f00\u53d1\u65b0\u7684\u4ea4\u4e92\u8282\u70b9\u3002\n\n## \u73b0\u6709\u4ea4\u4e92\u8282\u70b9\u7c7b\u578b\n\n\u5f53\u524d\u7cfb\u7edf\u652f\u6301\u4ee5\u4e0b\u4ea4\u4e92\u8282\u70b9\u7c7b\u578b:\n\n1. **userSelect** - \u7528\u6237\u9009\u62e9\u8282\u70b9(\u5355\u9009)\n2. **formInput** - \u8868\u5355\u8f93\u5165\u8282\u70b9(\u591a\u5b57\u6bb5\u8868\u5355)\n3. **childrenInteractive** - \u5b50\u5de5\u4f5c\u6d41\u4ea4\u4e92\n4. **loopInteractive** - \u5faa\u73af\u4ea4\u4e92\n5. **paymentPause** - \u6b20\u8d39\u6682\u505c\u4ea4\u4e92\n\n## \u4ea4\u4e92\u8282\u70b9\u67b6\u6784\n\n### \u6838\u5fc3\u7c7b\u578b\u5b9a\u4e49\n\n\u4ea4\u4e92\u8282\u70b9\u7684\u7c7b\u578b\u5b9a\u4e49\u4f4d\u4e8e `packages/global/core/workflow/template/system/interactive/type.d.ts`\n\n```typescript\n// \u57fa\u7840\u4ea4\u4e92\u7ed3\u6784\ntype InteractiveBasicType = {\n  entryNodeIds: string[];                    // \u5165\u53e3\u8282\u70b9ID\u5217\u8868\n  memoryEdges: RuntimeEdgeItemType[];        // \u9700\u8981\u8bb0\u5fc6\u7684\u8fb9\n  nodeOutputs: NodeOutputItemType[];         // \u8282\u70b9\u8f93\u51fa\n  skipNodeQueue?: Array;                     // \u8df3\u8fc7\u7684\u8282\u70b9\u961f\u5217\n  usageId?: string;                          // \u7528\u91cf\u8bb0\u5f55ID\n};\n\n// \u5177\u4f53\u4ea4\u4e92\u8282\u70b9\u7c7b\u578b\ntype YourInteractiveNode = InteractiveNodeType & {\n  type: ''''yourNodeType'''';\n  params: {\n    // \u8282\u70b9\u7279\u5b9a\u53c2\u6570\n  };\n};\n```\n\n### \u5de5\u4f5c\u6d41\u6267\u884c\u673a\u5236\n\n\u4ea4\u4e92\u8282\u70b9\u5728\u5de5\u4f5c\u6d41\u6267\u884c\u4e2d\u7684\u7279\u6b8a\u5904\u7406(\u4f4d\u4e8e `packages/service/core/workflow/dispatch/index.ts:1012-1019`):\n\n```typescript\n// \u90e8\u5206\u4ea4\u4e92\u8282\u70b9\u4e0d\u4f1a\u81ea\u52a8\u91cd\u7f6e isEntry \u6807\u5fd7\uff08\u56e0\u4e3a\u9700\u8981\u6839\u636e isEntry \u5b57\u6bb5\u6765\u5224\u65ad\u662f\u9996\u6b21\u8fdb\u5165\u8fd8\u662f\u6d41\u7a0b\u8fdb\u5165\uff09\nruntimeNodes.forEach((item) => {\n  if (\n    item.flowNodeType !== FlowNodeTypeEnum.userSelect &&\n    item.flowNodeType !== FlowNodeTypeEnum.formInput &&\n    item.flowNodeType !== FlowNodeTypeEnum.agent\n  ) {\n    item.isEntry = false;\n  }\n});\n```\n\n## \u5f00\u53d1\u65b0\u4ea4\u4e92\u54cd\u5e94\u7684\u6b65\u9aa4\n\n### \u6b65\u9aa4 1: \u5b9a\u4e49\u8282\u70b9\u7c7b\u578b\n\n**\u6587\u4ef6**: `packages/global/core/workflow/template/system/interactive/type.d.ts`\n\n```typescript\nexport type YourInputItemType = {\n  // \u5b9a\u4e49\u8f93\u5165\u9879\u7684\u7ed3\u6784\n  key: string;\n  label: string;\n  value: any;\n  // ... \u5176\u4ed6\u5b57\u6bb5\n};\n\ntype YourInteractiveNode = InteractiveNodeType & {\n  type: ''''yourNodeType'''';\n  params: {\n    description: string;\n    yourInputField: YourInputItemType[];\n    submitted?: boolean;  // \u53ef\u9009:\u662f\u5426\u5df2\u63d0\u4ea4\n  };\n};\n\n// \u6dfb\u52a0\u5230\u8054\u5408\u7c7b\u578b\nexport type InteractiveNodeResponseType =\n  | UserSelectInteractive\n  | UserInputInteractive\n  | YourInteractiveNode  // \u65b0\u589e\n  | ChildrenInteractive\n  | LoopInteractive\n  | PaymentPauseInteractive;\n```\n\n### \u6b65\u9aa4 2: \u5b9a\u4e49\u8282\u70b9\u679a\u4e3e\uff08\u53ef\u9009\uff09\n\n**\u6587\u4ef6**: `packages/global/core/workflow/node/constant.ts`\n\n\u5982\u679c\u4e0d\u9700\u8981\u6dfb\u52a0\u65b0\u7684\u8282\u70b9\u7c7b\u578b\uff0c\u5219\u4e0d\u9700\u8981\u4fee\u6539\u8fd9\u4e2a\u6587\u4ef6\u3002\n\n```typescript\nexport enum FlowNodeTypeEnum {\n  // ... \u73b0\u6709\u7c7b\u578b\n  yourNodeType = ''''yourNodeType'''',  // \u65b0\u589e\u8282\u70b9\u7c7b\u578b\n}\n```\n\n### \u6b65\u9aa4 3: \u521b\u5efa\u8282\u70b9\u6a21\u677f\uff08\u53ef\u9009\uff09\n\n**\u6587\u4ef6**: `packages/global/core/workflow/template/system/interactive/yourNode.ts`\n\n```typescript\nimport { i18nT } from ''''../../../../../../web/i18n/utils'''';\nimport {\n  FlowNodeTemplateTypeEnum,\n  NodeInputKeyEnum,\n  NodeOutputKeyEnum,\n  WorkflowIOValueTypeEnum\n} from ''''../../../constants'''';\nimport {\n  FlowNodeInputTypeEnum,\n  FlowNodeOutputTypeEnum,\n  FlowNodeTypeEnum\n} from ''''../../../node/constant'''';\nimport { type FlowNodeTemplateType } from ''''../../../type/node'''';\n\nexport const YourNode: FlowNodeTemplateType = {\n  id: FlowNodeTypeEnum.yourNodeType,\n  templateType: FlowNodeTemplateTypeEnum.interactive,\n  flowNodeType: FlowNodeTypeEnum.yourNodeType,\n  showSourceHandle: true,   // \u662f\u5426\u663e\u793a\u6e90\u8fde\u63a5\u70b9\n  showTargetHandle: true,   // \u662f\u5426\u663e\u793a\u76ee\u6807\u8fde\u63a5\u70b9\n  avatar: ''''core/workflow/template/yourNode'''',\n  name: i18nT(''''app:workflow.your_node''''),\n  intro: i18nT(''''app:workflow.your_node_tip''''),\n  isTool: true,  // \u6807\u8bb0\u4e3a\u5de5\u5177\u8282\u70b9\n  inputs: [\n    {\n      key: NodeInputKeyEnum.description,\n      renderTypeList: [FlowNodeInputTypeEnum.textarea],\n      valueType: WorkflowIOValueTypeEnum.string,\n      label: i18nT(''''app:workflow.node_description''''),\n      placeholder: i18nT(''''app:workflow.your_node_placeholder'''')\n    },\n    {\n      key: NodeInputKeyEnum.yourInputField,\n      renderTypeList: [FlowNodeInputTypeEnum.custom],\n      valueType: WorkflowIOValueTypeEnum.any,\n      label: '''''''',\n      value: []  // \u9ed8\u8ba4\u503c\n    }\n  ],\n  outputs: [\n    {\n      id: NodeOutputKeyEnum.yourResult,\n      key: NodeOutputKeyEnum.yourResult,\n      required: true,\n      label: i18nT(''''workflow:your_result''''),\n      valueType: WorkflowIOValueTypeEnum.object,\n      type: FlowNodeOutputTypeEnum.static\n    }\n  ]\n};\n```\n\n### \u6b65\u9aa4 4: \u521b\u5efa\u8282\u70b9\u6267\u884c\u903b\u8f91\u6216\u5728\u9700\u8981\u5904\u7406\u4ea4\u4e92\u903b\u8f91\u7684\u8282\u70b9\u4e0a\u589e\u52a0\u65b0\u903b\u8f91\n\n**\u6587\u4ef6**: `packages/service/core/workflow/dispatch/interactive/yourNode.ts`\n\n```typescript\nimport { DispatchNodeResponseKeyEnum } from ''''@fastgpt/global/core/workflow/runtime/constants'''';\nimport type {\n  DispatchNodeResultType,\n  ModuleDispatchProps\n} from ''''@fastgpt/global/core/workflow/runtime/type'''';\nimport type { NodeInputKeyEnum } from ''''@fastgpt/global/core/workflow/constants'''';\nimport { NodeOutputKeyEnum } from ''''@fastgpt/global/core/workflow/constants'''';\nimport type { YourInputItemType } from ''''@fastgpt/global/core/workflow/template/system/interactive/type'''';\nimport { chatValue2RuntimePrompt } from ''''@fastgpt/global/core/chat/adapt'''';\n\ntype Props = ModuleDispatchProps<{\n  [NodeInputKeyEnum.description]: string;\n  [NodeInputKeyEnum.yourInputField]: YourInputItemType[];\n}>;\n\ntype YourNodeResponse = DispatchNodeResultType<{\n  [NodeOutputKeyEnum.yourResult]?: Record<string, any>;\n}>;\n\nexport const dispatchYourNode = async (props: Props): Promise<YourNodeResponse> => {\n  const {\n    histories,\n    node,\n    params: { description, yourInputField },\n    query,\n    lastInteractive\n  } = props;\n  const { isEntry } = node;\n\n  // \u7b2c\u4e00\u9636\u6bb5:\u975e\u5165\u53e3\u8282\u70b9\u6216\u4e0d\u662f\u5bf9\u5e94\u7684\u4ea4\u4e92\u7c7b\u578b,\u8fd4\u56de\u4ea4\u4e92\u8bf7\u6c42\n  if (!isEntry || lastInteractive?.type !== ''''yourNodeType'''') {\n    return {\n      [DispatchNodeResponseKeyEnum.interactive]: {\n        type: ''''yourNodeType'''',\n        params: {\n          description,\n          yourInputField\n        }\n      }\n    };\n  }\n\n  // \u7b2c\u4e8c\u9636\u6bb5:\u5904\u7406\u7528\u6237\u63d0\u4ea4\u7684\u6570\u636e\n  node.isEntry = false;  // \u91cd\u8981:\u91cd\u7f6e\u5165\u53e3\u6807\u5fd7\n\n  const { text } = chatValue2RuntimePrompt(query);\n  const userInputVal = (() => {\n    try {\n      return JSON.parse(text);  // \u6839\u636e\u5b9e\u9645\u683c\u5f0f\u89e3\u6790\n    } catch (error) {\n      return {};\n    }\n  })();\n\n  return {\n    data: {\n      [NodeOutputKeyEnum.yourResult]: userInputVal\n    },\n    // \u79fb\u9664\u5f53\u524d\u4ea4\u4e92\u7684\u5386\u53f2\u8bb0\u5f55(\u6700\u540e2\u6761)\n    [DispatchNodeResponseKeyEnum.rewriteHistories]: histories.slice(0, -2),\n    [DispatchNodeResponseKeyEnum.toolResponses]: userInputVal,\n    [DispatchNodeResponseKeyEnum.nodeResponse]: {\n      yourResult: userInputVal\n    }\n  };\n};\n```\n\n### \u6b65\u9aa4 5: \u6ce8\u518c\u8282\u70b9\u56de\u8c03\n\n**\u6587\u4ef6**: `packages/service/core/workflow/dispatch/constants.ts`\n\n```typescript\nimport { dispatchYourNode } from ''''./interactive/yourNode'''';\n\nexport const callbackMap: Record<FlowNodeTypeEnum, any> = {\n  // ... \u73b0\u6709\u8282\u70b9\n  [FlowNodeTypeEnum.yourNodeType]: dispatchYourNode,\n};\n```\n\n### \u6b65\u9aa4 6: \u521b\u5efa\u524d\u7aef\u6e32\u67d3\u7ec4\u4ef6\n\n#### 6.1 \u804a\u5929\u754c\u9762\u4ea4\u4e92\u7ec4\u4ef6\n\n**\u6587\u4ef6**: `projects/app/src/components/core/chat/components/Interactive/InteractiveComponents.tsx`\n\n```typescript\nexport const YourNodeComponent = React.memo(function YourNodeComponent({\n  interactiveParams: { description, yourInputField, submitted },\n  defaultValues = {},\n  SubmitButton\n}: {\n  interactiveParams: YourInteractiveNode[''''params''''];\n  defaultValues?: Record<string, any>;\n  SubmitButton: (e: { onSubmit: UseFormHandleSubmit<Record<string, any>> }) => React.JSX.Element;\n}) {\n  const { handleSubmit, control } = useForm({\n    defaultValues\n  });\n\n  return (\n    <Box>\n      <DescriptionBox description={description} />\n      <Flex flexDirection={''''column''''} gap={3}>\n        {yourInputField.map((input) => (\n          <Box key={input.key}>\n            {/* \u6e32\u67d3\u4f60\u7684\u8f93\u5165\u7ec4\u4ef6 */}\n            <Controller\n              control={control}\n              name={input.key}\n              render={({ field: { onChange, value } }) => (\n                <YourInputComponent\n                  value={value}\n                  onChange={onChange}\n                  isDisabled={submitted}\n                />\n              )}\n            />\n          </Box>\n        ))}\n      </Flex>\n\n      {!submitted && (\n        <Flex justifyContent={''''flex-end''''} mt={4}>\n          <SubmitButton onSubmit={handleSubmit} />\n        </Flex>\n      )}\n    </Box>\n  );\n});\n```\n\n#### 6.2 \u5de5\u4f5c\u6d41\u7f16\u8f91\u5668\u8282\u70b9\u7ec4\u4ef6\n\n**\u6587\u4ef6**: `projects/app/src/pageComponents/app/detail/WorkflowComponents/Flow/nodes/NodeYourNode.tsx`\n\n```typescript\nimport React, { useMemo } from ''''react'''';\nimport { type NodeProps } from ''''reactflow'''';\nimport { Box, Button } from ''''@chakra-ui/react'''';\nimport NodeCard from ''''./render/NodeCard'''';\nimport { type FlowNodeItemType } from ''''@fastgpt/global/core/workflow/type/node.d'''';\nimport Container from ''''../components/Container'''';\nimport RenderInput from ''''./render/RenderInput'''';\nimport { NodeInputKeyEnum } from ''''@fastgpt/global/core/workflow/constants'''';\nimport { useTranslation } from ''''next-i18next'''';\nimport { type FlowNodeInputItemType } from ''''@fastgpt/global/core/workflow/type/io.d'''';\nimport { useContextSelector } from ''''use-context-selector'''';\nimport IOTitle from ''''../components/IOTitle'''';\nimport RenderOutput from ''''./render/RenderOutput'''';\nimport { WorkflowActionsContext } from ''''../../context/workflowActionsContext'''';\n\nconst NodeYourNode = ({ data, selected }: NodeProps<FlowNodeItemType>) => {\n  const { t } = useTranslation();\n  const { nodeId, inputs, outputs } = data;\n  const onChangeNode = useContextSelector(WorkflowActionsContext, (v) => v.onChangeNode);\n\n  const CustomComponent = useMemo(\n    () => ({\n      [NodeInputKeyEnum.yourInputField]: (v: FlowNodeInputItemType) => {\n        // \u81ea\u5b9a\u4e49\u6e32\u67d3\u903b\u8f91\n        return (\n          <Box>\n            {/* \u4f60\u7684\u81ea\u5b9a\u4e49UI */}\n          </Box>\n        );\n      }\n    }),\n    [nodeId, onChangeNode, t]\n  );\n\n  return (\n    <NodeCard minW={''''400px''''} selected={selected} {...data}>\n      <Container>\n        <RenderInput nodeId={nodeId} flowInputList={inputs} CustomComponent={CustomComponent} />\n      </Container>\n      <Container>\n        <IOTitle text={t(''''common:Output'''')} />\n        <RenderOutput nodeId={nodeId} flowOutputList={outputs} />\n      </Container>\n    </NodeCard>\n  );\n};\n\nexport default React.memo(NodeYourNode);\n```\n\n### \u6b65\u9aa4 7: \u6ce8\u518c\u8282\u70b9\u7ec4\u4ef6\n\n\u9700\u8981\u5728\u8282\u70b9\u6ce8\u518c\u8868\u4e2d\u6dfb\u52a0\u4f60\u7684\u8282\u70b9\u7ec4\u4ef6(\u5177\u4f53\u4f4d\u7f6e\u6839\u636e\u9879\u76ee\u914d\u7f6e\u800c\u5b9a)\u3002\n\n### \u6b65\u9aa4 8: \u6dfb\u52a0\u56fd\u9645\u5316\n\n**\u6587\u4ef6**: `packages/web/i18n/zh-CN/app.json` \u548c\u5176\u4ed6\u8bed\u8a00\u6587\u4ef6\n\n```json\n{\n  \"workflow\": {\n    \"your_node\": \"\u4f60\u7684\u8282\u70b9\u540d\u79f0\",\n    \"your_node_tip\": \"\u8282\u70b9\u529f\u80fd\u8bf4\u660e\",\n    \"your_node_placeholder\": \"\u63d0\u793a\u6587\u672c\"\n  }\n}\n```\n\n### \u6b65\u9aa49 \u8c03\u6574\u4fdd\u5b58\u5bf9\u8bdd\u8bb0\u5f55\u903b\u8f91\n\n**\u6587\u4ef6**: `FastGPT/packages/service/core/chat/saveChat.ts`\n\n\u4fee\u6539 `updateInteractiveChat` \u65b9\u6cd5\uff0c\u652f\u6301\u65b0\u7684\u4ea4\u4e92\n\n### \u6b65\u9aa410 \u6839\u636e\u5386\u53f2\u8bb0\u5f55\u83b7\u53d6/\u8bbe\u7f6e\u4ea4\u4e92\u72b6\u6001\n\n**\u6587\u4ef6**: `FastGPT/projects/app/src/components/core/chat/ChatContainer/ChatBox/utils.ts`\n**\u6587\u4ef6**: `FastGPT/packages/global/core/workflow/runtime/utils.ts`\n\n\u8c03\u6574`setInteractiveResultToHistories`, `getInteractiveByHistories` \u548c `getLastInteractiveValue`\u65b9\u6cd5\u3002\n\n## \u5173\u952e\u6ce8\u610f\u4e8b\u9879\n\n### 1. isEntry \u6807\u5fd7\u7ba1\u7406\n\n\u4ea4\u4e92\u8282\u70b9\u9700\u8981\u4fdd\u6301 `isEntry` \u6807\u5fd7\u5728\u5de5\u4f5c\u6d41\u6062\u590d\u65f6\u6709\u6548:\n\n```typescript\n// \u5728 packages/service/core/workflow/dispatch/index.ts \u4e2d\n// \u786e\u4fdd\u4f60\u7684\u8282\u70b9\u7c7b\u578b\u88ab\u6dfb\u52a0\u5230\u767d\u540d\u5355\nif (\n  item.flowNodeType !== FlowNodeTypeEnum.userSelect &&\n  item.flowNodeType !== FlowNodeTypeEnum.formInput &&\n  item.flowNodeType !== FlowNodeTypeEnum.yourNodeType  // \u65b0\u589e\n) {\n  item.isEntry = false;\n}\n```\n\n### 2. \u4ea4\u4e92\u54cd\u5e94\u6d41\u7a0b\n\n\u4ea4\u4e92\u8282\u70b9\u6709\u4e24\u4e2a\u6267\u884c\u9636\u6bb5:\n\n1. **\u7b2c\u4e00\u6b21\u6267\u884c**: \u8fd4\u56de `interactive` \u54cd\u5e94,\u6682\u505c\u5de5\u4f5c\u6d41\n2. **\u7b2c\u4e8c\u6b21\u6267\u884c**: \u63a5\u6536\u7528\u6237\u8f93\u5165,\u7ee7\u7eed\u5de5\u4f5c\u6d41\n\n```typescript\n// \u7b2c\u4e00\u9636\u6bb5\nif (!isEntry || lastInteractive?.type !== ''''yourNodeType'''') {\n  return {\n    [DispatchNodeResponseKeyEnum.interactive]: {\n      type: ''''yourNodeType'''',\n      params: { /* ... */ }\n    }\n  };\n}\n\n// \u7b2c\u4e8c\u9636\u6bb5\nnode.isEntry = false;  // \u91cd\u8981!\u91cd\u7f6e\u6807\u5fd7\n// \u5904\u7406\u7528\u6237\u8f93\u5165...\n```\n\n### 3. \u5386\u53f2\u8bb0\u5f55\u7ba1\u7406\n\n\u4ea4\u4e92\u8282\u70b9\u9700\u8981\u6b63\u786e\u5904\u7406\u5386\u53f2\u8bb0\u5f55:\n\n```typescript\nreturn {\n  // \u79fb\u9664\u4ea4\u4e92\u5bf9\u8bdd\u7684\u5386\u53f2\u8bb0\u5f55(\u7528\u6237\u95ee\u9898 + \u7cfb\u7edf\u54cd\u5e94)\n  [DispatchNodeResponseKeyEnum.rewriteHistories]: histories.slice(0, -2),\n  // ... \u5176\u4ed6\u8fd4\u56de\u503c\n};\n```\n\n### 4. Skip \u8282\u70b9\u961f\u5217\n\n\u4ea4\u4e92\u8282\u70b9\u89e6\u53d1\u65f6,\u7cfb\u7edf\u4f1a\u4fdd\u5b58 `skipNodeQueue` \u4ee5\u4fbf\u6062\u590d\u65f6\u8df3\u8fc7\u5df2\u5904\u7406\u7684\u8282\u70b9\u3002\n\n### 5. \u5de5\u5177\u8c03\u7528\u652f\u6301\n\n\u5982\u679c\u8282\u70b9\u9700\u8981\u5728\u5de5\u5177\u8c03\u7528\u4e2d\u4f7f\u7528,\u8bbe\u7f6e `isTool: true`\u3002\n\n## \u6d4b\u8bd5\u6e05\u5355\n\n\u5f00\u53d1\u5b8c\u6210\u540e,\u8bf7\u6d4b\u8bd5\u4ee5\u4e0b\u573a\u666f:\n\n- [ ] \u8282\u70b9\u5728\u5de5\u4f5c\u6d41\u7f16\u8f91\u5668\u4e2d\u6b63\u5e38\u663e\u793a\n- [ ] \u8282\u70b9\u914d\u7f6e\u4fdd\u5b58\u548c\u52a0\u8f7d\u6b63\u786e\n- [ ] \u4ea4\u4e92\u8bf7\u6c42\u6b63\u786e\u53d1\u9001\u5230\u524d\u7aef\n- [ ] \u524d\u7aef\u7ec4\u4ef6\u6b63\u786e\u6e32\u67d3\u4ea4\u4e92\u754c\u9762\n- [ ] \u7528\u6237\u8f93\u5165\u6b63\u786e\u4f20\u56de\u540e\u7aef\n- [ ] \u5de5\u4f5c\u6d41\u6b63\u786e\u6062\u590d\u5e76\u7ee7\u7eed\u6267\u884c\n- [ ] \u5386\u53f2\u8bb0\u5f55\u6b63\u786e\u66f4\u65b0\n- [ ] \u8282\u70b9\u8f93\u51fa\u6b63\u786e\u8fde\u63a5\u5230\u540e\u7eed\u8282\u70b9\n- [ ] \u9519\u8bef\u60c5\u51b5\u5904\u7406\u6b63\u786e\n- [ ] \u591a\u8bed\u8a00\u652f\u6301\u5b8c\u6574\n\n## \u53c2\u8003\u5b9e\u73b0\n\n\u53ef\u4ee5\u53c2\u8003\u4ee5\u4e0b\u73b0\u6709\u5b9e\u73b0:\n\n1. **\u7b80\u5355\u5355\u9009**: `userSelect` \u8282\u70b9\n   - \u7c7b\u578b\u5b9a\u4e49: `packages/global/core/workflow/template/system/interactive/type.d.ts:48-55`\n   - \u6267\u884c\u903b\u8f91: `packages/service/core/workflow/dispatch/interactive/userSelect.ts`\n   - \u524d\u7aef\u7ec4\u4ef6: `projects/app/src/components/core/chat/components/Interactive/InteractiveComponents.tsx:29-63`\n\n2. **\u590d\u6742\u8868\u5355**: `formInput` \u8282\u70b9\n   - \u7c7b\u578b\u5b9a\u4e49: `packages/global/core/workflow/template/system/interactive/type.d.ts:57-82`\n   - \u6267\u884c\u903b\u8f91: `packages/service/core/workflow/dispatch/interactive/formInput.ts`\n   - \u524d\u7aef\u7ec4\u4ef6: `projects/app/src/components/core/chat/components/Interactive/InteractiveComponents.tsx:65-126`\n\n## \u5e38\u89c1\u95ee\u9898\n\n### Q: \u4ea4\u4e92\u8282\u70b9\u6267\u884c\u4e86\u4e24\u6b21?\nA: \u8fd9\u662f\u6b63\u5e38\u7684\u3002\u7b2c\u4e00\u6b21\u8fd4\u56de\u4ea4\u4e92\u8bf7\u6c42,\u7b2c\u4e8c\u6b21\u5904\u7406\u7528\u6237\u8f93\u5165\u3002\u786e\u4fdd\u5728\u7b2c\u4e8c\u6b21\u6267\u884c\u65f6\u8bbe\u7f6e `node.isEntry = false`\u3002\n\n### Q: \u5de5\u4f5c\u6d41\u6062\u590d\u540e\u6ca1\u6709\u7ee7\u7eed\u6267\u884c?\nA: \u68c0\u67e5\u4f60\u7684\u8282\u70b9\u7c7b\u578b\u662f\u5426\u5728 `isEntry` \u767d\u540d\u5355\u4e2d(dispatch/index.ts:1013-1018)\u3002\n\n### Q: \u7528\u6237\u8f93\u5165\u683c\u5f0f\u4e0d\u5bf9?\nA: \u68c0\u67e5 `chatValue2RuntimePrompt` \u7684\u8fd4\u56de\u503c,\u6839\u636e\u4f60\u7684\u6570\u636e\u683c\u5f0f\u8fdb\u884c\u89e3\u6790\u3002\n\n### Q: \u5982\u4f55\u652f\u6301\u591a\u4e2a\u4ea4\u4e92\u8282\u70b9\u4e32\u8054?\nA: \u6bcf\u4e2a\u4ea4\u4e92\u8282\u70b9\u90fd\u4f1a\u6682\u505c\u5de5\u4f5c\u6d41,\u7528\u6237\u5b8c\u6210\u540e\u4f1a\u81ea\u52a8\u7ee7\u7eed\u5230\u4e0b\u4e00\u4e2a\u8282\u70b9\u3002\n\n## \u6587\u4ef6\u6e05\u5355\u603b\u7ed3\n\n\u5f00\u53d1\u65b0\u4ea4\u4e92\u8282\u70b9\u9700\u8981\u4fee\u6539/\u521b\u5efa\u4ee5\u4e0b\u6587\u4ef6:\n\n### \u540e\u7aef\u6838\u5fc3\u6587\u4ef6\n1. `packages/global/core/workflow/template/system/interactive/type.d.ts` - \u7c7b\u578b\u5b9a\u4e49\n2. `packages/global/core/workflow/node/constant.ts` - \u8282\u70b9\u679a\u4e3e\n3. `packages/global/core/workflow/template/system/interactive/yourNode.ts` - \u8282\u70b9\u6a21\u677f\n4. `packages/service/core/workflow/dispatch/interactive/yourNode.ts` - \u6267\u884c\u903b\u8f91\n5. `packages/service/core/workflow/dispatch/constants.ts` - \u56de\u8c03\u6ce8\u518c\n6. `packages/service/core/workflow/dispatch/index.ts` - isEntry \u767d\u540d\u5355\n\n### \u524d\u7aef\u7ec4\u4ef6\u6587\u4ef6\n7. `projects/app/src/components/core/chat/components/Interactive/InteractiveComponents.tsx` - \u804a\u5929\u4ea4\u4e92\u7ec4\u4ef6\n8. `projects/app/src/pageComponents/app/detail/WorkflowComponents/Flow/nodes/NodeYourNode.tsx` - \u5de5\u4f5c\u6d41\u7f16\u8f91\u5668\u7ec4\u4ef6\n\n### \u56fd\u9645\u5316\u6587\u4ef6\n9. `packages/web/i18n/zh-CN/app.json` - \u4e2d\u6587\u7ffb\u8bd1\n10. `packages/web/i18n/en/app.json` - \u82f1\u6587\u7ffb\u8bd1\n11. `packages/web/i18n/zh-Hant/app.json` - \u7e41\u4f53\u4e2d\u6587\u7ffb\u8bd1\n\n## \u9644\u5f55:\u5173\u952e\u8f93\u5165\u8f93\u51fa\u952e\u5b9a\u4e49\n\n\u5982\u679c\u9700\u8981\u65b0\u7684\u8f93\u5165\u8f93\u51fa\u952e,\u5728\u4ee5\u4e0b\u6587\u4ef6\u4e2d\u5b9a\u4e49:\n\n**\u6587\u4ef6**: `packages/global/core/workflow/constants.ts`\n\n```typescript\nexport enum NodeInputKeyEnum {\n  // ... \u73b0\u6709\u952e\n  yourInputKey = ''''yourInputKey'''',\n}\n\nexport enum NodeOutputKeyEnum {\n  // ... \u73b0\u6709\u952e\n  yourOutputKey = ''''yourOutputKey'''',\n}\n```\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 13, completeness: 10, problem_definition: 0',
  'Score: 76, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 18, error_handling: 15',
  'https://skillsmp.com/skills/labring-fastgpt-claude-skills-core-app-workflow-inteactive-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'secrets-management - Implement secure secrets management for CI/CD pipelines using Vault, AWS Secrets Manager, or native platform solutions. Use when handling sensitive cr',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Secrets Management\n\nSecure secrets management practices for CI/CD pipelines using Vault, AWS Secrets Manager, and other tools.\n\n## Purpose\n\nImplement secure secrets management in CI/CD pipelines without hardcoding sensitive information.\n\n## When to Use\n\n- Store API keys and credentials\n- Manage database passwords\n- Handle TLS certificates\n- Rotate secrets automatically\n- Implement least-privilege access\n\n## Secrets Management Tools\n\n### HashiCorp Vault\n- Centralized secrets management\n- Dynamic secrets generation\n- Secret rotation\n- Audit logging\n- Fine-grained access control\n\n### AWS Secrets Manager\n- AWS-native solution\n- Automatic rotation\n- Integration with RDS\n- CloudFormation support\n\n### Azure Key Vault\n- Azure-native solution\n- HSM-backed keys\n- Certificate management\n- RBAC integration\n\n### Google Secret Manager\n- GCP-native solution\n- Versioning\n- IAM integration\n\n## HashiCorp Vault Integration\n\n### Setup Vault\n\n```bash\n# Start Vault dev server\nvault server -dev\n\n# Set environment\nexport VAULT_ADDR=''''http://127.0.0.1:8200''''\nexport VAULT_TOKEN=''''root''''\n\n# Enable secrets engine\nvault secrets enable -path=secret kv-v2\n\n# Store secret\nvault kv put secret/database/config username=admin password=secret\n```\n\n### GitHub Actions with Vault\n\n```yaml\nname: Deploy with Vault Secrets\n\non: [push]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Import Secrets from Vault\n      uses: hashicorp/vault-action@v2\n      with:\n        url: https://vault.example.com:8200\n        token: ${{ secrets.VAULT_TOKEN }}\n        secrets: |\n          secret/data/database username | DB_USERNAME ;\n          secret/data/database password | DB_PASSWORD ;\n          secret/data/api key | API_KEY\n\n    - name: Use secrets\n      run: |\n        echo \"Connecting to database as $DB_USERNAME\"\n        # Use $DB_PASSWORD, $API_KEY\n```\n\n### GitLab CI with Vault\n\n```yaml\ndeploy:\n  image: vault:latest\n  before_script:\n    - export VAULT_ADDR=https://vault.example.com:8200\n    - export VAULT_TOKEN=$VAULT_TOKEN\n    - apk add curl jq\n  script:\n    - |\n      DB_PASSWORD=$(vault kv get -field=password secret/database/config)\n      API_KEY=$(vault kv get -field=key secret/api/credentials)\n      echo \"Deploying with secrets...\"\n      # Use $DB_PASSWORD, $API_KEY\n```\n\n**Reference:** See `references/vault-setup.md`\n\n## AWS Secrets Manager\n\n### Store Secret\n\n```bash\naws secretsmanager create-secret \\\n  --name production/database/password \\\n  --secret-string \"super-secret-password\"\n```\n\n### Retrieve in GitHub Actions\n\n```yaml\n- name: Configure AWS credentials\n  uses: aws-actions/configure-aws-credentials@v4\n  with:\n    aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n    aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n    aws-region: us-west-2\n\n- name: Get secret from AWS\n  run: |\n    SECRET=$(aws secretsmanager get-secret-value \\\n      --secret-id production/database/password \\\n      --query SecretString \\\n      --output text)\n    echo \"::add-mask::$SECRET\"\n    echo \"DB_PASSWORD=$SECRET\" >> $GITHUB_ENV\n\n- name: Use secret\n  run: |\n    # Use $DB_PASSWORD\n    ./deploy.sh\n```\n\n### Terraform with AWS Secrets Manager\n\n```hcl\ndata \"aws_secretsmanager_secret_version\" \"db_password\" {\n  secret_id = \"production/database/password\"\n}\n\nresource \"aws_db_instance\" \"main\" {\n  allocated_storage    = 100\n  engine              = \"postgres\"\n  instance_class      = \"db.t3.large\"\n  username            = \"admin\"\n  password            = jsondecode(data.aws_secretsmanager_secret_version.db_password.secret_string)[\"password\"]\n}\n```\n\n## GitHub Secrets\n\n### Organization/Repository Secrets\n\n```yaml\n- name: Use GitHub secret\n  run: |\n    echo \"API Key: ${{ secrets.API_KEY }}\"\n    echo \"Database URL: ${{ secrets.DATABASE_URL }}\"\n```\n\n### Environment Secrets\n\n```yaml\ndeploy:\n  runs-on: ubuntu-latest\n  environment: production\n  steps:\n  - name: Deploy\n    run: |\n      echo \"Deploying with ${{ secrets.PROD_API_KEY }}\"\n```\n\n**Reference:** See `references/github-secrets.md`\n\n## GitLab CI/CD Variables\n\n### Project Variables\n\n```yaml\ndeploy:\n  script:\n    - echo \"Deploying with $API_KEY\"\n    - echo \"Database: $DATABASE_URL\"\n```\n\n### Protected and Masked Variables\n- Protected: Only available in protected branches\n- Masked: Hidden in job logs\n- File type: Stored as file\n\n## Best Practices\n\n1. **Never commit secrets** to Git\n2. **Use different secrets** per environment\n3. **Rotate secrets regularly**\n4. **Implement least-privilege access**\n5. **Enable audit logging**\n6. **Use secret scanning** (GitGuardian, TruffleHog)\n7. **Mask secrets in logs**\n8. **Encrypt secrets at rest**\n9. **Use short-lived tokens** when possible\n10. **Document secret requirements**\n\n## Secret Rotation\n\n### Automated Rotation with AWS\n\n```python\nimport boto3\nimport json\n\ndef lambda_handler(event, context):\n    client = boto3.client(''''secretsmanager'''')\n\n    # Get current secret\n    response = client.get_secret_value(SecretId=''''my-secret'''')\n    current_secret = json.loads(response[''''SecretString''''])\n\n    # Generate new password\n    new_password = generate_strong_password()\n\n    # Update database password\n    update_database_password(new_password)\n\n    # Update secret\n    client.put_secret_value(\n        SecretId=''''my-secret'''',\n        SecretString=json.dumps({\n            ''''username'''': current_secret[''''username''''],\n            ''''password'''': new_password\n        })\n    )\n\n    return {''''statusCode'''': 200}\n```\n\n### Manual Rotation Process\n\n1. Generate new secret\n2. Update secret in secret store\n3. Update applications to use new secret\n4. Verify functionality\n5. Revoke old secret\n\n## External Secrets Operator\n\n### Kubernetes Integration\n\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: SecretStore\nmetadata:\n  name: vault-backend\n  namespace: production\nspec:\n  provider:\n    vault:\n      server: \"https://vault.example.com:8200\"\n      path: \"secret\"\n      version: \"v2\"\n      auth:\n        kubernetes:\n          mountPath: \"kubernetes\"\n          role: \"production\"\n\n---\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: database-credentials\n  namespace: production\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: vault-backend\n    kind: SecretStore\n  target:\n    name: database-credentials\n    creationPolicy: Owner\n  data:\n  - secretKey: username\n    remoteRef:\n      key: database/config\n      property: username\n  - secretKey: password\n    remoteRef:\n      key: database/config\n      property: password\n```\n\n## Secret Scanning\n\n### Pre-commit Hook\n\n```bash\n#!/bin/bash\n# .git/hooks/pre-commit\n\n# Check for secrets with TruffleHog\ndocker run --rm -v \"$(pwd):/repo\" \\\n  trufflesecurity/trufflehog:latest \\\n  filesystem --directory=/repo\n\nif [ $? -ne 0 ]; then\n  echo \"\u274c Secret detected! Commit blocked.\"\n  exit 1\nfi\n```\n\n### CI/CD Secret Scanning\n\n```yaml\nsecret-scan:\n  stage: security\n  image: trufflesecurity/trufflehog:latest\n  script:\n    - trufflehog filesystem .\n  allow_failure: false\n```\n\n## Reference Files\n\n- `references/vault-setup.md` - HashiCorp Vault configuration\n- `references/github-secrets.md` - GitHub Secrets best practices\n\n## Related Skills\n\n- `github-actions-templates` - For GitHub Actions integration\n- `gitlab-ci-patterns` - For GitLab CI integration\n- `deployment-pipeline-design` - For pipeline architecture\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 8',
  'Score: 90, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-cicd-automation-skills-secrets-management-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'error-handling-patterns - Master error handling patterns across languages including exceptions, Result types, error propagation, and graceful degradation to build resilient app',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Error Handling Patterns\n\nBuild resilient applications with robust error handling strategies that gracefully handle failures and provide excellent debugging experiences.\n\n## When to Use This Skill\n\n- Implementing error handling in new features\n- Designing error-resilient APIs\n- Debugging production issues\n- Improving application reliability\n- Creating better error messages for users and developers\n- Implementing retry and circuit breaker patterns\n- Handling async/concurrent errors\n- Building fault-tolerant distributed systems\n\n## Core Concepts\n\n### 1. Error Handling Philosophies\n\n**Exceptions vs Result Types:**\n- **Exceptions**: Traditional try-catch, disrupts control flow\n- **Result Types**: Explicit success/failure, functional approach\n- **Error Codes**: C-style, requires discipline\n- **Option/Maybe Types**: For nullable values\n\n**When to Use Each:**\n- Exceptions: Unexpected errors, exceptional conditions\n- Result Types: Expected errors, validation failures\n- Panics/Crashes: Unrecoverable errors, programming bugs\n\n### 2. Error Categories\n\n**Recoverable Errors:**\n- Network timeouts\n- Missing files\n- Invalid user input\n- API rate limits\n\n**Unrecoverable Errors:**\n- Out of memory\n- Stack overflow\n- Programming bugs (null pointer, etc.)\n\n## Language-Specific Patterns\n\n### Python Error Handling\n\n**Custom Exception Hierarchy:**\n```python\nclass ApplicationError(Exception):\n    \"\"\"Base exception for all application errors.\"\"\"\n    def __init__(self, message: str, code: str = None, details: dict = None):\n        super().__init__(message)\n        self.code = code\n        self.details = details or {}\n        self.timestamp = datetime.utcnow()\n\nclass ValidationError(ApplicationError):\n    \"\"\"Raised when validation fails.\"\"\"\n    pass\n\nclass NotFoundError(ApplicationError):\n    \"\"\"Raised when resource not found.\"\"\"\n    pass\n\nclass ExternalServiceError(ApplicationError):\n    \"\"\"Raised when external service fails.\"\"\"\n    def __init__(self, message: str, service: str, **kwargs):\n        super().__init__(message, **kwargs)\n        self.service = service\n\n# Usage\ndef get_user(user_id: str) -> User:\n    user = db.query(User).filter_by(id=user_id).first()\n    if not user:\n        raise NotFoundError(\n            f\"User not found\",\n            code=\"USER_NOT_FOUND\",\n            details={\"user_id\": user_id}\n        )\n    return user\n```\n\n**Context Managers for Cleanup:**\n```python\nfrom contextlib import contextmanager\n\n@contextmanager\ndef database_transaction(session):\n    \"\"\"Ensure transaction is committed or rolled back.\"\"\"\n    try:\n        yield session\n        session.commit()\n    except Exception as e:\n        session.rollback()\n        raise\n    finally:\n        session.close()\n\n# Usage\nwith database_transaction(db.session) as session:\n    user = User(name=\"Alice\")\n    session.add(user)\n    # Automatic commit or rollback\n```\n\n**Retry with Exponential Backoff:**\n```python\nimport time\nfrom functools import wraps\nfrom typing import TypeVar, Callable\n\nT = TypeVar(''''T'''')\n\ndef retry(\n    max_attempts: int = 3,\n    backoff_factor: float = 2.0,\n    exceptions: tuple = (Exception,)\n):\n    \"\"\"Retry decorator with exponential backoff.\"\"\"\n    def decorator(func: Callable[..., T]) -> Callable[..., T]:\n        @wraps(func)\n        def wrapper(*args, **kwargs) -> T:\n            last_exception = None\n            for attempt in range(max_attempts):\n                try:\n                    return func(*args, **kwargs)\n                except exceptions as e:\n                    last_exception = e\n                    if attempt < max_attempts - 1:\n                        sleep_time = backoff_factor ** attempt\n                        time.sleep(sleep_time)\n                        continue\n                    raise\n            raise last_exception\n        return wrapper\n    return decorator\n\n# Usage\n@retry(max_attempts=3, exceptions=(NetworkError,))\ndef fetch_data(url: str) -> dict:\n    response = requests.get(url, timeout=5)\n    response.raise_for_status()\n    return response.json()\n```\n\n### TypeScript/JavaScript Error Handling\n\n**Custom Error Classes:**\n```typescript\n// Custom error classes\nclass ApplicationError extends Error {\n    constructor(\n        message: string,\n        public code: string,\n        public statusCode: number = 500,\n        public details?: Record<string, any>\n    ) {\n        super(message);\n        this.name = this.constructor.name;\n        Error.captureStackTrace(this, this.constructor);\n    }\n}\n\nclass ValidationError extends ApplicationError {\n    constructor(message: string, details?: Record<string, any>) {\n        super(message, ''''VALIDATION_ERROR'''', 400, details);\n    }\n}\n\nclass NotFoundError extends ApplicationError {\n    constructor(resource: string, id: string) {\n        super(\n            `${resource} not found`,\n            ''''NOT_FOUND'''',\n            404,\n            { resource, id }\n        );\n    }\n}\n\n// Usage\nfunction getUser(id: string): User {\n    const user = users.find(u => u.id === id);\n    if (!user) {\n        throw new NotFoundError(''''User'''', id);\n    }\n    return user;\n}\n```\n\n**Result Type Pattern:**\n```typescript\n// Result type for explicit error handling\ntype Result<T, E = Error> =\n    | { ok: true; value: T }\n    | { ok: false; error: E };\n\n// Helper functions\nfunction Ok<T>(value: T): Result<T, never> {\n    return { ok: true, value };\n}\n\nfunction Err<E>(error: E): Result<never, E> {\n    return { ok: false, error };\n}\n\n// Usage\nfunction parseJSON<T>(json: string): Result<T, SyntaxError> {\n    try {\n        const value = JSON.parse(json) as T;\n        return Ok(value);\n    } catch (error) {\n        return Err(error as SyntaxError);\n    }\n}\n\n// Consuming Result\nconst result = parseJSON<User>(userJson);\nif (result.ok) {\n    console.log(result.value.name);\n} else {\n    console.error(''''Parse failed:'''', result.error.message);\n}\n\n// Chaining Results\nfunction chain<T, U, E>(\n    result: Result<T, E>,\n    fn: (value: T) => Result<U, E>\n): Result<U, E> {\n    return result.ok ? fn(result.value) : result;\n}\n```\n\n**Async Error Handling:**\n```typescript\n// Async/await with proper error handling\nasync function fetchUserOrders(userId: string): Promise<Order[]> {\n    try {\n        const user = await getUser(userId);\n        const orders = await getOrders(user.id);\n        return orders;\n    } catch (error) {\n        if (error instanceof NotFoundError) {\n            return [];  // Return empty array for not found\n        }\n        if (error instanceof NetworkError) {\n            // Retry logic\n            return retryFetchOrders(userId);\n        }\n        // Re-throw unexpected errors\n        throw error;\n    }\n}\n\n// Promise error handling\nfunction fetchData(url: string): Promise<Data> {\n    return fetch(url)\n        .then(response => {\n            if (!response.ok) {\n                throw new NetworkError(`HTTP ${response.status}`);\n            }\n            return response.json();\n        })\n        .catch(error => {\n            console.error(''''Fetch failed:'''', error);\n            throw error;\n        });\n}\n```\n\n### Rust Error Handling\n\n**Result and Option Types:**\n```rust\nuse std::fs::File;\nuse std::io::{self, Read};\n\n// Result type for operations that can fail\nfn read_file(path: &str) -> Result<String, io::Error> {\n    let mut file = File::open(path)?;  // ? operator propagates errors\n    let mut contents = String::new();\n    file.read_to_string(&mut contents)?;\n    Ok(contents)\n}\n\n// Custom error types\n#[derive(Debug)]\nenum AppError {\n    Io(io::Error),\n    Parse(std::num::ParseIntError),\n    NotFound(String),\n    Validation(String),\n}\n\nimpl From<io::Error> for AppError {\n    fn from(error: io::Error) -> Self {\n        AppError::Io(error)\n    }\n}\n\n// Using custom error type\nfn read_number_from_file(path: &str) -> Result<i32, AppError> {\n    let contents = read_file(path)?;  // Auto-converts io::Error\n    let number = contents.trim().parse()\n        .map_err(AppError::Parse)?;   // Explicitly convert ParseIntError\n    Ok(number)\n}\n\n// Option for nullable values\nfn find_user(id: &str) -> Option<User> {\n    users.iter().find(|u| u.id == id).cloned()\n}\n\n// Combining Option and Result\nfn get_user_age(id: &str) -> Result<u32, AppError> {\n    find_user(id)\n        .ok_or_else(|| AppError::NotFound(id.to_string()))\n        .map(|user| user.age)\n}\n```\n\n### Go Error Handling\n\n**Explicit Error Returns:**\n```go\n// Basic error handling\nfunc getUser(id string) (*User, error) {\n    user, err := db.QueryUser(id)\n    if err != nil {\n        return nil, fmt.Errorf(\"failed to query user: %w\", err)\n    }\n    if user == nil {\n        return nil, errors.New(\"user not found\")\n    }\n    return user, nil\n}\n\n// Custom error types\ntype ValidationError struct {\n    Field   string\n    Message string\n}\n\nfunc (e *ValidationError) Error() string {\n    return fmt.Sprintf(\"validation failed for %s: %s\", e.Field, e.Message)\n}\n\n// Sentinel errors for comparison\nvar (\n    ErrNotFound     = errors.New(\"not found\")\n    ErrUnauthorized = errors.New(\"unauthorized\")\n    ErrInvalidInput = errors.New(\"invalid input\")\n)\n\n// Error checking\nuser, err := getUser(\"123\")\nif err != nil {\n    if errors.Is(err, ErrNotFound) {\n        // Handle not found\n    } else {\n        // Handle other errors\n    }\n}\n\n// Error wrapping and unwrapping\nfunc processUser(id string) error {\n    user, err := getUser(id)\n    if err != nil {\n        return fmt.Errorf(\"process user failed: %w\", err)\n    }\n    // Process user\n    return nil\n}\n\n// Unwrap errors\nerr := processUser(\"123\")\nif err != nil {\n    var valErr *ValidationError\n    if errors.As(err, &valErr) {\n        fmt.Printf(\"Validation error: %s\\n\", valErr.Field)\n    }\n}\n```\n\n## Universal Patterns\n\n### Pattern 1: Circuit Breaker\n\nPrevent cascading failures in distributed systems.\n\n```python\nfrom enum import Enum\nfrom datetime import datetime, timedelta\nfrom typing import Callable, TypeVar\n\nT = TypeVar(''''T'''')\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"       # Normal operation\n    OPEN = \"open\"          # Failing, reject requests\n    HALF_OPEN = \"half_open\"  # Testing if recovered\n\nclass CircuitBreaker:\n    def __init__(\n        self,\n        failure_threshold: int = 5,\n        timeout: timedelta = timedelta(seconds=60),\n        success_threshold: int = 2\n    ):\n        self.failure_threshold = failure_threshold\n        self.timeout = timeout\n        self.success_threshold = success_threshold\n        self.failure_count = 0\n        self.success_count = 0\n        self.state = CircuitState.CLOSED\n        self.last_failure_time = None\n\n    def call(self, func: Callable[[], T]) -> T:\n        if self.state == CircuitState.OPEN:\n            if datetime.now() - self.last_failure_time > self.timeout:\n                self.state = CircuitState.HALF_OPEN\n                self.success_count = 0\n            else:\n                raise Exception(\"Circuit breaker is OPEN\")\n\n        try:\n            result = func()\n            self.on_success()\n            return result\n        except Exception as e:\n            self.on_failure()\n            raise\n\n    def on_success(self):\n        self.failure_count = 0\n        if self.state == CircuitState.HALF_OPEN:\n            self.success_count += 1\n            if self.success_count >= self.success_threshold:\n                self.state = CircuitState.CLOSED\n                self.success_count = 0\n\n    def on_failure(self):\n        self.failure_count += 1\n        self.last_failure_time = datetime.now()\n        if self.failure_count >= self.failure_threshold:\n            self.state = CircuitState.OPEN\n\n# Usage\ncircuit_breaker = CircuitBreaker()\n\ndef fetch_data():\n    return circuit_breaker.call(lambda: external_api.get_data())\n```\n\n### Pattern 2: Error Aggregation\n\nCollect multiple errors instead of failing on first error.\n\n```typescript\nclass ErrorCollector {\n    private errors: Error[] = [];\n\n    add(error: Error): void {\n        this.errors.push(error);\n    }\n\n    hasErrors(): boolean {\n        return this.errors.length > 0;\n    }\n\n    getErrors(): Error[] {\n        return [...this.errors];\n    }\n\n    throw(): never {\n        if (this.errors.length === 1) {\n            throw this.errors[0];\n        }\n        throw new AggregateError(\n            this.errors,\n            `${this.errors.length} errors occurred`\n        );\n    }\n}\n\n// Usage: Validate multiple fields\nfunction validateUser(data: any): User {\n    const errors = new ErrorCollector();\n\n    if (!data.email) {\n        errors.add(new ValidationError(''''Email is required''''));\n    } else if (!isValidEmail(data.email)) {\n        errors.add(new ValidationError(''''Email is invalid''''));\n    }\n\n    if (!data.name || data.name.length < 2) {\n        errors.add(new ValidationError(''''Name must be at least 2 characters''''));\n    }\n\n    if (!data.age || data.age < 18) {\n        errors.add(new ValidationError(''''Age must be 18 or older''''));\n    }\n\n    if (errors.hasErrors()) {\n        errors.throw();\n    }\n\n    return data as User;\n}\n```\n\n### Pattern 3: Graceful Degradation\n\nProvide fallback functionality when errors occur.\n\n```python\nfrom typing import Optional, Callable, TypeVar\n\nT = TypeVar(''''T'''')\n\ndef with_fallback(\n    primary: Callable[[], T],\n    fallback: Callable[[], T],\n    log_error: bool = True\n) -> T:\n    \"\"\"Try primary function, fall back to fallback on error.\"\"\"\n    try:\n        return primary()\n    except Exception as e:\n        if log_error:\n            logger.error(f\"Primary function failed: {e}\")\n        return fallback()\n\n# Usage\ndef get_user_profile(user_id: str) -> UserProfile:\n    return with_fallback(\n        primary=lambda: fetch_from_cache(user_id),\n        fallback=lambda: fetch_from_database(user_id)\n    )\n\n# Multiple fallbacks\ndef get_exchange_rate(currency: str) -> float:\n    return (\n        try_function(lambda: api_provider_1.get_rate(currency))\n        or try_function(lambda: api_provider_2.get_rate(currency))\n        or try_function(lambda: cache.get_rate(currency))\n        or DEFAULT_RATE\n    )\n\ndef try_function(func: Callable[[], Optional[T]]) -> Optional[T]:\n    try:\n        return func()\n    except Exception:\n        return None\n```\n\n## Best Practices\n\n1. **Fail Fast**: Validate input early, fail quickly\n2. **Preserve Context**: Include stack traces, metadata, timestamps\n3. **Meaningful Messages**: Explain what happened and how to fix it\n4. **Log Appropriately**: Error = log, expected failure = don''''t spam logs\n5. **Handle at Right Level**: Catch where you can meaningfully handle\n6. **Clean Up Resources**: Use try-finally, context managers, defer\n7. **Don''''t Swallow Errors**: Log or re-throw, don''''t silently ignore\n8. **Type-Safe Errors**: Use typed errors when possible\n\n```python\n# Good error handling example\ndef process_order(order_id: str) -> Order:\n    \"\"\"Process order with comprehensive error handling.\"\"\"\n    try:\n        # Validate input\n        if not order_id:\n            raise ValidationError(\"Order ID is required\")\n\n        # Fetch order\n        order = db.get_order(order_id)\n        if not order:\n            raise NotFoundError(\"Order\", order_id)\n\n        # Process payment\n        try:\n            payment_result = payment_service.charge(order.total)\n        except PaymentServiceError as e:\n            # Log and wrap external service error\n            logger.error(f\"Payment failed for order {order_id}: {e}\")\n            raise ExternalServiceError(\n                f\"Payment processing failed\",\n                service=\"payment_service\",\n                details={\"order_id\": order_id, \"amount\": order.total}\n            ) from e\n\n        # Update order\n        order.status = \"completed\"\n        order.payment_id = payment_result.id\n        db.save(order)\n\n        return order\n\n    except ApplicationError:\n        # Re-raise known application errors\n        raise\n    except Exception as e:\n        # Log unexpected errors\n        logger.exception(f\"Unexpected error processing order {order_id}\")\n        raise ApplicationError(\n            \"Order processing failed\",\n            code=\"INTERNAL_ERROR\"\n        ) from e\n```\n\n## Common Pitfalls\n\n- **Catching Too Broadly**: `except Exception` hides bugs\n- **Empty Catch Blocks**: Silently swallowing errors\n- **Logging and Re-throwing**: Creates duplicate log entries\n- **Not Cleaning Up**: Forgetting to close files, connections\n- **Poor Error Messages**: \"Error occurred\" is not helpful\n- **Returning Error Codes**: Use exceptions or Result types\n- **Ignoring Async Errors**: Unhandled promise rejections\n\n## Resources\n\n- **references/exception-hierarchy-design.md**: Designing error class hierarchies\n- **references/error-recovery-strategies.md**: Recovery patterns for different scenarios\n- **references/async-error-handling.md**: Handling errors in concurrent code\n- **assets/error-handling-checklist.md**: Review checklist for error handling\n- **assets/error-message-guide.md**: Writing helpful error messages\n- **scripts/error-analyzer.py**: Analyze error patterns in logs\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 15, completeness: 10',
  'Score: 90, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-developer-essentials-skills-error-handling-patterns-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'distributed-tracing - Implement distributed tracing with Jaeger and Tempo to track requests across microservices and identify performance bottlenecks. Use when debugging mi',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Distributed Tracing\n\nImplement distributed tracing with Jaeger and Tempo for request flow visibility across microservices.\n\n## Purpose\n\nTrack requests across distributed systems to understand latency, dependencies, and failure points.\n\n## When to Use\n\n- Debug latency issues\n- Understand service dependencies\n- Identify bottlenecks\n- Trace error propagation\n- Analyze request paths\n\n## Distributed Tracing Concepts\n\n### Trace Structure\n```\nTrace (Request ID: abc123)\n  \u2193\nSpan (frontend) [100ms]\n  \u2193\nSpan (api-gateway) [80ms]\n  \u251c\u2192 Span (auth-service) [10ms]\n  \u2514\u2192 Span (user-service) [60ms]\n      \u2514\u2192 Span (database) [40ms]\n```\n\n### Key Components\n- **Trace** - End-to-end request journey\n- **Span** - Single operation within a trace\n- **Context** - Metadata propagated between services\n- **Tags** - Key-value pairs for filtering\n- **Logs** - Timestamped events within a span\n\n## Jaeger Setup\n\n### Kubernetes Deployment\n\n```bash\n# Deploy Jaeger Operator\nkubectl create namespace observability\nkubectl create -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.51.0/jaeger-operator.yaml -n observability\n\n# Deploy Jaeger instance\nkubectl apply -f - <<EOF\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: jaeger\n  namespace: observability\nspec:\n  strategy: production\n  storage:\n    type: elasticsearch\n    options:\n      es:\n        server-urls: http://elasticsearch:9200\n  ingress:\n    enabled: true\nEOF\n```\n\n### Docker Compose\n\n```yaml\nversion: ''''3.8''''\nservices:\n  jaeger:\n    image: jaegertracing/all-in-one:latest\n    ports:\n      - \"5775:5775/udp\"\n      - \"6831:6831/udp\"\n      - \"6832:6832/udp\"\n      - \"5778:5778\"\n      - \"16686:16686\"  # UI\n      - \"14268:14268\"  # Collector\n      - \"14250:14250\"  # gRPC\n      - \"9411:9411\"    # Zipkin\n    environment:\n      - COLLECTOR_ZIPKIN_HOST_PORT=:9411\n```\n\n**Reference:** See `references/jaeger-setup.md`\n\n## Application Instrumentation\n\n### OpenTelemetry (Recommended)\n\n#### Python (Flask)\n```python\nfrom opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom flask import Flask\n\n# Initialize tracer\nresource = Resource(attributes={SERVICE_NAME: \"my-service\"})\nprovider = TracerProvider(resource=resource)\nprocessor = BatchSpanProcessor(JaegerExporter(\n    agent_host_name=\"jaeger\",\n    agent_port=6831,\n))\nprovider.add_span_processor(processor)\ntrace.set_tracer_provider(provider)\n\n# Instrument Flask\napp = Flask(__name__)\nFlaskInstrumentor().instrument_app(app)\n\n@app.route(''''/api/users'''')\ndef get_users():\n    tracer = trace.get_tracer(__name__)\n\n    with tracer.start_as_current_span(\"get_users\") as span:\n        span.set_attribute(\"user.count\", 100)\n        # Business logic\n        users = fetch_users_from_db()\n        return {\"users\": users}\n\ndef fetch_users_from_db():\n    tracer = trace.get_tracer(__name__)\n\n    with tracer.start_as_current_span(\"database_query\") as span:\n        span.set_attribute(\"db.system\", \"postgresql\")\n        span.set_attribute(\"db.statement\", \"SELECT * FROM users\")\n        # Database query\n        return query_database()\n```\n\n#### Node.js (Express)\n```javascript\nconst { NodeTracerProvider } = require(''''@opentelemetry/sdk-trace-node'''');\nconst { JaegerExporter } = require(''''@opentelemetry/exporter-jaeger'''');\nconst { BatchSpanProcessor } = require(''''@opentelemetry/sdk-trace-base'''');\nconst { registerInstrumentations } = require(''''@opentelemetry/instrumentation'''');\nconst { HttpInstrumentation } = require(''''@opentelemetry/instrumentation-http'''');\nconst { ExpressInstrumentation } = require(''''@opentelemetry/instrumentation-express'''');\n\n// Initialize tracer\nconst provider = new NodeTracerProvider({\n  resource: { attributes: { ''''service.name'''': ''''my-service'''' } }\n});\n\nconst exporter = new JaegerExporter({\n  endpoint: ''''http://jaeger:14268/api/traces''''\n});\n\nprovider.addSpanProcessor(new BatchSpanProcessor(exporter));\nprovider.register();\n\n// Instrument libraries\nregisterInstrumentations({\n  instrumentations: [\n    new HttpInstrumentation(),\n    new ExpressInstrumentation(),\n  ],\n});\n\nconst express = require(''''express'''');\nconst app = express();\n\napp.get(''''/api/users'''', async (req, res) => {\n  const tracer = trace.getTracer(''''my-service'''');\n  const span = tracer.startSpan(''''get_users'''');\n\n  try {\n    const users = await fetchUsers();\n    span.setAttributes({ ''''user.count'''': users.length });\n    res.json({ users });\n  } finally {\n    span.end();\n  }\n});\n```\n\n#### Go\n```go\npackage main\n\nimport (\n    \"context\"\n    \"go.opentelemetry.io/otel\"\n    \"go.opentelemetry.io/otel/exporters/jaeger\"\n    \"go.opentelemetry.io/otel/sdk/resource\"\n    sdktrace \"go.opentelemetry.io/otel/sdk/trace\"\n    semconv \"go.opentelemetry.io/otel/semconv/v1.4.0\"\n)\n\nfunc initTracer() (*sdktrace.TracerProvider, error) {\n    exporter, err := jaeger.New(jaeger.WithCollectorEndpoint(\n        jaeger.WithEndpoint(\"http://jaeger:14268/api/traces\"),\n    ))\n    if err != nil {\n        return nil, err\n    }\n\n    tp := sdktrace.NewTracerProvider(\n        sdktrace.WithBatcher(exporter),\n        sdktrace.WithResource(resource.NewWithAttributes(\n            semconv.SchemaURL,\n            semconv.ServiceNameKey.String(\"my-service\"),\n        )),\n    )\n\n    otel.SetTracerProvider(tp)\n    return tp, nil\n}\n\nfunc getUsers(ctx context.Context) ([]User, error) {\n    tracer := otel.Tracer(\"my-service\")\n    ctx, span := tracer.Start(ctx, \"get_users\")\n    defer span.End()\n\n    span.SetAttributes(attribute.String(\"user.filter\", \"active\"))\n\n    users, err := fetchUsersFromDB(ctx)\n    if err != nil {\n        span.RecordError(err)\n        return nil, err\n    }\n\n    span.SetAttributes(attribute.Int(\"user.count\", len(users)))\n    return users, nil\n}\n```\n\n**Reference:** See `references/instrumentation.md`\n\n## Context Propagation\n\n### HTTP Headers\n```\ntraceparent: 00-0af7651916cd43dd8448eb211c80319c-b7ad6b7169203331-01\ntracestate: congo=t61rcWkgMzE\n```\n\n### Propagation in HTTP Requests\n\n#### Python\n```python\nfrom opentelemetry.propagate import inject\n\nheaders = {}\ninject(headers)  # Injects trace context\n\nresponse = requests.get(''''http://downstream-service/api'''', headers=headers)\n```\n\n#### Node.js\n```javascript\nconst { propagation } = require(''''@opentelemetry/api'''');\n\nconst headers = {};\npropagation.inject(context.active(), headers);\n\naxios.get(''''http://downstream-service/api'''', { headers });\n```\n\n## Tempo Setup (Grafana)\n\n### Kubernetes Deployment\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: tempo-config\ndata:\n  tempo.yaml: |\n    server:\n      http_listen_port: 3200\n\n    distributor:\n      receivers:\n        jaeger:\n          protocols:\n            thrift_http:\n            grpc:\n        otlp:\n          protocols:\n            http:\n            grpc:\n\n    storage:\n      trace:\n        backend: s3\n        s3:\n          bucket: tempo-traces\n          endpoint: s3.amazonaws.com\n\n    querier:\n      frontend_worker:\n        frontend_address: tempo-query-frontend:9095\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tempo\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: tempo\n        image: grafana/tempo:latest\n        args:\n          - -config.file=/etc/tempo/tempo.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/tempo\n      volumes:\n      - name: config\n        configMap:\n          name: tempo-config\n```\n\n**Reference:** See `assets/jaeger-config.yaml.template`\n\n## Sampling Strategies\n\n### Probabilistic Sampling\n```yaml\n# Sample 1% of traces\nsampler:\n  type: probabilistic\n  param: 0.01\n```\n\n### Rate Limiting Sampling\n```yaml\n# Sample max 100 traces per second\nsampler:\n  type: ratelimiting\n  param: 100\n```\n\n### Adaptive Sampling\n```python\nfrom opentelemetry.sdk.trace.sampling import ParentBased, TraceIdRatioBased\n\n# Sample based on trace ID (deterministic)\nsampler = ParentBased(root=TraceIdRatioBased(0.01))\n```\n\n## Trace Analysis\n\n### Finding Slow Requests\n\n**Jaeger Query:**\n```\nservice=my-service\nduration > 1s\n```\n\n### Finding Errors\n\n**Jaeger Query:**\n```\nservice=my-service\nerror=true\ntags.http.status_code >= 500\n```\n\n### Service Dependency Graph\n\nJaeger automatically generates service dependency graphs showing:\n- Service relationships\n- Request rates\n- Error rates\n- Average latencies\n\n## Best Practices\n\n1. **Sample appropriately** (1-10% in production)\n2. **Add meaningful tags** (user_id, request_id)\n3. **Propagate context** across all service boundaries\n4. **Log exceptions** in spans\n5. **Use consistent naming** for operations\n6. **Monitor tracing overhead** (<1% CPU impact)\n7. **Set up alerts** for trace errors\n8. **Implement distributed context** (baggage)\n9. **Use span events** for important milestones\n10. **Document instrumentation** standards\n\n## Integration with Logging\n\n### Correlated Logs\n```python\nimport logging\nfrom opentelemetry import trace\n\nlogger = logging.getLogger(__name__)\n\ndef process_request():\n    span = trace.get_current_span()\n    trace_id = span.get_span_context().trace_id\n\n    logger.info(\n        \"Processing request\",\n        extra={\"trace_id\": format(trace_id, ''''032x'''')}\n    )\n```\n\n## Troubleshooting\n\n**No traces appearing:**\n- Check collector endpoint\n- Verify network connectivity\n- Check sampling configuration\n- Review application logs\n\n**High latency overhead:**\n- Reduce sampling rate\n- Use batch span processor\n- Check exporter configuration\n\n## Reference Files\n\n- `references/jaeger-setup.md` - Jaeger installation\n- `references/instrumentation.md` - Instrumentation patterns\n- `assets/jaeger-config.yaml.template` - Jaeger configuration\n\n## Related Skills\n\n- `prometheus-configuration` - For metrics\n- `grafana-dashboards` - For visualization\n- `slo-implementation` - For latency SLOs\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 10, problem_definition: 5',
  'Score: 82, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, error_handling: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-observability-monitoring-skills-distributed-tracing-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'paypal-integration - Integrate PayPal payment processing with support for express checkout, subscriptions, and refund management. Use when implementing PayPal payments, pr',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# PayPal Integration\n\nMaster PayPal payment integration including Express Checkout, IPN handling, recurring billing, and refund workflows.\n\n## When to Use This Skill\n\n- Integrating PayPal as a payment option\n- Implementing express checkout flows\n- Setting up recurring billing with PayPal\n- Processing refunds and payment disputes\n- Handling PayPal webhooks (IPN)\n- Supporting international payments\n- Implementing PayPal subscriptions\n\n## Core Concepts\n\n### 1. Payment Products\n**PayPal Checkout**\n- One-time payments\n- Express checkout experience\n- Guest and PayPal account payments\n\n**PayPal Subscriptions**\n- Recurring billing\n- Subscription plans\n- Automatic renewals\n\n**PayPal Payouts**\n- Send money to multiple recipients\n- Marketplace and platform payments\n\n### 2. Integration Methods\n**Client-Side (JavaScript SDK)**\n- Smart Payment Buttons\n- Hosted payment flow\n- Minimal backend code\n\n**Server-Side (REST API)**\n- Full control over payment flow\n- Custom checkout UI\n- Advanced features\n\n### 3. IPN (Instant Payment Notification)\n- Webhook-like payment notifications\n- Asynchronous payment updates\n- Verification required\n\n## Quick Start\n\n```javascript\n// Frontend - PayPal Smart Buttons\n<div id=\"paypal-button-container\"></div>\n\n<script src=\"https://www.paypal.com/sdk/js?client-id=YOUR_CLIENT_ID&currency=USD\"></script>\n<script>\n  paypal.Buttons({\n    createOrder: function(data, actions) {\n      return actions.order.create({\n        purchase_units: [{\n          amount: {\n            value: ''''25.00''''\n          }\n        }]\n      });\n    },\n    onApprove: function(data, actions) {\n      return actions.order.capture().then(function(details) {\n        // Payment successful\n        console.log(''''Transaction completed by '''' + details.payer.name.given_name);\n\n        // Send to backend for verification\n        fetch(''''/api/paypal/capture'''', {\n          method: ''''POST'''',\n          headers: {''''Content-Type'''': ''''application/json''''},\n          body: JSON.stringify({orderID: data.orderID})\n        });\n      });\n    }\n  }).render(''''#paypal-button-container'''');\n</script>\n```\n\n```python\n# Backend - Verify and capture order\nfrom paypalrestsdk import Payment\nimport paypalrestsdk\n\npaypalrestsdk.configure({\n    \"mode\": \"sandbox\",  # or \"live\"\n    \"client_id\": \"YOUR_CLIENT_ID\",\n    \"client_secret\": \"YOUR_CLIENT_SECRET\"\n})\n\ndef capture_paypal_order(order_id):\n    \"\"\"Capture a PayPal order.\"\"\"\n    payment = Payment.find(order_id)\n\n    if payment.execute({\"payer_id\": payment.payer.payer_info.payer_id}):\n        # Payment successful\n        return {\n            ''''status'''': ''''success'''',\n            ''''transaction_id'''': payment.id,\n            ''''amount'''': payment.transactions[0].amount.total\n        }\n    else:\n        # Payment failed\n        return {\n            ''''status'''': ''''failed'''',\n            ''''error'''': payment.error\n        }\n```\n\n## Express Checkout Implementation\n\n### Server-Side Order Creation\n```python\nimport requests\nimport json\n\nclass PayPalClient:\n    def __init__(self, client_id, client_secret, mode=''''sandbox''''):\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.base_url = ''''https://api-m.sandbox.paypal.com'''' if mode == ''''sandbox'''' else ''''https://api-m.paypal.com''''\n        self.access_token = self.get_access_token()\n\n    def get_access_token(self):\n        \"\"\"Get OAuth access token.\"\"\"\n        url = f\"{self.base_url}/v1/oauth2/token\"\n        headers = {\"Accept\": \"application/json\", \"Accept-Language\": \"en_US\"}\n\n        response = requests.post(\n            url,\n            headers=headers,\n            data={\"grant_type\": \"client_credentials\"},\n            auth=(self.client_id, self.client_secret)\n        )\n\n        return response.json()[''''access_token'''']\n\n    def create_order(self, amount, currency=''''USD''''):\n        \"\"\"Create a PayPal order.\"\"\"\n        url = f\"{self.base_url}/v2/checkout/orders\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.access_token}\"\n        }\n\n        payload = {\n            \"intent\": \"CAPTURE\",\n            \"purchase_units\": [{\n                \"amount\": {\n                    \"currency_code\": currency,\n                    \"value\": str(amount)\n                }\n            }]\n        }\n\n        response = requests.post(url, headers=headers, json=payload)\n        return response.json()\n\n    def capture_order(self, order_id):\n        \"\"\"Capture payment for an order.\"\"\"\n        url = f\"{self.base_url}/v2/checkout/orders/{order_id}/capture\"\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.access_token}\"\n        }\n\n        response = requests.post(url, headers=headers)\n        return response.json()\n\n    def get_order_details(self, order_id):\n        \"\"\"Get order details.\"\"\"\n        url = f\"{self.base_url}/v2/checkout/orders/{order_id}\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\"\n        }\n\n        response = requests.get(url, headers=headers)\n        return response.json()\n```\n\n## IPN (Instant Payment Notification) Handling\n\n### IPN Verification and Processing\n```python\nfrom flask import Flask, request\nimport requests\nfrom urllib.parse import parse_qs\n\napp = Flask(__name__)\n\n@app.route(''''/ipn'''', methods=[''''POST''''])\ndef handle_ipn():\n    \"\"\"Handle PayPal IPN notifications.\"\"\"\n    # Get IPN message\n    ipn_data = request.form.to_dict()\n\n    # Verify IPN with PayPal\n    if not verify_ipn(ipn_data):\n        return ''''IPN verification failed'''', 400\n\n    # Process IPN based on transaction type\n    payment_status = ipn_data.get(''''payment_status'''')\n    txn_type = ipn_data.get(''''txn_type'''')\n\n    if payment_status == ''''Completed'''':\n        handle_payment_completed(ipn_data)\n    elif payment_status == ''''Refunded'''':\n        handle_refund(ipn_data)\n    elif payment_status == ''''Reversed'''':\n        handle_chargeback(ipn_data)\n\n    return ''''IPN processed'''', 200\n\ndef verify_ipn(ipn_data):\n    \"\"\"Verify IPN message authenticity.\"\"\"\n    # Add ''''cmd'''' parameter\n    verify_data = ipn_data.copy()\n    verify_data[''''cmd''''] = ''''_notify-validate''''\n\n    # Send back to PayPal for verification\n    paypal_url = ''''https://ipnpb.sandbox.paypal.com/cgi-bin/webscr''''  # or production URL\n\n    response = requests.post(paypal_url, data=verify_data)\n\n    return response.text == ''''VERIFIED''''\n\ndef handle_payment_completed(ipn_data):\n    \"\"\"Process completed payment.\"\"\"\n    txn_id = ipn_data.get(''''txn_id'''')\n    payer_email = ipn_data.get(''''payer_email'''')\n    mc_gross = ipn_data.get(''''mc_gross'''')\n    item_name = ipn_data.get(''''item_name'''')\n\n    # Check if already processed (prevent duplicates)\n    if is_transaction_processed(txn_id):\n        return\n\n    # Update database\n    # Send confirmation email\n    # Fulfill order\n    print(f\"Payment completed: {txn_id}, Amount: ${mc_gross}\")\n\ndef handle_refund(ipn_data):\n    \"\"\"Handle refund.\"\"\"\n    parent_txn_id = ipn_data.get(''''parent_txn_id'''')\n    mc_gross = ipn_data.get(''''mc_gross'''')\n\n    # Process refund in your system\n    print(f\"Refund processed: {parent_txn_id}, Amount: ${mc_gross}\")\n\ndef handle_chargeback(ipn_data):\n    \"\"\"Handle payment reversal/chargeback.\"\"\"\n    txn_id = ipn_data.get(''''txn_id'''')\n    reason_code = ipn_data.get(''''reason_code'''')\n\n    # Handle chargeback\n    print(f\"Chargeback: {txn_id}, Reason: {reason_code}\")\n```\n\n## Subscription/Recurring Billing\n\n### Create Subscription Plan\n```python\ndef create_subscription_plan(name, amount, interval=''''MONTH''''):\n    \"\"\"Create a subscription plan.\"\"\"\n    client = PayPalClient(CLIENT_ID, CLIENT_SECRET)\n\n    url = f\"{client.base_url}/v1/billing/plans\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {client.access_token}\"\n    }\n\n    payload = {\n        \"product_id\": \"PRODUCT_ID\",  # Create product first\n        \"name\": name,\n        \"billing_cycles\": [{\n            \"frequency\": {\n                \"interval_unit\": interval,\n                \"interval_count\": 1\n            },\n            \"tenure_type\": \"REGULAR\",\n            \"sequence\": 1,\n            \"total_cycles\": 0,  # Infinite\n            \"pricing_scheme\": {\n                \"fixed_price\": {\n                    \"value\": str(amount),\n                    \"currency_code\": \"USD\"\n                }\n            }\n        }],\n        \"payment_preferences\": {\n            \"auto_bill_outstanding\": True,\n            \"setup_fee\": {\n                \"value\": \"0\",\n                \"currency_code\": \"USD\"\n            },\n            \"setup_fee_failure_action\": \"CONTINUE\",\n            \"payment_failure_threshold\": 3\n        }\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n    return response.json()\n\ndef create_subscription(plan_id, subscriber_email):\n    \"\"\"Create a subscription for a customer.\"\"\"\n    client = PayPalClient(CLIENT_ID, CLIENT_SECRET)\n\n    url = f\"{client.base_url}/v1/billing/subscriptions\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {client.access_token}\"\n    }\n\n    payload = {\n        \"plan_id\": plan_id,\n        \"subscriber\": {\n            \"email_address\": subscriber_email\n        },\n        \"application_context\": {\n            \"return_url\": \"https://yourdomain.com/subscription/success\",\n            \"cancel_url\": \"https://yourdomain.com/subscription/cancel\"\n        }\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n    subscription = response.json()\n\n    # Get approval URL\n    for link in subscription.get(''''links'''', []):\n        if link[''''rel''''] == ''''approve'''':\n            return {\n                ''''subscription_id'''': subscription[''''id''''],\n                ''''approval_url'''': link[''''href'''']\n            }\n```\n\n## Refund Workflows\n\n```python\ndef create_refund(capture_id, amount=None, note=None):\n    \"\"\"Create a refund for a captured payment.\"\"\"\n    client = PayPalClient(CLIENT_ID, CLIENT_SECRET)\n\n    url = f\"{client.base_url}/v2/payments/captures/{capture_id}/refund\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {client.access_token}\"\n    }\n\n    payload = {}\n    if amount:\n        payload[\"amount\"] = {\n            \"value\": str(amount),\n            \"currency_code\": \"USD\"\n        }\n\n    if note:\n        payload[\"note_to_payer\"] = note\n\n    response = requests.post(url, headers=headers, json=payload)\n    return response.json()\n\ndef get_refund_details(refund_id):\n    \"\"\"Get refund details.\"\"\"\n    client = PayPalClient(CLIENT_ID, CLIENT_SECRET)\n\n    url = f\"{client.base_url}/v2/payments/refunds/{refund_id}\"\n    headers = {\n        \"Authorization\": f\"Bearer {client.access_token}\"\n    }\n\n    response = requests.get(url, headers=headers)\n    return response.json()\n```\n\n## Error Handling\n\n```python\nclass PayPalError(Exception):\n    \"\"\"Custom PayPal error.\"\"\"\n    pass\n\ndef handle_paypal_api_call(api_function):\n    \"\"\"Wrapper for PayPal API calls with error handling.\"\"\"\n    try:\n        result = api_function()\n        return result\n    except requests.exceptions.RequestException as e:\n        # Network error\n        raise PayPalError(f\"Network error: {str(e)}\")\n    except Exception as e:\n        # Other errors\n        raise PayPalError(f\"PayPal API error: {str(e)}\")\n\n# Usage\ntry:\n    order = handle_paypal_api_call(lambda: client.create_order(25.00))\nexcept PayPalError as e:\n    # Handle error appropriately\n    log_error(e)\n```\n\n## Testing\n\n```python\n# Use sandbox credentials\nSANDBOX_CLIENT_ID = \"...\"\nSANDBOX_SECRET = \"...\"\n\n# Test accounts\n# Create test buyer and seller accounts at developer.paypal.com\n\ndef test_payment_flow():\n    \"\"\"Test complete payment flow.\"\"\"\n    client = PayPalClient(SANDBOX_CLIENT_ID, SANDBOX_SECRET, mode=''''sandbox'''')\n\n    # Create order\n    order = client.create_order(10.00)\n    assert ''''id'''' in order\n\n    # Get approval URL\n    approval_url = next((link[''''href''''] for link in order[''''links''''] if link[''''rel''''] == ''''approve''''), None)\n    assert approval_url is not None\n\n    # After approval (manual step with test account)\n    # Capture order\n    # captured = client.capture_order(order[''''id''''])\n    # assert captured[''''status''''] == ''''COMPLETED''''\n```\n\n## Resources\n\n- **references/express-checkout.md**: Express Checkout implementation guide\n- **references/ipn-handling.md**: IPN verification and processing\n- **references/refund-workflows.md**: Refund handling patterns\n- **references/billing-agreements.md**: Recurring billing setup\n- **assets/paypal-client.py**: Production PayPal client\n- **assets/ipn-processor.py**: IPN webhook processor\n- **assets/recurring-billing.py**: Subscription management\n\n## Best Practices\n\n1. **Always Verify IPN**: Never trust IPN without verification\n2. **Idempotent Processing**: Handle duplicate IPN notifications\n3. **Error Handling**: Implement robust error handling\n4. **Logging**: Log all transactions and errors\n5. **Test Thoroughly**: Use sandbox extensively\n6. **Webhook Backup**: Don''''t rely solely on client-side callbacks\n7. **Currency Handling**: Always specify currency explicitly\n\n## Common Pitfalls\n\n- **Not Verifying IPN**: Accepting IPN without verification\n- **Duplicate Processing**: Not checking for duplicate transactions\n- **Wrong Environment**: Mixing sandbox and production URLs/credentials\n- **Missing Webhooks**: Not handling all payment states\n- **Hardcoded Values**: Not making configurable for different environments\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 12, completeness: 10',
  'Score: 87, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-payment-processing-skills-paypal-integration-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'sql-optimization-patterns - Master SQL query optimization, indexing strategies, and EXPLAIN analysis to dramatically improve database performance and eliminate slow queries. Use ',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# SQL Optimization Patterns\n\nTransform slow database queries into lightning-fast operations through systematic optimization, proper indexing, and query plan analysis.\n\n## When to Use This Skill\n\n- Debugging slow-running queries\n- Designing performant database schemas\n- Optimizing application response times\n- Reducing database load and costs\n- Improving scalability for growing datasets\n- Analyzing EXPLAIN query plans\n- Implementing efficient indexes\n- Resolving N+1 query problems\n\n## Core Concepts\n\n### 1. Query Execution Plans (EXPLAIN)\n\nUnderstanding EXPLAIN output is fundamental to optimization.\n\n**PostgreSQL EXPLAIN:**\n```sql\n-- Basic explain\nEXPLAIN SELECT * FROM users WHERE email = ''''user@example.com'''';\n\n-- With actual execution stats\nEXPLAIN ANALYZE\nSELECT * FROM users WHERE email = ''''user@example.com'''';\n\n-- Verbose output with more details\nEXPLAIN (ANALYZE, BUFFERS, VERBOSE)\nSELECT u.*, o.order_total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at > NOW() - INTERVAL ''''30 days'''';\n```\n\n**Key Metrics to Watch:**\n- **Seq Scan**: Full table scan (usually slow for large tables)\n- **Index Scan**: Using index (good)\n- **Index Only Scan**: Using index without touching table (best)\n- **Nested Loop**: Join method (okay for small datasets)\n- **Hash Join**: Join method (good for larger datasets)\n- **Merge Join**: Join method (good for sorted data)\n- **Cost**: Estimated query cost (lower is better)\n- **Rows**: Estimated rows returned\n- **Actual Time**: Real execution time\n\n### 2. Index Strategies\n\nIndexes are the most powerful optimization tool.\n\n**Index Types:**\n- **B-Tree**: Default, good for equality and range queries\n- **Hash**: Only for equality (=) comparisons\n- **GIN**: Full-text search, array queries, JSONB\n- **GiST**: Geometric data, full-text search\n- **BRIN**: Block Range INdex for very large tables with correlation\n\n```sql\n-- Standard B-Tree index\nCREATE INDEX idx_users_email ON users(email);\n\n-- Composite index (order matters!)\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n\n-- Partial index (index subset of rows)\nCREATE INDEX idx_active_users ON users(email)\nWHERE status = ''''active'''';\n\n-- Expression index\nCREATE INDEX idx_users_lower_email ON users(LOWER(email));\n\n-- Covering index (include additional columns)\nCREATE INDEX idx_users_email_covering ON users(email)\nINCLUDE (name, created_at);\n\n-- Full-text search index\nCREATE INDEX idx_posts_search ON posts\nUSING GIN(to_tsvector(''''english'''', title || '''' '''' || body));\n\n-- JSONB index\nCREATE INDEX idx_metadata ON events USING GIN(metadata);\n```\n\n### 3. Query Optimization Patterns\n\n**Avoid SELECT \\*:**\n```sql\n-- Bad: Fetches unnecessary columns\nSELECT * FROM users WHERE id = 123;\n\n-- Good: Fetch only what you need\nSELECT id, email, name FROM users WHERE id = 123;\n```\n\n**Use WHERE Clause Efficiently:**\n```sql\n-- Bad: Function prevents index usage\nSELECT * FROM users WHERE LOWER(email) = ''''user@example.com'''';\n\n-- Good: Create functional index or use exact match\nCREATE INDEX idx_users_email_lower ON users(LOWER(email));\n-- Then:\nSELECT * FROM users WHERE LOWER(email) = ''''user@example.com'''';\n\n-- Or store normalized data\nSELECT * FROM users WHERE email = ''''user@example.com'''';\n```\n\n**Optimize JOINs:**\n```sql\n-- Bad: Cartesian product then filter\nSELECT u.name, o.total\nFROM users u, orders o\nWHERE u.id = o.user_id AND u.created_at > ''''2024-01-01'''';\n\n-- Good: Filter before join\nSELECT u.name, o.total\nFROM users u\nJOIN orders o ON u.id = o.user_id\nWHERE u.created_at > ''''2024-01-01'''';\n\n-- Better: Filter both tables\nSELECT u.name, o.total\nFROM (SELECT * FROM users WHERE created_at > ''''2024-01-01'''') u\nJOIN orders o ON u.id = o.user_id;\n```\n\n## Optimization Patterns\n\n### Pattern 1: Eliminate N+1 Queries\n\n**Problem: N+1 Query Anti-Pattern**\n```python\n# Bad: Executes N+1 queries\nusers = db.query(\"SELECT * FROM users LIMIT 10\")\nfor user in users:\n    orders = db.query(\"SELECT * FROM orders WHERE user_id = ?\", user.id)\n    # Process orders\n```\n\n**Solution: Use JOINs or Batch Loading**\n```sql\n-- Solution 1: JOIN\nSELECT\n    u.id, u.name,\n    o.id as order_id, o.total\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nWHERE u.id IN (1, 2, 3, 4, 5);\n\n-- Solution 2: Batch query\nSELECT * FROM orders\nWHERE user_id IN (1, 2, 3, 4, 5);\n```\n\n```python\n# Good: Single query with JOIN or batch load\n# Using JOIN\nresults = db.query(\"\"\"\n    SELECT u.id, u.name, o.id as order_id, o.total\n    FROM users u\n    LEFT JOIN orders o ON u.id = o.user_id\n    WHERE u.id IN (1, 2, 3, 4, 5)\n\"\"\")\n\n# Or batch load\nusers = db.query(\"SELECT * FROM users LIMIT 10\")\nuser_ids = [u.id for u in users]\norders = db.query(\n    \"SELECT * FROM orders WHERE user_id IN (?)\",\n    user_ids\n)\n# Group orders by user_id\norders_by_user = {}\nfor order in orders:\n    orders_by_user.setdefault(order.user_id, []).append(order)\n```\n\n### Pattern 2: Optimize Pagination\n\n**Bad: OFFSET on Large Tables**\n```sql\n-- Slow for large offsets\nSELECT * FROM users\nORDER BY created_at DESC\nLIMIT 20 OFFSET 100000;  -- Very slow!\n```\n\n**Good: Cursor-Based Pagination**\n```sql\n-- Much faster: Use cursor (last seen ID)\nSELECT * FROM users\nWHERE created_at < ''''2024-01-15 10:30:00''''  -- Last cursor\nORDER BY created_at DESC\nLIMIT 20;\n\n-- With composite sorting\nSELECT * FROM users\nWHERE (created_at, id) < (''''2024-01-15 10:30:00'''', 12345)\nORDER BY created_at DESC, id DESC\nLIMIT 20;\n\n-- Requires index\nCREATE INDEX idx_users_cursor ON users(created_at DESC, id DESC);\n```\n\n### Pattern 3: Aggregate Efficiently\n\n**Optimize COUNT Queries:**\n```sql\n-- Bad: Counts all rows\nSELECT COUNT(*) FROM orders;  -- Slow on large tables\n\n-- Good: Use estimates for approximate counts\nSELECT reltuples::bigint AS estimate\nFROM pg_class\nWHERE relname = ''''orders'''';\n\n-- Good: Filter before counting\nSELECT COUNT(*) FROM orders\nWHERE created_at > NOW() - INTERVAL ''''7 days'''';\n\n-- Better: Use index-only scan\nCREATE INDEX idx_orders_created ON orders(created_at);\nSELECT COUNT(*) FROM orders\nWHERE created_at > NOW() - INTERVAL ''''7 days'''';\n```\n\n**Optimize GROUP BY:**\n```sql\n-- Bad: Group by then filter\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nGROUP BY user_id\nHAVING COUNT(*) > 10;\n\n-- Better: Filter first, then group (if possible)\nSELECT user_id, COUNT(*) as order_count\nFROM orders\nWHERE status = ''''completed''''\nGROUP BY user_id\nHAVING COUNT(*) > 10;\n\n-- Best: Use covering index\nCREATE INDEX idx_orders_user_status ON orders(user_id, status);\n```\n\n### Pattern 4: Subquery Optimization\n\n**Transform Correlated Subqueries:**\n```sql\n-- Bad: Correlated subquery (runs for each row)\nSELECT u.name, u.email,\n    (SELECT COUNT(*) FROM orders o WHERE o.user_id = u.id) as order_count\nFROM users u;\n\n-- Good: JOIN with aggregation\nSELECT u.name, u.email, COUNT(o.id) as order_count\nFROM users u\nLEFT JOIN orders o ON o.user_id = u.id\nGROUP BY u.id, u.name, u.email;\n\n-- Better: Use window functions\nSELECT DISTINCT ON (u.id)\n    u.name, u.email,\n    COUNT(o.id) OVER (PARTITION BY u.id) as order_count\nFROM users u\nLEFT JOIN orders o ON o.user_id = u.id;\n```\n\n**Use CTEs for Clarity:**\n```sql\n-- Using Common Table Expressions\nWITH recent_users AS (\n    SELECT id, name, email\n    FROM users\n    WHERE created_at > NOW() - INTERVAL ''''30 days''''\n),\nuser_order_counts AS (\n    SELECT user_id, COUNT(*) as order_count\n    FROM orders\n    WHERE created_at > NOW() - INTERVAL ''''30 days''''\n    GROUP BY user_id\n)\nSELECT ru.name, ru.email, COALESCE(uoc.order_count, 0) as orders\nFROM recent_users ru\nLEFT JOIN user_order_counts uoc ON ru.id = uoc.user_id;\n```\n\n### Pattern 5: Batch Operations\n\n**Batch INSERT:**\n```sql\n-- Bad: Multiple individual inserts\nINSERT INTO users (name, email) VALUES (''''Alice'''', ''''alice@example.com'''');\nINSERT INTO users (name, email) VALUES (''''Bob'''', ''''bob@example.com'''');\nINSERT INTO users (name, email) VALUES (''''Carol'''', ''''carol@example.com'''');\n\n-- Good: Batch insert\nINSERT INTO users (name, email) VALUES\n    (''''Alice'''', ''''alice@example.com''''),\n    (''''Bob'''', ''''bob@example.com''''),\n    (''''Carol'''', ''''carol@example.com'''');\n\n-- Better: Use COPY for bulk inserts (PostgreSQL)\nCOPY users (name, email) FROM ''''/tmp/users.csv'''' CSV HEADER;\n```\n\n**Batch UPDATE:**\n```sql\n-- Bad: Update in loop\nUPDATE users SET status = ''''active'''' WHERE id = 1;\nUPDATE users SET status = ''''active'''' WHERE id = 2;\n-- ... repeat for many IDs\n\n-- Good: Single UPDATE with IN clause\nUPDATE users\nSET status = ''''active''''\nWHERE id IN (1, 2, 3, 4, 5, ...);\n\n-- Better: Use temporary table for large batches\nCREATE TEMP TABLE temp_user_updates (id INT, new_status VARCHAR);\nINSERT INTO temp_user_updates VALUES (1, ''''active''''), (2, ''''active''''), ...;\n\nUPDATE users u\nSET status = t.new_status\nFROM temp_user_updates t\nWHERE u.id = t.id;\n```\n\n## Advanced Techniques\n\n### Materialized Views\n\nPre-compute expensive queries.\n\n```sql\n-- Create materialized view\nCREATE MATERIALIZED VIEW user_order_summary AS\nSELECT\n    u.id,\n    u.name,\n    COUNT(o.id) as total_orders,\n    SUM(o.total) as total_spent,\n    MAX(o.created_at) as last_order_date\nFROM users u\nLEFT JOIN orders o ON u.id = o.user_id\nGROUP BY u.id, u.name;\n\n-- Add index to materialized view\nCREATE INDEX idx_user_summary_spent ON user_order_summary(total_spent DESC);\n\n-- Refresh materialized view\nREFRESH MATERIALIZED VIEW user_order_summary;\n\n-- Concurrent refresh (PostgreSQL)\nREFRESH MATERIALIZED VIEW CONCURRENTLY user_order_summary;\n\n-- Query materialized view (very fast)\nSELECT * FROM user_order_summary\nWHERE total_spent > 1000\nORDER BY total_spent DESC;\n```\n\n### Partitioning\n\nSplit large tables for better performance.\n\n```sql\n-- Range partitioning by date (PostgreSQL)\nCREATE TABLE orders (\n    id SERIAL,\n    user_id INT,\n    total DECIMAL,\n    created_at TIMESTAMP\n) PARTITION BY RANGE (created_at);\n\n-- Create partitions\nCREATE TABLE orders_2024_q1 PARTITION OF orders\n    FOR VALUES FROM (''''2024-01-01'''') TO (''''2024-04-01'''');\n\nCREATE TABLE orders_2024_q2 PARTITION OF orders\n    FOR VALUES FROM (''''2024-04-01'''') TO (''''2024-07-01'''');\n\n-- Queries automatically use appropriate partition\nSELECT * FROM orders\nWHERE created_at BETWEEN ''''2024-02-01'''' AND ''''2024-02-28'''';\n-- Only scans orders_2024_q1 partition\n```\n\n### Query Hints and Optimization\n\n```sql\n-- Force index usage (MySQL)\nSELECT * FROM users\nUSE INDEX (idx_users_email)\nWHERE email = ''''user@example.com'''';\n\n-- Parallel query (PostgreSQL)\nSET max_parallel_workers_per_gather = 4;\nSELECT * FROM large_table WHERE condition;\n\n-- Join hints (PostgreSQL)\nSET enable_nestloop = OFF;  -- Force hash or merge join\n```\n\n## Best Practices\n\n1. **Index Selectively**: Too many indexes slow down writes\n2. **Monitor Query Performance**: Use slow query logs\n3. **Keep Statistics Updated**: Run ANALYZE regularly\n4. **Use Appropriate Data Types**: Smaller types = better performance\n5. **Normalize Thoughtfully**: Balance normalization vs performance\n6. **Cache Frequently Accessed Data**: Use application-level caching\n7. **Connection Pooling**: Reuse database connections\n8. **Regular Maintenance**: VACUUM, ANALYZE, rebuild indexes\n\n```sql\n-- Update statistics\nANALYZE users;\nANALYZE VERBOSE orders;\n\n-- Vacuum (PostgreSQL)\nVACUUM ANALYZE users;\nVACUUM FULL users;  -- Reclaim space (locks table)\n\n-- Reindex\nREINDEX INDEX idx_users_email;\nREINDEX TABLE users;\n```\n\n## Common Pitfalls\n\n- **Over-Indexing**: Each index slows down INSERT/UPDATE/DELETE\n- **Unused Indexes**: Waste space and slow writes\n- **Missing Indexes**: Slow queries, full table scans\n- **Implicit Type Conversion**: Prevents index usage\n- **OR Conditions**: Can''''t use indexes efficiently\n- **LIKE with Leading Wildcard**: `LIKE ''''%abc''''` can''''t use index\n- **Function in WHERE**: Prevents index usage unless functional index exists\n\n## Monitoring Queries\n\n```sql\n-- Find slow queries (PostgreSQL)\nSELECT query, calls, total_time, mean_time\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n\n-- Find missing indexes (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    seq_scan,\n    seq_tup_read,\n    idx_scan,\n    seq_tup_read / seq_scan AS avg_seq_tup_read\nFROM pg_stat_user_tables\nWHERE seq_scan > 0\nORDER BY seq_tup_read DESC\nLIMIT 10;\n\n-- Find unused indexes (PostgreSQL)\nSELECT\n    schemaname,\n    tablename,\n    indexname,\n    idx_scan,\n    idx_tup_read,\n    idx_tup_fetch\nFROM pg_stat_user_indexes\nWHERE idx_scan = 0\nORDER BY pg_relation_size(indexrelid) DESC;\n```\n\n## Resources\n\n- **references/postgres-optimization-guide.md**: PostgreSQL-specific optimization\n- **references/mysql-optimization-guide.md**: MySQL/MariaDB optimization\n- **references/query-plan-analysis.md**: Deep dive into EXPLAIN plans\n- **assets/index-strategy-checklist.md**: When and how to create indexes\n- **assets/query-optimization-checklist.md**: Step-by-step optimization guide\n- **scripts/analyze-slow-queries.sql**: Identify slow queries in your database\n- **scripts/index-recommendations.sql**: Generate index recommendations\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 5',
  'Score: 87, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-developer-essentials-skills-sql-optimization-patterns-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'hybrid-cloud-networking - Configure secure, high-performance connectivity between on-premises infrastructure and cloud platforms using VPN and dedicated connections. Use when b',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Hybrid Cloud Networking\n\nConfigure secure, high-performance connectivity between on-premises and cloud environments using VPN, Direct Connect, and ExpressRoute.\n\n## Purpose\n\nEstablish secure, reliable network connectivity between on-premises data centers and cloud providers (AWS, Azure, GCP).\n\n## When to Use\n\n- Connect on-premises to cloud\n- Extend datacenter to cloud\n- Implement hybrid active-active setups\n- Meet compliance requirements\n- Migrate to cloud gradually\n\n## Connection Options\n\n### AWS Connectivity\n\n#### 1. Site-to-Site VPN\n- IPSec VPN over internet\n- Up to 1.25 Gbps per tunnel\n- Cost-effective for moderate bandwidth\n- Higher latency, internet-dependent\n\n```hcl\nresource \"aws_vpn_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  tags = {\n    Name = \"main-vpn-gateway\"\n  }\n}\n\nresource \"aws_customer_gateway\" \"main\" {\n  bgp_asn    = 65000\n  ip_address = \"203.0.113.1\"\n  type       = \"ipsec.1\"\n}\n\nresource \"aws_vpn_connection\" \"main\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.main.id\n  type                = \"ipsec.1\"\n  static_routes_only  = false\n}\n```\n\n#### 2. AWS Direct Connect\n- Dedicated network connection\n- 1 Gbps to 100 Gbps\n- Lower latency, consistent bandwidth\n- More expensive, setup time required\n\n**Reference:** See `references/direct-connect.md`\n\n### Azure Connectivity\n\n#### 1. Site-to-Site VPN\n```hcl\nresource \"azurerm_virtual_network_gateway\" \"vpn\" {\n  name                = \"vpn-gateway\"\n  location            = azurerm_resource_group.main.location\n  resource_group_name = azurerm_resource_group.main.name\n\n  type     = \"Vpn\"\n  vpn_type = \"RouteBased\"\n  sku      = \"VpnGw1\"\n\n  ip_configuration {\n    name                          = \"vnetGatewayConfig\"\n    public_ip_address_id          = azurerm_public_ip.vpn.id\n    private_ip_address_allocation = \"Dynamic\"\n    subnet_id                     = azurerm_subnet.gateway.id\n  }\n}\n```\n\n#### 2. Azure ExpressRoute\n- Private connection via connectivity provider\n- Up to 100 Gbps\n- Low latency, high reliability\n- Premium for global connectivity\n\n### GCP Connectivity\n\n#### 1. Cloud VPN\n- IPSec VPN (Classic or HA VPN)\n- HA VPN: 99.99% SLA\n- Up to 3 Gbps per tunnel\n\n#### 2. Cloud Interconnect\n- Dedicated (10 Gbps, 100 Gbps)\n- Partner (50 Mbps to 50 Gbps)\n- Lower latency than VPN\n\n## Hybrid Network Patterns\n\n### Pattern 1: Hub-and-Spoke\n```\nOn-Premises Datacenter\n         \u2193\n    VPN/Direct Connect\n         \u2193\n    Transit Gateway (AWS) / vWAN (Azure)\n         \u2193\n    \u251c\u2500 Production VPC/VNet\n    \u251c\u2500 Staging VPC/VNet\n    \u2514\u2500 Development VPC/VNet\n```\n\n### Pattern 2: Multi-Region Hybrid\n```\nOn-Premises\n    \u251c\u2500 Direct Connect \u2192 us-east-1\n    \u2514\u2500 Direct Connect \u2192 us-west-2\n            \u2193\n        Cross-Region Peering\n```\n\n### Pattern 3: Multi-Cloud Hybrid\n```\nOn-Premises Datacenter\n    \u251c\u2500 Direct Connect \u2192 AWS\n    \u251c\u2500 ExpressRoute \u2192 Azure\n    \u2514\u2500 Interconnect \u2192 GCP\n```\n\n## Routing Configuration\n\n### BGP Configuration\n```\nOn-Premises Router:\n- AS Number: 65000\n- Advertise: 10.0.0.0/8\n\nCloud Router:\n- AS Number: 64512 (AWS), 65515 (Azure)\n- Advertise: Cloud VPC/VNet CIDRs\n```\n\n### Route Propagation\n- Enable route propagation on route tables\n- Use BGP for dynamic routing\n- Implement route filtering\n- Monitor route advertisements\n\n## Security Best Practices\n\n1. **Use private connectivity** (Direct Connect/ExpressRoute)\n2. **Implement encryption** for VPN tunnels\n3. **Use VPC endpoints** to avoid internet routing\n4. **Configure network ACLs** and security groups\n5. **Enable VPC Flow Logs** for monitoring\n6. **Implement DDoS protection**\n7. **Use PrivateLink/Private Endpoints**\n8. **Monitor connections** with CloudWatch/Monitor\n9. **Implement redundancy** (dual tunnels)\n10. **Regular security audits**\n\n## High Availability\n\n### Dual VPN Tunnels\n```hcl\nresource \"aws_vpn_connection\" \"primary\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.primary.id\n  type                = \"ipsec.1\"\n}\n\nresource \"aws_vpn_connection\" \"secondary\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.secondary.id\n  type                = \"ipsec.1\"\n}\n```\n\n### Active-Active Configuration\n- Multiple connections from different locations\n- BGP for automatic failover\n- Equal-cost multi-path (ECMP) routing\n- Monitor health of all connections\n\n## Monitoring and Troubleshooting\n\n### Key Metrics\n- Tunnel status (up/down)\n- Bytes in/out\n- Packet loss\n- Latency\n- BGP session status\n\n### Troubleshooting\n```bash\n# AWS VPN\naws ec2 describe-vpn-connections\naws ec2 get-vpn-connection-telemetry\n\n# Azure VPN\naz network vpn-connection show\naz network vpn-connection show-device-config-script\n```\n\n## Cost Optimization\n\n1. **Right-size connections** based on traffic\n2. **Use VPN for low-bandwidth** workloads\n3. **Consolidate traffic** through fewer connections\n4. **Minimize data transfer** costs\n5. **Use Direct Connect** for high bandwidth\n6. **Implement caching** to reduce traffic\n\n## Reference Files\n\n- `references/vpn-setup.md` - VPN configuration guide\n- `references/direct-connect.md` - Direct Connect setup\n\n## Related Skills\n\n- `multi-cloud-architecture` - For architecture decisions\n- `terraform-module-library` - For IaC implementation\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 15, clarity_structure: 12, error_handling: 2',
  'Score: 74, Tier: TIER_2_GOOD, Strengths: problem_definition: 15, workflow_structure: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-cloud-infrastructure-skills-hybrid-cloud-networking-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'solidity-security - Master smart contract security best practices to prevent common vulnerabilities and implement secure Solidity patterns. Use when writing smart contrac',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Solidity Security\n\nMaster smart contract security best practices, vulnerability prevention, and secure Solidity development patterns.\n\n## When to Use This Skill\n\n- Writing secure smart contracts\n- Auditing existing contracts for vulnerabilities\n- Implementing secure DeFi protocols\n- Preventing reentrancy, overflow, and access control issues\n- Optimizing gas usage while maintaining security\n- Preparing contracts for professional audits\n- Understanding common attack vectors\n\n## Critical Vulnerabilities\n\n### 1. Reentrancy\nAttacker calls back into your contract before state is updated.\n\n**Vulnerable Code:**\n```solidity\n// VULNERABLE TO REENTRANCY\ncontract VulnerableBank {\n    mapping(address => uint256) public balances;\n\n    function withdraw() public {\n        uint256 amount = balances[msg.sender];\n\n        // DANGER: External call before state update\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success);\n\n        balances[msg.sender] = 0;  // Too late!\n    }\n}\n```\n\n**Secure Pattern (Checks-Effects-Interactions):**\n```solidity\ncontract SecureBank {\n    mapping(address => uint256) public balances;\n\n    function withdraw() public {\n        uint256 amount = balances[msg.sender];\n        require(amount > 0, \"Insufficient balance\");\n\n        // EFFECTS: Update state BEFORE external call\n        balances[msg.sender] = 0;\n\n        // INTERACTIONS: External call last\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n**Alternative: ReentrancyGuard**\n```solidity\nimport \"@openzeppelin/contracts/security/ReentrancyGuard.sol\";\n\ncontract SecureBank is ReentrancyGuard {\n    mapping(address => uint256) public balances;\n\n    function withdraw() public nonReentrant {\n        uint256 amount = balances[msg.sender];\n        require(amount > 0, \"Insufficient balance\");\n\n        balances[msg.sender] = 0;\n\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n### 2. Integer Overflow/Underflow\n\n**Vulnerable Code (Solidity < 0.8.0):**\n```solidity\n// VULNERABLE\ncontract VulnerableToken {\n    mapping(address => uint256) public balances;\n\n    function transfer(address to, uint256 amount) public {\n        // No overflow check - can wrap around\n        balances[msg.sender] -= amount;  // Can underflow!\n        balances[to] += amount;          // Can overflow!\n    }\n}\n```\n\n**Secure Pattern (Solidity >= 0.8.0):**\n```solidity\n// Solidity 0.8+ has built-in overflow/underflow checks\ncontract SecureToken {\n    mapping(address => uint256) public balances;\n\n    function transfer(address to, uint256 amount) public {\n        // Automatically reverts on overflow/underflow\n        balances[msg.sender] -= amount;\n        balances[to] += amount;\n    }\n}\n```\n\n**For Solidity < 0.8.0, use SafeMath:**\n```solidity\nimport \"@openzeppelin/contracts/utils/math/SafeMath.sol\";\n\ncontract SecureToken {\n    using SafeMath for uint256;\n    mapping(address => uint256) public balances;\n\n    function transfer(address to, uint256 amount) public {\n        balances[msg.sender] = balances[msg.sender].sub(amount);\n        balances[to] = balances[to].add(amount);\n    }\n}\n```\n\n### 3. Access Control\n\n**Vulnerable Code:**\n```solidity\n// VULNERABLE: Anyone can call critical functions\ncontract VulnerableContract {\n    address public owner;\n\n    function withdraw(uint256 amount) public {\n        // No access control!\n        payable(msg.sender).transfer(amount);\n    }\n}\n```\n\n**Secure Pattern:**\n```solidity\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\n\ncontract SecureContract is Ownable {\n    function withdraw(uint256 amount) public onlyOwner {\n        payable(owner()).transfer(amount);\n    }\n}\n\n// Or implement custom role-based access\ncontract RoleBasedContract {\n    mapping(address => bool) public admins;\n\n    modifier onlyAdmin() {\n        require(admins[msg.sender], \"Not an admin\");\n        _;\n    }\n\n    function criticalFunction() public onlyAdmin {\n        // Protected function\n    }\n}\n```\n\n### 4. Front-Running\n\n**Vulnerable:**\n```solidity\n// VULNERABLE TO FRONT-RUNNING\ncontract VulnerableDEX {\n    function swap(uint256 amount, uint256 minOutput) public {\n        // Attacker sees this in mempool and front-runs\n        uint256 output = calculateOutput(amount);\n        require(output >= minOutput, \"Slippage too high\");\n        // Perform swap\n    }\n}\n```\n\n**Mitigation:**\n```solidity\ncontract SecureDEX {\n    mapping(bytes32 => bool) public usedCommitments;\n\n    // Step 1: Commit to trade\n    function commitTrade(bytes32 commitment) public {\n        usedCommitments[commitment] = true;\n    }\n\n    // Step 2: Reveal trade (next block)\n    function revealTrade(\n        uint256 amount,\n        uint256 minOutput,\n        bytes32 secret\n    ) public {\n        bytes32 commitment = keccak256(abi.encodePacked(\n            msg.sender, amount, minOutput, secret\n        ));\n        require(usedCommitments[commitment], \"Invalid commitment\");\n        // Perform swap\n    }\n}\n```\n\n## Security Best Practices\n\n### Checks-Effects-Interactions Pattern\n```solidity\ncontract SecurePattern {\n    mapping(address => uint256) public balances;\n\n    function withdraw(uint256 amount) public {\n        // 1. CHECKS: Validate conditions\n        require(amount <= balances[msg.sender], \"Insufficient balance\");\n        require(amount > 0, \"Amount must be positive\");\n\n        // 2. EFFECTS: Update state\n        balances[msg.sender] -= amount;\n\n        // 3. INTERACTIONS: External calls last\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n### Pull Over Push Pattern\n```solidity\n// Prefer this (pull)\ncontract SecurePayment {\n    mapping(address => uint256) public pendingWithdrawals;\n\n    function recordPayment(address recipient, uint256 amount) internal {\n        pendingWithdrawals[recipient] += amount;\n    }\n\n    function withdraw() public {\n        uint256 amount = pendingWithdrawals[msg.sender];\n        require(amount > 0, \"Nothing to withdraw\");\n\n        pendingWithdrawals[msg.sender] = 0;\n        payable(msg.sender).transfer(amount);\n    }\n}\n\n// Over this (push)\ncontract RiskyPayment {\n    function distributePayments(address[] memory recipients, uint256[] memory amounts) public {\n        for (uint i = 0; i < recipients.length; i++) {\n            // If any transfer fails, entire batch fails\n            payable(recipients[i]).transfer(amounts[i]);\n        }\n    }\n}\n```\n\n### Input Validation\n```solidity\ncontract SecureContract {\n    function transfer(address to, uint256 amount) public {\n        // Validate inputs\n        require(to != address(0), \"Invalid recipient\");\n        require(to != address(this), \"Cannot send to contract\");\n        require(amount > 0, \"Amount must be positive\");\n        require(amount <= balances[msg.sender], \"Insufficient balance\");\n\n        // Proceed with transfer\n        balances[msg.sender] -= amount;\n        balances[to] += amount;\n    }\n}\n```\n\n### Emergency Stop (Circuit Breaker)\n```solidity\nimport \"@openzeppelin/contracts/security/Pausable.sol\";\n\ncontract EmergencyStop is Pausable, Ownable {\n    function criticalFunction() public whenNotPaused {\n        // Function logic\n    }\n\n    function emergencyStop() public onlyOwner {\n        _pause();\n    }\n\n    function resume() public onlyOwner {\n        _unpause();\n    }\n}\n```\n\n## Gas Optimization\n\n### Use `uint256` Instead of Smaller Types\n```solidity\n// More gas efficient\ncontract GasEfficient {\n    uint256 public value;  // Optimal\n\n    function set(uint256 _value) public {\n        value = _value;\n    }\n}\n\n// Less efficient\ncontract GasInefficient {\n    uint8 public value;  // Still uses 256-bit slot\n\n    function set(uint8 _value) public {\n        value = _value;  // Extra gas for type conversion\n    }\n}\n```\n\n### Pack Storage Variables\n```solidity\n// Gas efficient (3 variables in 1 slot)\ncontract PackedStorage {\n    uint128 public a;  // Slot 0\n    uint64 public b;   // Slot 0\n    uint64 public c;   // Slot 0\n    uint256 public d;  // Slot 1\n}\n\n// Gas inefficient (each variable in separate slot)\ncontract UnpackedStorage {\n    uint256 public a;  // Slot 0\n    uint256 public b;  // Slot 1\n    uint256 public c;  // Slot 2\n    uint256 public d;  // Slot 3\n}\n```\n\n### Use `calldata` Instead of `memory` for Function Arguments\n```solidity\ncontract GasOptimized {\n    // More gas efficient\n    function processData(uint256[] calldata data) public pure returns (uint256) {\n        return data[0];\n    }\n\n    // Less efficient\n    function processDataMemory(uint256[] memory data) public pure returns (uint256) {\n        return data[0];\n    }\n}\n```\n\n### Use Events for Data Storage (When Appropriate)\n```solidity\ncontract EventStorage {\n    // Emitting events is cheaper than storage\n    event DataStored(address indexed user, uint256 indexed id, bytes data);\n\n    function storeData(uint256 id, bytes calldata data) public {\n        emit DataStored(msg.sender, id, data);\n        // Don''''t store in contract storage unless needed\n    }\n}\n```\n\n## Common Vulnerabilities Checklist\n\n```solidity\n// Security Checklist Contract\ncontract SecurityChecklist {\n    /**\n     * [ ] Reentrancy protection (ReentrancyGuard or CEI pattern)\n     * [ ] Integer overflow/underflow (Solidity 0.8+ or SafeMath)\n     * [ ] Access control (Ownable, roles, modifiers)\n     * [ ] Input validation (require statements)\n     * [ ] Front-running mitigation (commit-reveal if applicable)\n     * [ ] Gas optimization (packed storage, calldata)\n     * [ ] Emergency stop mechanism (Pausable)\n     * [ ] Pull over push pattern for payments\n     * [ ] No delegatecall to untrusted contracts\n     * [ ] No tx.origin for authentication (use msg.sender)\n     * [ ] Proper event emission\n     * [ ] External calls at end of function\n     * [ ] Check return values of external calls\n     * [ ] No hardcoded addresses\n     * [ ] Upgrade mechanism (if proxy pattern)\n     */\n}\n```\n\n## Testing for Security\n\n```javascript\n// Hardhat test example\nconst { expect } = require(\"chai\");\nconst { ethers } = require(\"hardhat\");\n\ndescribe(\"Security Tests\", function () {\n    it(\"Should prevent reentrancy attack\", async function () {\n        const [attacker] = await ethers.getSigners();\n\n        const VictimBank = await ethers.getContractFactory(\"SecureBank\");\n        const bank = await VictimBank.deploy();\n\n        const Attacker = await ethers.getContractFactory(\"ReentrancyAttacker\");\n        const attackerContract = await Attacker.deploy(bank.address);\n\n        // Deposit funds\n        await bank.deposit({value: ethers.utils.parseEther(\"10\")});\n\n        // Attempt reentrancy attack\n        await expect(\n            attackerContract.attack({value: ethers.utils.parseEther(\"1\")})\n        ).to.be.revertedWith(\"ReentrancyGuard: reentrant call\");\n    });\n\n    it(\"Should prevent integer overflow\", async function () {\n        const Token = await ethers.getContractFactory(\"SecureToken\");\n        const token = await Token.deploy();\n\n        // Attempt overflow\n        await expect(\n            token.transfer(attacker.address, ethers.constants.MaxUint256)\n        ).to.be.reverted;\n    });\n\n    it(\"Should enforce access control\", async function () {\n        const [owner, attacker] = await ethers.getSigners();\n\n        const Contract = await ethers.getContractFactory(\"SecureContract\");\n        const contract = await Contract.deploy();\n\n        // Attempt unauthorized withdrawal\n        await expect(\n            contract.connect(attacker).withdraw(100)\n        ).to.be.revertedWith(\"Ownable: caller is not the owner\");\n    });\n});\n```\n\n## Audit Preparation\n\n```solidity\ncontract WellDocumentedContract {\n    /**\n     * @title Well Documented Contract\n     * @dev Example of proper documentation for audits\n     * @notice This contract handles user deposits and withdrawals\n     */\n\n    /// @notice Mapping of user balances\n    mapping(address => uint256) public balances;\n\n    /**\n     * @dev Deposits ETH into the contract\n     * @notice Anyone can deposit funds\n     */\n    function deposit() public payable {\n        require(msg.value > 0, \"Must send ETH\");\n        balances[msg.sender] += msg.value;\n    }\n\n    /**\n     * @dev Withdraws user''''s balance\n     * @notice Follows CEI pattern to prevent reentrancy\n     * @param amount Amount to withdraw in wei\n     */\n    function withdraw(uint256 amount) public {\n        // CHECKS\n        require(amount <= balances[msg.sender], \"Insufficient balance\");\n\n        // EFFECTS\n        balances[msg.sender] -= amount;\n\n        // INTERACTIONS\n        (bool success, ) = msg.sender.call{value: amount}(\"\");\n        require(success, \"Transfer failed\");\n    }\n}\n```\n\n## Resources\n\n- **references/reentrancy.md**: Comprehensive reentrancy prevention\n- **references/access-control.md**: Role-based access patterns\n- **references/overflow-underflow.md**: SafeMath and integer safety\n- **references/gas-optimization.md**: Gas saving techniques\n- **references/vulnerability-patterns.md**: Common vulnerability catalog\n- **assets/solidity-contracts-templates.sol**: Secure contract templates\n- **assets/security-checklist.md**: Pre-audit checklist\n- **scripts/analyze-contract.sh**: Static analysis tools\n\n## Tools for Security Analysis\n\n- **Slither**: Static analysis tool\n- **Mythril**: Security analysis tool\n- **Echidna**: Fuzzing tool\n- **Manticore**: Symbolic execution\n- **Securify**: Automated security scanner\n\n## Common Pitfalls\n\n1. **Using `tx.origin` for Authentication**: Use `msg.sender` instead\n2. **Unchecked External Calls**: Always check return values\n3. **Delegatecall to Untrusted Contracts**: Can hijack your contract\n4. **Floating Pragma**: Pin to specific Solidity version\n5. **Missing Events**: Emit events for state changes\n6. **Excessive Gas in Loops**: Can hit block gas limit\n7. **No Upgrade Path**: Consider proxy patterns if upgrades needed\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'completeness: 15, error_handling: 12, clarity_structure: 12',
  'Score: 92, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 18, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-blockchain-web3-skills-solidity-security-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'python-packaging - Create distributable Python packages with proper project structure, setup.py/pyproject.toml, and publishing to PyPI. Use when packaging Python librari',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Python Packaging\n\nComprehensive guide to creating, structuring, and distributing Python packages using modern packaging tools, pyproject.toml, and publishing to PyPI.\n\n## When to Use This Skill\n\n- Creating Python libraries for distribution\n- Building command-line tools with entry points\n- Publishing packages to PyPI or private repositories\n- Setting up Python project structure\n- Creating installable packages with dependencies\n- Building wheels and source distributions\n- Versioning and releasing Python packages\n- Creating namespace packages\n- Implementing package metadata and classifiers\n\n## Core Concepts\n\n### 1. Package Structure\n- **Source layout**: `src/package_name/` (recommended)\n- **Flat layout**: `package_name/` (simpler but less flexible)\n- **Package metadata**: pyproject.toml, setup.py, or setup.cfg\n- **Distribution formats**: wheel (.whl) and source distribution (.tar.gz)\n\n### 2. Modern Packaging Standards\n- **PEP 517/518**: Build system requirements\n- **PEP 621**: Metadata in pyproject.toml\n- **PEP 660**: Editable installs\n- **pyproject.toml**: Single source of configuration\n\n### 3. Build Backends\n- **setuptools**: Traditional, widely used\n- **hatchling**: Modern, opinionated\n- **flit**: Lightweight, for pure Python\n- **poetry**: Dependency management + packaging\n\n### 4. Distribution\n- **PyPI**: Python Package Index (public)\n- **TestPyPI**: Testing before production\n- **Private repositories**: JFrog, AWS CodeArtifact, etc.\n\n## Quick Start\n\n### Minimal Package Structure\n\n```\nmy-package/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_package/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 module.py\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_module.py\n```\n\n### Minimal pyproject.toml\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\nversion = \"0.1.0\"\ndescription = \"A short description\"\nauthors = [{name = \"Your Name\", email = \"you@example.com\"}]\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\ndependencies = [\n    \"requests>=2.28.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0\",\n    \"black>=22.0\",\n]\n```\n\n## Package Structure Patterns\n\n### Pattern 1: Source Layout (Recommended)\n\n```\nmy-package/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 .gitignore\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 my_package/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 core.py\n\u2502       \u251c\u2500\u2500 utils.py\n\u2502       \u2514\u2500\u2500 py.typed          # For type hints\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 test_core.py\n\u2502   \u2514\u2500\u2500 test_utils.py\n\u2514\u2500\u2500 docs/\n    \u2514\u2500\u2500 index.md\n```\n\n**Advantages:**\n- Prevents accidentally importing from source\n- Cleaner test imports\n- Better isolation\n\n**pyproject.toml for source layout:**\n```toml\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\n```\n\n### Pattern 2: Flat Layout\n\n```\nmy-package/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 my_package/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 module.py\n\u2514\u2500\u2500 tests/\n    \u2514\u2500\u2500 test_module.py\n```\n\n**Simpler but:**\n- Can import package without installing\n- Less professional for libraries\n\n### Pattern 3: Multi-Package Project\n\n```\nproject/\n\u251c\u2500\u2500 pyproject.toml\n\u251c\u2500\u2500 packages/\n\u2502   \u251c\u2500\u2500 package-a/\n\u2502   \u2502   \u2514\u2500\u2500 src/\n\u2502   \u2502       \u2514\u2500\u2500 package_a/\n\u2502   \u2514\u2500\u2500 package-b/\n\u2502       \u2514\u2500\u2500 src/\n\u2502           \u2514\u2500\u2500 package_b/\n\u2514\u2500\u2500 tests/\n```\n\n## Complete pyproject.toml Examples\n\n### Pattern 4: Full-Featured pyproject.toml\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-awesome-package\"\nversion = \"1.0.0\"\ndescription = \"An awesome Python package\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nlicense = {text = \"MIT\"}\nauthors = [\n    {name = \"Your Name\", email = \"you@example.com\"},\n]\nmaintainers = [\n    {name = \"Maintainer Name\", email = \"maintainer@example.com\"},\n]\nkeywords = [\"example\", \"package\", \"awesome\"]\nclassifiers = [\n    \"Development Status :: 4 - Beta\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n]\n\ndependencies = [\n    \"requests>=2.28.0,<3.0.0\",\n    \"click>=8.0.0\",\n    \"pydantic>=2.0.0\",\n]\n\n[project.optional-dependencies]\ndev = [\n    \"pytest>=7.0.0\",\n    \"pytest-cov>=4.0.0\",\n    \"black>=23.0.0\",\n    \"ruff>=0.1.0\",\n    \"mypy>=1.0.0\",\n]\ndocs = [\n    \"sphinx>=5.0.0\",\n    \"sphinx-rtd-theme>=1.0.0\",\n]\nall = [\n    \"my-awesome-package[dev,docs]\",\n]\n\n[project.urls]\nHomepage = \"https://github.com/username/my-awesome-package\"\nDocumentation = \"https://my-awesome-package.readthedocs.io\"\nRepository = \"https://github.com/username/my-awesome-package\"\n\"Bug Tracker\" = \"https://github.com/username/my-awesome-package/issues\"\nChangelog = \"https://github.com/username/my-awesome-package/blob/main/CHANGELOG.md\"\n\n[project.scripts]\nmy-cli = \"my_package.cli:main\"\nawesome-tool = \"my_package.tools:run\"\n\n[project.entry-points.\"my_package.plugins\"]\nplugin1 = \"my_package.plugins:plugin1\"\n\n[tool.setuptools]\npackage-dir = {\"\" = \"src\"}\nzip-safe = false\n\n[tool.setuptools.packages.find]\nwhere = [\"src\"]\ninclude = [\"my_package*\"]\nexclude = [\"tests*\"]\n\n[tool.setuptools.package-data]\nmy_package = [\"py.typed\", \"*.pyi\", \"data/*.json\"]\n\n# Black configuration\n[tool.black]\nline-length = 100\ntarget-version = [\"py38\", \"py39\", \"py310\", \"py311\"]\ninclude = ''''\\.pyi?$''''\n\n# Ruff configuration\n[tool.ruff]\nline-length = 100\ntarget-version = \"py38\"\n\n[tool.ruff.lint]\nselect = [\"E\", \"F\", \"I\", \"N\", \"W\", \"UP\"]\n\n# MyPy configuration\n[tool.mypy]\npython_version = \"3.8\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = true\n\n# Pytest configuration\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = \"-v --cov=my_package --cov-report=term-missing\"\n\n# Coverage configuration\n[tool.coverage.run]\nsource = [\"src\"]\nomit = [\"*/tests/*\"]\n\n[tool.coverage.report]\nexclude_lines = [\n    \"pragma: no cover\",\n    \"def __repr__\",\n    \"raise AssertionError\",\n    \"raise NotImplementedError\",\n]\n```\n\n### Pattern 5: Dynamic Versioning\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"setuptools-scm>=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\ndynamic = [\"version\"]\ndescription = \"Package with dynamic version\"\n\n[tool.setuptools.dynamic]\nversion = {attr = \"my_package.__version__\"}\n\n# Or use setuptools-scm for git-based versioning\n[tool.setuptools_scm]\nwrite_to = \"src/my_package/_version.py\"\n```\n\n**In __init__.py:**\n```python\n# src/my_package/__init__.py\n__version__ = \"1.0.0\"\n\n# Or with setuptools-scm\nfrom importlib.metadata import version\n__version__ = version(\"my-package\")\n```\n\n## Command-Line Interface (CLI) Patterns\n\n### Pattern 6: CLI with Click\n\n```python\n# src/my_package/cli.py\nimport click\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"My awesome CLI tool.\"\"\"\n    pass\n\n@cli.command()\n@click.argument(\"name\")\n@click.option(\"--greeting\", default=\"Hello\", help=\"Greeting to use\")\ndef greet(name: str, greeting: str):\n    \"\"\"Greet someone.\"\"\"\n    click.echo(f\"{greeting}, {name}!\")\n\n@cli.command()\n@click.option(\"--count\", default=1, help=\"Number of times to repeat\")\ndef repeat(count: int):\n    \"\"\"Repeat a message.\"\"\"\n    for i in range(count):\n        click.echo(f\"Message {i + 1}\")\n\ndef main():\n    \"\"\"Entry point for CLI.\"\"\"\n    cli()\n\nif __name__ == \"__main__\":\n    main()\n```\n\n**Register in pyproject.toml:**\n```toml\n[project.scripts]\nmy-tool = \"my_package.cli:main\"\n```\n\n**Usage:**\n```bash\npip install -e .\nmy-tool greet World\nmy-tool greet Alice --greeting=\"Hi\"\nmy-tool repeat --count=3\n```\n\n### Pattern 7: CLI with argparse\n\n```python\n# src/my_package/cli.py\nimport argparse\nimport sys\n\ndef main():\n    \"\"\"Main CLI entry point.\"\"\"\n    parser = argparse.ArgumentParser(\n        description=\"My awesome tool\",\n        prog=\"my-tool\"\n    )\n\n    parser.add_argument(\n        \"--version\",\n        action=\"version\",\n        version=\"%(prog)s 1.0.0\"\n    )\n\n    subparsers = parser.add_subparsers(dest=\"command\", help=\"Commands\")\n\n    # Add subcommand\n    process_parser = subparsers.add_parser(\"process\", help=\"Process data\")\n    process_parser.add_argument(\"input_file\", help=\"Input file path\")\n    process_parser.add_argument(\n        \"--output\", \"-o\",\n        default=\"output.txt\",\n        help=\"Output file path\"\n    )\n\n    args = parser.parse_args()\n\n    if args.command == \"process\":\n        process_data(args.input_file, args.output)\n    else:\n        parser.print_help()\n        sys.exit(1)\n\ndef process_data(input_file: str, output_file: str):\n    \"\"\"Process data from input to output.\"\"\"\n    print(f\"Processing {input_file} -> {output_file}\")\n\nif __name__ == \"__main__\":\n    main()\n```\n\n## Building and Publishing\n\n### Pattern 8: Build Package Locally\n\n```bash\n# Install build tools\npip install build twine\n\n# Build distribution\npython -m build\n\n# This creates:\n# dist/\n#   my-package-1.0.0.tar.gz (source distribution)\n#   my_package-1.0.0-py3-none-any.whl (wheel)\n\n# Check the distribution\ntwine check dist/*\n```\n\n### Pattern 9: Publishing to PyPI\n\n```bash\n# Install publishing tools\npip install twine\n\n# Test on TestPyPI first\ntwine upload --repository testpypi dist/*\n\n# Install from TestPyPI to test\npip install --index-url https://test.pypi.org/simple/ my-package\n\n# If all good, publish to PyPI\ntwine upload dist/*\n```\n\n**Using API tokens (recommended):**\n```bash\n# Create ~/.pypirc\n[distutils]\nindex-servers =\n    pypi\n    testpypi\n\n[pypi]\nusername = __token__\npassword = pypi-...your-token...\n\n[testpypi]\nusername = __token__\npassword = pypi-...your-test-token...\n```\n\n### Pattern 10: Automated Publishing with GitHub Actions\n\n```yaml\n# .github/workflows/publish.yml\nname: Publish to PyPI\n\non:\n  release:\n    types: [created]\n\njobs:\n  publish:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Set up Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n\n      - name: Install dependencies\n        run: |\n          pip install build twine\n\n      - name: Build package\n        run: python -m build\n\n      - name: Check package\n        run: twine check dist/*\n\n      - name: Publish to PyPI\n        env:\n          TWINE_USERNAME: __token__\n          TWINE_PASSWORD: ${{ secrets.PYPI_API_TOKEN }}\n        run: twine upload dist/*\n```\n\n## Advanced Patterns\n\n### Pattern 11: Including Data Files\n\n```toml\n[tool.setuptools.package-data]\nmy_package = [\n    \"data/*.json\",\n    \"templates/*.html\",\n    \"static/css/*.css\",\n    \"py.typed\",\n]\n```\n\n**Accessing data files:**\n```python\n# src/my_package/loader.py\nfrom importlib.resources import files\nimport json\n\ndef load_config():\n    \"\"\"Load configuration from package data.\"\"\"\n    config_file = files(\"my_package\").joinpath(\"data/config.json\")\n    with config_file.open() as f:\n        return json.load(f)\n\n# Python 3.9+\nfrom importlib.resources import files\n\ndata = files(\"my_package\").joinpath(\"data/file.txt\").read_text()\n```\n\n### Pattern 12: Namespace Packages\n\n**For large projects split across multiple repositories:**\n\n```\n# Package 1: company-core\ncompany/\n\u2514\u2500\u2500 core/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 models.py\n\n# Package 2: company-api\ncompany/\n\u2514\u2500\u2500 api/\n    \u251c\u2500\u2500 __init__.py\n    \u2514\u2500\u2500 routes.py\n```\n\n**Do NOT include __init__.py in the namespace directory (company/):**\n\n```toml\n# company-core/pyproject.toml\n[project]\nname = \"company-core\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"company.core*\"]\n\n# company-api/pyproject.toml\n[project]\nname = \"company-api\"\n\n[tool.setuptools.packages.find]\nwhere = [\".\"]\ninclude = [\"company.api*\"]\n```\n\n**Usage:**\n```python\n# Both packages can be imported under same namespace\nfrom company.core import models\nfrom company.api import routes\n```\n\n### Pattern 13: C Extensions\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"wheel\", \"Cython>=0.29\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools]\next-modules = [\n    {name = \"my_package.fast_module\", sources = [\"src/fast_module.c\"]},\n]\n```\n\n**Or with setup.py:**\n```python\n# setup.py\nfrom setuptools import setup, Extension\n\nsetup(\n    ext_modules=[\n        Extension(\n            \"my_package.fast_module\",\n            sources=[\"src/fast_module.c\"],\n            include_dirs=[\"src/include\"],\n        )\n    ]\n)\n```\n\n## Version Management\n\n### Pattern 14: Semantic Versioning\n\n```python\n# src/my_package/__init__.py\n__version__ = \"1.2.3\"\n\n# Semantic versioning: MAJOR.MINOR.PATCH\n# MAJOR: Breaking changes\n# MINOR: New features (backward compatible)\n# PATCH: Bug fixes\n```\n\n**Version constraints in dependencies:**\n```toml\ndependencies = [\n    \"requests>=2.28.0,<3.0.0\",  # Compatible range\n    \"click~=8.1.0\",              # Compatible release (~= 8.1.0 means >=8.1.0,<8.2.0)\n    \"pydantic>=2.0\",             # Minimum version\n    \"numpy==1.24.3\",             # Exact version (avoid if possible)\n]\n```\n\n### Pattern 15: Git-Based Versioning\n\n```toml\n[build-system]\nrequires = [\"setuptools>=61.0\", \"setuptools-scm>=8.0\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"my-package\"\ndynamic = [\"version\"]\n\n[tool.setuptools_scm]\nwrite_to = \"src/my_package/_version.py\"\nversion_scheme = \"post-release\"\nlocal_scheme = \"dirty-tag\"\n```\n\n**Creates versions like:**\n- `1.0.0` (from git tag)\n- `1.0.1.dev3+g1234567` (3 commits after tag)\n\n## Testing Installation\n\n### Pattern 16: Editable Install\n\n```bash\n# Install in development mode\npip install -e .\n\n# With optional dependencies\npip install -e \".[dev]\"\npip install -e \".[dev,docs]\"\n\n# Now changes to source code are immediately reflected\n```\n\n### Pattern 17: Testing in Isolated Environment\n\n```bash\n# Create virtual environment\npython -m venv test-env\nsource test-env/bin/activate  # Linux/Mac\n# test-env\\Scripts\\activate  # Windows\n\n# Install package\npip install dist/my_package-1.0.0-py3-none-any.whl\n\n# Test it works\npython -c \"import my_package; print(my_package.__version__)\"\n\n# Test CLI\nmy-tool --help\n\n# Cleanup\ndeactivate\nrm -rf test-env\n```\n\n## Documentation\n\n### Pattern 18: README.md Template\n\n```markdown\n# My Package\n\n[![PyPI version](https://badge.fury.io/py/my-package.svg)](https://pypi.org/project/my-package/)\n[![Python versions](https://img.shields.io/pypi/pyversions/my-package.svg)](https://pypi.org/project/my-package/)\n[![Tests](https://github.com/username/my-package/workflows/Tests/badge.svg)](https://github.com/username/my-package/actions)\n\nBrief description of your package.\n\n## Installation\n\n```bash\npip install my-package\n```\n\n## Quick Start\n\n```python\nfrom my_package import something\n\nresult = something.do_stuff()\n```\n\n## Features\n\n- Feature 1\n- Feature 2\n- Feature 3\n\n## Documentation\n\nFull documentation: https://my-package.readthedocs.io\n\n## Development\n\n```bash\ngit clone https://github.com/username/my-package.git\ncd my-package\npip install -e \".[dev]\"\npytest\n```\n\n## License\n\nMIT\n```\n\n## Common Patterns\n\n### Pattern 19: Multi-Architecture Wheels\n\n```yaml\n# .github/workflows/wheels.yml\nname: Build wheels\n\non: [push, pull_request]\n\njobs:\n  build_wheels:\n    name: Build wheels on ${{ matrix.os }}\n    runs-on: ${{ matrix.os }}\n    strategy:\n      matrix:\n        os: [ubuntu-latest, windows-latest, macos-latest]\n\n    steps:\n      - uses: actions/checkout@v3\n\n      - name: Build wheels\n        uses: pypa/cibuildwheel@v2.16.2\n\n      - uses: actions/upload-artifact@v3\n        with:\n          path: ./wheelhouse/*.whl\n```\n\n### Pattern 20: Private Package Index\n\n```bash\n# Install from private index\npip install my-package --index-url https://private.pypi.org/simple/\n\n# Or add to pip.conf\n[global]\nindex-url = https://private.pypi.org/simple/\nextra-index-url = https://pypi.org/simple/\n\n# Upload to private index\ntwine upload --repository-url https://private.pypi.org/ dist/*\n```\n\n## File Templates\n\n### .gitignore for Python Packages\n\n```gitignore\n# Build artifacts\nbuild/\ndist/\n*.egg-info/\n*.egg\n.eggs/\n\n# Python\n__pycache__/\n*.py[cod]\n*$py.class\n*.so\n\n# Virtual environments\nvenv/\nenv/\nENV/\n\n# IDE\n.vscode/\n.idea/\n*.swp\n\n# Testing\n.pytest_cache/\n.coverage\nhtmlcov/\n\n# Distribution\n*.whl\n*.tar.gz\n```\n\n### MANIFEST.in\n\n```\n# MANIFEST.in\ninclude README.md\ninclude LICENSE\ninclude pyproject.toml\n\nrecursive-include src/my_package/data *.json\nrecursive-include src/my_package/templates *.html\nrecursive-exclude * __pycache__\nrecursive-exclude * *.py[co]\n```\n\n## Checklist for Publishing\n\n- [ ] Code is tested (pytest passing)\n- [ ] Documentation is complete (README, docstrings)\n- [ ] Version number updated\n- [ ] CHANGELOG.md updated\n- [ ] License file included\n- [ ] pyproject.toml is complete\n- [ ] Package builds without errors\n- [ ] Installation tested in clean environment\n- [ ] CLI tools work (if applicable)\n- [ ] PyPI metadata is correct (classifiers, keywords)\n- [ ] GitHub repository linked\n- [ ] Tested on TestPyPI first\n- [ ] Git tag created for release\n\n## Resources\n\n- **Python Packaging Guide**: https://packaging.python.org/\n- **PyPI**: https://pypi.org/\n- **TestPyPI**: https://test.pypi.org/\n- **setuptools documentation**: https://setuptools.pypa.io/\n- **build**: https://pypa-build.readthedocs.io/\n- **twine**: https://twine.readthedocs.io/\n\n## Best Practices Summary\n\n1. **Use src/ layout** for cleaner package structure\n2. **Use pyproject.toml** for modern packaging\n3. **Pin build dependencies** in build-system.requires\n4. **Version appropriately** with semantic versioning\n5. **Include all metadata** (classifiers, URLs, etc.)\n6. **Test installation** in clean environments\n7. **Use TestPyPI** before publishing to PyPI\n8. **Document thoroughly** with README and docstrings\n9. **Include LICENSE** file\n10. **Automate publishing** with CI/CD\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 13',
  'Score: 98, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-python-development-skills-python-packaging-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'nft-standards - Implement NFT standards (ERC-721, ERC-1155) with proper metadata handling, minting strategies, and marketplace integration. Use when creating NFT cont',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# NFT Standards\n\nMaster ERC-721 and ERC-1155 NFT standards, metadata best practices, and advanced NFT features.\n\n## When to Use This Skill\n\n- Creating NFT collections (art, gaming, collectibles)\n- Implementing marketplace functionality\n- Building on-chain or off-chain metadata\n- Creating soulbound tokens (non-transferable)\n- Implementing royalties and revenue sharing\n- Developing dynamic/evolving NFTs\n\n## ERC-721 (Non-Fungible Token Standard)\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC721/extensions/ERC721URIStorage.sol\";\nimport \"@openzeppelin/contracts/token/ERC721/extensions/ERC721Enumerable.sol\";\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\nimport \"@openzeppelin/contracts/utils/Counters.sol\";\n\ncontract MyNFT is ERC721URIStorage, ERC721Enumerable, Ownable {\n    using Counters for Counters.Counter;\n    Counters.Counter private _tokenIds;\n\n    uint256 public constant MAX_SUPPLY = 10000;\n    uint256 public constant MINT_PRICE = 0.08 ether;\n    uint256 public constant MAX_PER_MINT = 20;\n\n    constructor() ERC721(\"MyNFT\", \"MNFT\") {}\n\n    function mint(uint256 quantity) external payable {\n        require(quantity > 0 && quantity <= MAX_PER_MINT, \"Invalid quantity\");\n        require(_tokenIds.current() + quantity <= MAX_SUPPLY, \"Exceeds max supply\");\n        require(msg.value >= MINT_PRICE * quantity, \"Insufficient payment\");\n\n        for (uint256 i = 0; i < quantity; i++) {\n            _tokenIds.increment();\n            uint256 newTokenId = _tokenIds.current();\n            _safeMint(msg.sender, newTokenId);\n            _setTokenURI(newTokenId, generateTokenURI(newTokenId));\n        }\n    }\n\n    function generateTokenURI(uint256 tokenId) internal pure returns (string memory) {\n        // Return IPFS URI or on-chain metadata\n        return string(abi.encodePacked(\"ipfs://QmHash/\", Strings.toString(tokenId), \".json\"));\n    }\n\n    // Required overrides\n    function _beforeTokenTransfer(\n        address from,\n        address to,\n        uint256 tokenId,\n        uint256 batchSize\n    ) internal override(ERC721, ERC721Enumerable) {\n        super._beforeTokenTransfer(from, to, tokenId, batchSize);\n    }\n\n    function _burn(uint256 tokenId) internal override(ERC721, ERC721URIStorage) {\n        super._burn(tokenId);\n    }\n\n    function tokenURI(uint256 tokenId) public view override(ERC721, ERC721URIStorage) returns (string memory) {\n        return super.tokenURI(tokenId);\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, ERC721Enumerable)\n        returns (bool)\n    {\n        return super.supportsInterface(interfaceId);\n    }\n\n    function withdraw() external onlyOwner {\n        payable(owner()).transfer(address(this).balance);\n    }\n}\n```\n\n## ERC-1155 (Multi-Token Standard)\n\n```solidity\n// SPDX-License-Identifier: MIT\npragma solidity ^0.8.0;\n\nimport \"@openzeppelin/contracts/token/ERC1155/ERC1155.sol\";\nimport \"@openzeppelin/contracts/access/Ownable.sol\";\n\ncontract GameItems is ERC1155, Ownable {\n    uint256 public constant SWORD = 1;\n    uint256 public constant SHIELD = 2;\n    uint256 public constant POTION = 3;\n\n    mapping(uint256 => uint256) public tokenSupply;\n    mapping(uint256 => uint256) public maxSupply;\n\n    constructor() ERC1155(\"ipfs://QmBaseHash/{id}.json\") {\n        maxSupply[SWORD] = 1000;\n        maxSupply[SHIELD] = 500;\n        maxSupply[POTION] = 10000;\n    }\n\n    function mint(\n        address to,\n        uint256 id,\n        uint256 amount\n    ) external onlyOwner {\n        require(tokenSupply[id] + amount <= maxSupply[id], \"Exceeds max supply\");\n\n        _mint(to, id, amount, \"\");\n        tokenSupply[id] += amount;\n    }\n\n    function mintBatch(\n        address to,\n        uint256[] memory ids,\n        uint256[] memory amounts\n    ) external onlyOwner {\n        for (uint256 i = 0; i < ids.length; i++) {\n            require(tokenSupply[ids[i]] + amounts[i] <= maxSupply[ids[i]], \"Exceeds max supply\");\n            tokenSupply[ids[i]] += amounts[i];\n        }\n\n        _mintBatch(to, ids, amounts, \"\");\n    }\n\n    function burn(\n        address from,\n        uint256 id,\n        uint256 amount\n    ) external {\n        require(from == msg.sender || isApprovedForAll(from, msg.sender), \"Not authorized\");\n        _burn(from, id, amount);\n        tokenSupply[id] -= amount;\n    }\n}\n```\n\n## Metadata Standards\n\n### Off-Chain Metadata (IPFS)\n```json\n{\n  \"name\": \"NFT #1\",\n  \"description\": \"Description of the NFT\",\n  \"image\": \"ipfs://QmImageHash\",\n  \"attributes\": [\n    {\n      \"trait_type\": \"Background\",\n      \"value\": \"Blue\"\n    },\n    {\n      \"trait_type\": \"Rarity\",\n      \"value\": \"Legendary\"\n    },\n    {\n      \"trait_type\": \"Power\",\n      \"value\": 95,\n      \"display_type\": \"number\",\n      \"max_value\": 100\n    }\n  ]\n}\n```\n\n### On-Chain Metadata\n```solidity\ncontract OnChainNFT is ERC721 {\n    struct Traits {\n        uint8 background;\n        uint8 body;\n        uint8 head;\n        uint8 rarity;\n    }\n\n    mapping(uint256 => Traits) public tokenTraits;\n\n    function tokenURI(uint256 tokenId) public view override returns (string memory) {\n        Traits memory traits = tokenTraits[tokenId];\n\n        string memory json = Base64.encode(\n            bytes(\n                string(\n                    abi.encodePacked(\n                        ''''{\"name\": \"NFT #'''', Strings.toString(tokenId), ''''\",'''',\n                        ''''\"description\": \"On-chain NFT\",'''',\n                        ''''\"image\": \"data:image/svg+xml;base64,'''', generateSVG(traits), ''''\",'''',\n                        ''''\"attributes\": ['''',\n                        ''''{\"trait_type\": \"Background\", \"value\": \"'''', Strings.toString(traits.background), ''''\"},'''',\n                        ''''{\"trait_type\": \"Rarity\", \"value\": \"'''', getRarityName(traits.rarity), ''''\"}'''',\n                        '''']}''''\n                    )\n                )\n            )\n        );\n\n        return string(abi.encodePacked(\"data:application/json;base64,\", json));\n    }\n\n    function generateSVG(Traits memory traits) internal pure returns (string memory) {\n        // Generate SVG based on traits\n        return \"...\";\n    }\n}\n```\n\n## Royalties (EIP-2981)\n\n```solidity\nimport \"@openzeppelin/contracts/interfaces/IERC2981.sol\";\n\ncontract NFTWithRoyalties is ERC721, IERC2981 {\n    address public royaltyRecipient;\n    uint96 public royaltyFee = 500; // 5%\n\n    constructor() ERC721(\"Royalty NFT\", \"RNFT\") {\n        royaltyRecipient = msg.sender;\n    }\n\n    function royaltyInfo(uint256 tokenId, uint256 salePrice)\n        external\n        view\n        override\n        returns (address receiver, uint256 royaltyAmount)\n    {\n        return (royaltyRecipient, (salePrice * royaltyFee) / 10000);\n    }\n\n    function setRoyalty(address recipient, uint96 fee) external onlyOwner {\n        require(fee <= 1000, \"Royalty fee too high\"); // Max 10%\n        royaltyRecipient = recipient;\n        royaltyFee = fee;\n    }\n\n    function supportsInterface(bytes4 interfaceId)\n        public\n        view\n        override(ERC721, IERC165)\n        returns (bool)\n    {\n        return interfaceId == type(IERC2981).interfaceId ||\n               super.supportsInterface(interfaceId);\n    }\n}\n```\n\n## Soulbound Tokens (Non-Transferable)\n\n```solidity\ncontract SoulboundToken is ERC721 {\n    constructor() ERC721(\"Soulbound\", \"SBT\") {}\n\n    function _beforeTokenTransfer(\n        address from,\n        address to,\n        uint256 tokenId,\n        uint256 batchSize\n    ) internal virtual override {\n        require(from == address(0) || to == address(0), \"Token is soulbound\");\n        super._beforeTokenTransfer(from, to, tokenId, batchSize);\n    }\n\n    function mint(address to) external {\n        uint256 tokenId = totalSupply() + 1;\n        _safeMint(to, tokenId);\n    }\n\n    // Burn is allowed (user can destroy their SBT)\n    function burn(uint256 tokenId) external {\n        require(ownerOf(tokenId) == msg.sender, \"Not token owner\");\n        _burn(tokenId);\n    }\n}\n```\n\n## Dynamic NFTs\n\n```solidity\ncontract DynamicNFT is ERC721 {\n    struct TokenState {\n        uint256 level;\n        uint256 experience;\n        uint256 lastUpdated;\n    }\n\n    mapping(uint256 => TokenState) public tokenStates;\n\n    function gainExperience(uint256 tokenId, uint256 exp) external {\n        require(ownerOf(tokenId) == msg.sender, \"Not token owner\");\n\n        TokenState storage state = tokenStates[tokenId];\n        state.experience += exp;\n\n        // Level up logic\n        if (state.experience >= state.level * 100) {\n            state.level++;\n        }\n\n        state.lastUpdated = block.timestamp;\n    }\n\n    function tokenURI(uint256 tokenId) public view override returns (string memory) {\n        TokenState memory state = tokenStates[tokenId];\n\n        // Generate metadata based on current state\n        return generateMetadata(tokenId, state);\n    }\n\n    function generateMetadata(uint256 tokenId, TokenState memory state)\n        internal\n        pure\n        returns (string memory)\n    {\n        // Dynamic metadata generation\n        return \"\";\n    }\n}\n```\n\n## Gas-Optimized Minting (ERC721A)\n\n```solidity\nimport \"erc721a/contracts/ERC721A.sol\";\n\ncontract OptimizedNFT is ERC721A {\n    uint256 public constant MAX_SUPPLY = 10000;\n    uint256 public constant MINT_PRICE = 0.05 ether;\n\n    constructor() ERC721A(\"Optimized NFT\", \"ONFT\") {}\n\n    function mint(uint256 quantity) external payable {\n        require(_totalMinted() + quantity <= MAX_SUPPLY, \"Exceeds max supply\");\n        require(msg.value >= MINT_PRICE * quantity, \"Insufficient payment\");\n\n        _mint(msg.sender, quantity);\n    }\n\n    function _baseURI() internal pure override returns (string memory) {\n        return \"ipfs://QmBaseHash/\";\n    }\n}\n```\n\n## Resources\n\n- **references/erc721.md**: ERC-721 specification details\n- **references/erc1155.md**: ERC-1155 multi-token standard\n- **references/metadata-standards.md**: Metadata best practices\n- **references/enumeration.md**: Token enumeration patterns\n- **assets/erc721-contract.sol**: Production ERC-721 template\n- **assets/erc1155-contract.sol**: Production ERC-1155 template\n- **assets/metadata-schema.json**: Standard metadata format\n- **assets/metadata-uploader.py**: IPFS upload utility\n\n## Best Practices\n\n1. **Use OpenZeppelin**: Battle-tested implementations\n2. **Pin Metadata**: Use IPFS with pinning service\n3. **Implement Royalties**: EIP-2981 for marketplace compatibility\n4. **Gas Optimization**: Use ERC721A for batch minting\n5. **Reveal Mechanism**: Placeholder \u2192 reveal pattern\n6. **Enumeration**: Support walletOfOwner for marketplaces\n7. **Whitelist**: Merkle trees for efficient whitelisting\n\n## Marketplace Integration\n\n- OpenSea: ERC-721/1155, metadata standards\n- LooksRare: Royalty enforcement\n- Rarible: Protocol fees, lazy minting\n- Blur: Gas-optimized trading\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 5',
  'Score: 80, Tier: TIER_1_EXCELLENT, Strengths: problem_definition: 15, workflow_structure: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-blockchain-web3-skills-nft-standards-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'prompt-engineering-patterns - Master advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability in production. Use when optimizing prompts',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Prompt Engineering Patterns\n\nMaster advanced prompt engineering techniques to maximize LLM performance, reliability, and controllability.\n\n## When to Use This Skill\n\n- Designing complex prompts for production LLM applications\n- Optimizing prompt performance and consistency\n- Implementing structured reasoning patterns (chain-of-thought, tree-of-thought)\n- Building few-shot learning systems with dynamic example selection\n- Creating reusable prompt templates with variable interpolation\n- Debugging and refining prompts that produce inconsistent outputs\n- Implementing system prompts for specialized AI assistants\n\n## Core Capabilities\n\n### 1. Few-Shot Learning\n- Example selection strategies (semantic similarity, diversity sampling)\n- Balancing example count with context window constraints\n- Constructing effective demonstrations with input-output pairs\n- Dynamic example retrieval from knowledge bases\n- Handling edge cases through strategic example selection\n\n### 2. Chain-of-Thought Prompting\n- Step-by-step reasoning elicitation\n- Zero-shot CoT with \"Let''''s think step by step\"\n- Few-shot CoT with reasoning traces\n- Self-consistency techniques (sampling multiple reasoning paths)\n- Verification and validation steps\n\n### 3. Prompt Optimization\n- Iterative refinement workflows\n- A/B testing prompt variations\n- Measuring prompt performance metrics (accuracy, consistency, latency)\n- Reducing token usage while maintaining quality\n- Handling edge cases and failure modes\n\n### 4. Template Systems\n- Variable interpolation and formatting\n- Conditional prompt sections\n- Multi-turn conversation templates\n- Role-based prompt composition\n- Modular prompt components\n\n### 5. System Prompt Design\n- Setting model behavior and constraints\n- Defining output formats and structure\n- Establishing role and expertise\n- Safety guidelines and content policies\n- Context setting and background information\n\n## Quick Start\n\n```python\nfrom prompt_optimizer import PromptTemplate, FewShotSelector\n\n# Define a structured prompt template\ntemplate = PromptTemplate(\n    system=\"You are an expert SQL developer. Generate efficient, secure SQL queries.\",\n    instruction=\"Convert the following natural language query to SQL:\\n{query}\",\n    few_shot_examples=True,\n    output_format=\"SQL code block with explanatory comments\"\n)\n\n# Configure few-shot learning\nselector = FewShotSelector(\n    examples_db=\"sql_examples.jsonl\",\n    selection_strategy=\"semantic_similarity\",\n    max_examples=3\n)\n\n# Generate optimized prompt\nprompt = template.render(\n    query=\"Find all users who registered in the last 30 days\",\n    examples=selector.select(query=\"user registration date filter\")\n)\n```\n\n## Key Patterns\n\n### Progressive Disclosure\nStart with simple prompts, add complexity only when needed:\n\n1. **Level 1**: Direct instruction\n   - \"Summarize this article\"\n\n2. **Level 2**: Add constraints\n   - \"Summarize this article in 3 bullet points, focusing on key findings\"\n\n3. **Level 3**: Add reasoning\n   - \"Read this article, identify the main findings, then summarize in 3 bullet points\"\n\n4. **Level 4**: Add examples\n   - Include 2-3 example summaries with input-output pairs\n\n### Instruction Hierarchy\n```\n[System Context] \u2192 [Task Instruction] \u2192 [Examples] \u2192 [Input Data] \u2192 [Output Format]\n```\n\n### Error Recovery\nBuild prompts that gracefully handle failures:\n- Include fallback instructions\n- Request confidence scores\n- Ask for alternative interpretations when uncertain\n- Specify how to indicate missing information\n\n## Best Practices\n\n1. **Be Specific**: Vague prompts produce inconsistent results\n2. **Show, Don''''t Tell**: Examples are more effective than descriptions\n3. **Test Extensively**: Evaluate on diverse, representative inputs\n4. **Iterate Rapidly**: Small changes can have large impacts\n5. **Monitor Performance**: Track metrics in production\n6. **Version Control**: Treat prompts as code with proper versioning\n7. **Document Intent**: Explain why prompts are structured as they are\n\n## Common Pitfalls\n\n- **Over-engineering**: Starting with complex prompts before trying simple ones\n- **Example pollution**: Using examples that don''''t match the target task\n- **Context overflow**: Exceeding token limits with excessive examples\n- **Ambiguous instructions**: Leaving room for multiple interpretations\n- **Ignoring edge cases**: Not testing on unusual or boundary inputs\n\n## Integration Patterns\n\n### With RAG Systems\n```python\n# Combine retrieved context with prompt engineering\nprompt = f\"\"\"Given the following context:\n{retrieved_context}\n\n{few_shot_examples}\n\nQuestion: {user_question}\n\nProvide a detailed answer based solely on the context above. If the context doesn''''t contain enough information, explicitly state what''''s missing.\"\"\"\n```\n\n### With Validation\n```python\n# Add self-verification step\nprompt = f\"\"\"{main_task_prompt}\n\nAfter generating your response, verify it meets these criteria:\n1. Answers the question directly\n2. Uses only information from provided context\n3. Cites specific sources\n4. Acknowledges any uncertainty\n\nIf verification fails, revise your response.\"\"\"\n```\n\n## Performance Optimization\n\n### Token Efficiency\n- Remove redundant words and phrases\n- Use abbreviations consistently after first definition\n- Consolidate similar instructions\n- Move stable content to system prompts\n\n### Latency Reduction\n- Minimize prompt length without sacrificing quality\n- Use streaming for long-form outputs\n- Cache common prompt prefixes\n- Batch similar requests when possible\n\n## Resources\n\n- **references/few-shot-learning.md**: Deep dive on example selection and construction\n- **references/chain-of-thought.md**: Advanced reasoning elicitation techniques\n- **references/prompt-optimization.md**: Systematic refinement workflows\n- **references/prompt-templates.md**: Reusable template patterns\n- **references/system-prompts.md**: System-level prompt design\n- **assets/prompt-template-library.md**: Battle-tested prompt templates\n- **assets/few-shot-examples.json**: Curated example datasets\n- **scripts/optimize-prompt.py**: Automated prompt optimization tool\n\n## Success Metrics\n\nTrack these KPIs for your prompts:\n- **Accuracy**: Correctness of outputs\n- **Consistency**: Reproducibility across similar inputs\n- **Latency**: Response time (P50, P95, P99)\n- **Token Usage**: Average tokens per request\n- **Success Rate**: Percentage of valid outputs\n- **User Satisfaction**: Ratings and feedback\n\n## Next Steps\n\n1. Review the prompt template library for common patterns\n2. Experiment with few-shot learning for your specific use case\n3. Implement prompt versioning and A/B testing\n4. Set up automated evaluation pipelines\n5. Document your prompt engineering decisions and learnings\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 12, completeness: 10',
  'Score: 89, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 17, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-llm-application-dev-skills-prompt-engineering-patterns-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'debugging-strategies - Master systematic debugging techniques, profiling tools, and root cause analysis to efficiently track down bugs across any codebase or technology stac',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Debugging Strategies\n\nTransform debugging from frustrating guesswork into systematic problem-solving with proven strategies, powerful tools, and methodical approaches.\n\n## When to Use This Skill\n\n- Tracking down elusive bugs\n- Investigating performance issues\n- Understanding unfamiliar codebases\n- Debugging production issues\n- Analyzing crash dumps and stack traces\n- Profiling application performance\n- Investigating memory leaks\n- Debugging distributed systems\n\n## Core Principles\n\n### 1. The Scientific Method\n\n**1. Observe**: What''''s the actual behavior?\n**2. Hypothesize**: What could be causing it?\n**3. Experiment**: Test your hypothesis\n**4. Analyze**: Did it prove/disprove your theory?\n**5. Repeat**: Until you find the root cause\n\n### 2. Debugging Mindset\n\n**Don''''t Assume:**\n- \"It can''''t be X\" - Yes it can\n- \"I didn''''t change Y\" - Check anyway\n- \"It works on my machine\" - Find out why\n\n**Do:**\n- Reproduce consistently\n- Isolate the problem\n- Keep detailed notes\n- Question everything\n- Take breaks when stuck\n\n### 3. Rubber Duck Debugging\n\nExplain your code and problem out loud (to a rubber duck, colleague, or yourself). Often reveals the issue.\n\n## Systematic Debugging Process\n\n### Phase 1: Reproduce\n\n```markdown\n## Reproduction Checklist\n\n1. **Can you reproduce it?**\n   - Always? Sometimes? Randomly?\n   - Specific conditions needed?\n   - Can others reproduce it?\n\n2. **Create minimal reproduction**\n   - Simplify to smallest example\n   - Remove unrelated code\n   - Isolate the problem\n\n3. **Document steps**\n   - Write down exact steps\n   - Note environment details\n   - Capture error messages\n```\n\n### Phase 2: Gather Information\n\n```markdown\n## Information Collection\n\n1. **Error Messages**\n   - Full stack trace\n   - Error codes\n   - Console/log output\n\n2. **Environment**\n   - OS version\n   - Language/runtime version\n   - Dependencies versions\n   - Environment variables\n\n3. **Recent Changes**\n   - Git history\n   - Deployment timeline\n   - Configuration changes\n\n4. **Scope**\n   - Affects all users or specific ones?\n   - All browsers or specific ones?\n   - Production only or also dev?\n```\n\n### Phase 3: Form Hypothesis\n\n```markdown\n## Hypothesis Formation\n\nBased on gathered info, ask:\n\n1. **What changed?**\n   - Recent code changes\n   - Dependency updates\n   - Infrastructure changes\n\n2. **What''''s different?**\n   - Working vs broken environment\n   - Working vs broken user\n   - Before vs after\n\n3. **Where could this fail?**\n   - Input validation\n   - Business logic\n   - Data layer\n   - External services\n```\n\n### Phase 4: Test & Verify\n\n```markdown\n## Testing Strategies\n\n1. **Binary Search**\n   - Comment out half the code\n   - Narrow down problematic section\n   - Repeat until found\n\n2. **Add Logging**\n   - Strategic console.log/print\n   - Track variable values\n   - Trace execution flow\n\n3. **Isolate Components**\n   - Test each piece separately\n   - Mock dependencies\n   - Remove complexity\n\n4. **Compare Working vs Broken**\n   - Diff configurations\n   - Diff environments\n   - Diff data\n```\n\n## Debugging Tools\n\n### JavaScript/TypeScript Debugging\n\n```typescript\n// Chrome DevTools Debugger\nfunction processOrder(order: Order) {\n    debugger;  // Execution pauses here\n\n    const total = calculateTotal(order);\n    console.log(''''Total:'''', total);\n\n    // Conditional breakpoint\n    if (order.items.length > 10) {\n        debugger;  // Only breaks if condition true\n    }\n\n    return total;\n}\n\n// Console debugging techniques\nconsole.log(''''Value:'''', value);                    // Basic\nconsole.table(arrayOfObjects);                   // Table format\nconsole.time(''''operation''''); /* code */ console.timeEnd(''''operation'''');  // Timing\nconsole.trace();                                 // Stack trace\nconsole.assert(value > 0, ''''Value must be positive'''');  // Assertion\n\n// Performance profiling\nperformance.mark(''''start-operation'''');\n// ... operation code\nperformance.mark(''''end-operation'''');\nperformance.measure(''''operation'''', ''''start-operation'''', ''''end-operation'''');\nconsole.log(performance.getEntriesByType(''''measure''''));\n```\n\n**VS Code Debugger Configuration:**\n```json\n// .vscode/launch.json\n{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {\n            \"type\": \"node\",\n            \"request\": \"launch\",\n            \"name\": \"Debug Program\",\n            \"program\": \"${workspaceFolder}/src/index.ts\",\n            \"preLaunchTask\": \"tsc: build - tsconfig.json\",\n            \"outFiles\": [\"${workspaceFolder}/dist/**/*.js\"],\n            \"skipFiles\": [\"<node_internals>/**\"]\n        },\n        {\n            \"type\": \"node\",\n            \"request\": \"launch\",\n            \"name\": \"Debug Tests\",\n            \"program\": \"${workspaceFolder}/node_modules/jest/bin/jest\",\n            \"args\": [\"--runInBand\", \"--no-cache\"],\n            \"console\": \"integratedTerminal\"\n        }\n    ]\n}\n```\n\n### Python Debugging\n\n```python\n# Built-in debugger (pdb)\nimport pdb\n\ndef calculate_total(items):\n    total = 0\n    pdb.set_trace()  # Debugger starts here\n\n    for item in items:\n        total += item.price * item.quantity\n\n    return total\n\n# Breakpoint (Python 3.7+)\ndef process_order(order):\n    breakpoint()  # More convenient than pdb.set_trace()\n    # ... code\n\n# Post-mortem debugging\ntry:\n    risky_operation()\nexcept Exception:\n    import pdb\n    pdb.post_mortem()  # Debug at exception point\n\n# IPython debugging (ipdb)\nfrom ipdb import set_trace\nset_trace()  # Better interface than pdb\n\n# Logging for debugging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = logging.getLogger(__name__)\n\ndef fetch_user(user_id):\n    logger.debug(f''''Fetching user: {user_id}'''')\n    user = db.query(User).get(user_id)\n    logger.debug(f''''Found user: {user}'''')\n    return user\n\n# Profile performance\nimport cProfile\nimport pstats\n\ncProfile.run(''''slow_function()'''', ''''profile_stats'''')\nstats = pstats.Stats(''''profile_stats'''')\nstats.sort_stats(''''cumulative'''')\nstats.print_stats(10)  # Top 10 slowest\n```\n\n### Go Debugging\n\n```go\n// Delve debugger\n// Install: go install github.com/go-delve/delve/cmd/dlv@latest\n// Run: dlv debug main.go\n\nimport (\n    \"fmt\"\n    \"runtime\"\n    \"runtime/debug\"\n)\n\n// Print stack trace\nfunc debugStack() {\n    debug.PrintStack()\n}\n\n// Panic recovery with debugging\nfunc processRequest() {\n    defer func() {\n        if r := recover(); r != nil {\n            fmt.Println(\"Panic:\", r)\n            debug.PrintStack()\n        }\n    }()\n\n    // ... code that might panic\n}\n\n// Memory profiling\nimport _ \"net/http/pprof\"\n// Visit http://localhost:6060/debug/pprof/\n\n// CPU profiling\nimport (\n    \"os\"\n    \"runtime/pprof\"\n)\n\nf, _ := os.Create(\"cpu.prof\")\npprof.StartCPUProfile(f)\ndefer pprof.StopCPUProfile()\n// ... code to profile\n```\n\n## Advanced Debugging Techniques\n\n### Technique 1: Binary Search Debugging\n\n```bash\n# Git bisect for finding regression\ngit bisect start\ngit bisect bad                    # Current commit is bad\ngit bisect good v1.0.0            # v1.0.0 was good\n\n# Git checks out middle commit\n# Test it, then:\ngit bisect good   # if it works\ngit bisect bad    # if it''''s broken\n\n# Continue until bug found\ngit bisect reset  # when done\n```\n\n### Technique 2: Differential Debugging\n\nCompare working vs broken:\n\n```markdown\n## What''''s Different?\n\n| Aspect       | Working         | Broken          |\n|--------------|-----------------|-----------------|\n| Environment  | Development     | Production      |\n| Node version | 18.16.0         | 18.15.0         |\n| Data         | Empty DB        | 1M records      |\n| User         | Admin           | Regular user    |\n| Browser      | Chrome          | Safari          |\n| Time         | During day      | After midnight  |\n\nHypothesis: Time-based issue? Check timezone handling.\n```\n\n### Technique 3: Trace Debugging\n\n```typescript\n// Function call tracing\nfunction trace(target: any, propertyKey: string, descriptor: PropertyDescriptor) {\n    const originalMethod = descriptor.value;\n\n    descriptor.value = function(...args: any[]) {\n        console.log(`Calling ${propertyKey} with args:`, args);\n        const result = originalMethod.apply(this, args);\n        console.log(`${propertyKey} returned:`, result);\n        return result;\n    };\n\n    return descriptor;\n}\n\nclass OrderService {\n    @trace\n    calculateTotal(items: Item[]): number {\n        return items.reduce((sum, item) => sum + item.price, 0);\n    }\n}\n```\n\n### Technique 4: Memory Leak Detection\n\n```typescript\n// Chrome DevTools Memory Profiler\n// 1. Take heap snapshot\n// 2. Perform action\n// 3. Take another snapshot\n// 4. Compare snapshots\n\n// Node.js memory debugging\nif (process.memoryUsage().heapUsed > 500 * 1024 * 1024) {\n    console.warn(''''High memory usage:'''', process.memoryUsage());\n\n    // Generate heap dump\n    require(''''v8'''').writeHeapSnapshot();\n}\n\n// Find memory leaks in tests\nlet beforeMemory: number;\n\nbeforeEach(() => {\n    beforeMemory = process.memoryUsage().heapUsed;\n});\n\nafterEach(() => {\n    const afterMemory = process.memoryUsage().heapUsed;\n    const diff = afterMemory - beforeMemory;\n\n    if (diff > 10 * 1024 * 1024) {  // 10MB threshold\n        console.warn(`Possible memory leak: ${diff / 1024 / 1024}MB`);\n    }\n});\n```\n\n## Debugging Patterns by Issue Type\n\n### Pattern 1: Intermittent Bugs\n\n```markdown\n## Strategies for Flaky Bugs\n\n1. **Add extensive logging**\n   - Log timing information\n   - Log all state transitions\n   - Log external interactions\n\n2. **Look for race conditions**\n   - Concurrent access to shared state\n   - Async operations completing out of order\n   - Missing synchronization\n\n3. **Check timing dependencies**\n   - setTimeout/setInterval\n   - Promise resolution order\n   - Animation frame timing\n\n4. **Stress test**\n   - Run many times\n   - Vary timing\n   - Simulate load\n```\n\n### Pattern 2: Performance Issues\n\n```markdown\n## Performance Debugging\n\n1. **Profile first**\n   - Don''''t optimize blindly\n   - Measure before and after\n   - Find bottlenecks\n\n2. **Common culprits**\n   - N+1 queries\n   - Unnecessary re-renders\n   - Large data processing\n   - Synchronous I/O\n\n3. **Tools**\n   - Browser DevTools Performance tab\n   - Lighthouse\n   - Python: cProfile, line_profiler\n   - Node: clinic.js, 0x\n```\n\n### Pattern 3: Production Bugs\n\n```markdown\n## Production Debugging\n\n1. **Gather evidence**\n   - Error tracking (Sentry, Bugsnag)\n   - Application logs\n   - User reports\n   - Metrics/monitoring\n\n2. **Reproduce locally**\n   - Use production data (anonymized)\n   - Match environment\n   - Follow exact steps\n\n3. **Safe investigation**\n   - Don''''t change production\n   - Use feature flags\n   - Add monitoring/logging\n   - Test fixes in staging\n```\n\n## Best Practices\n\n1. **Reproduce First**: Can''''t fix what you can''''t reproduce\n2. **Isolate the Problem**: Remove complexity until minimal case\n3. **Read Error Messages**: They''''re usually helpful\n4. **Check Recent Changes**: Most bugs are recent\n5. **Use Version Control**: Git bisect, blame, history\n6. **Take Breaks**: Fresh eyes see better\n7. **Document Findings**: Help future you\n8. **Fix Root Cause**: Not just symptoms\n\n## Common Debugging Mistakes\n\n- **Making Multiple Changes**: Change one thing at a time\n- **Not Reading Error Messages**: Read the full stack trace\n- **Assuming It''''s Complex**: Often it''''s simple\n- **Debug Logging in Prod**: Remove before shipping\n- **Not Using Debugger**: console.log isn''''t always best\n- **Giving Up Too Soon**: Persistence pays off\n- **Not Testing the Fix**: Verify it actually works\n\n## Quick Debugging Checklist\n\n```markdown\n## When Stuck, Check:\n\n- [ ] Spelling errors (typos in variable names)\n- [ ] Case sensitivity (fileName vs filename)\n- [ ] Null/undefined values\n- [ ] Array index off-by-one\n- [ ] Async timing (race conditions)\n- [ ] Scope issues (closure, hoisting)\n- [ ] Type mismatches\n- [ ] Missing dependencies\n- [ ] Environment variables\n- [ ] File paths (absolute vs relative)\n- [ ] Cache issues (clear cache)\n- [ ] Stale data (refresh database)\n```\n\n## Resources\n\n- **references/debugging-tools-guide.md**: Comprehensive tool documentation\n- **references/performance-profiling.md**: Performance debugging guide\n- **references/production-debugging.md**: Debugging live systems\n- **assets/debugging-checklist.md**: Quick reference checklist\n- **assets/common-bugs.md**: Common bug patterns\n- **scripts/debug-helper.ts**: Debugging utility functions\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 15, completeness: 15',
  'Score: 100, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-developer-essentials-skills-debugging-strategies-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'k8s-security-policies - Implement Kubernetes security policies including NetworkPolicy, PodSecurityPolicy, and RBAC for production-grade security. Use when securing Kubernete',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Kubernetes Security Policies\n\nComprehensive guide for implementing NetworkPolicy, PodSecurityPolicy, RBAC, and Pod Security Standards in Kubernetes.\n\n## Purpose\n\nImplement defense-in-depth security for Kubernetes clusters using network policies, pod security standards, and RBAC.\n\n## When to Use This Skill\n\n- Implement network segmentation\n- Configure pod security standards\n- Set up RBAC for least-privilege access\n- Create security policies for compliance\n- Implement admission control\n- Secure multi-tenant clusters\n\n## Pod Security Standards\n\n### 1. Privileged (Unrestricted)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: privileged-ns\n  labels:\n    pod-security.kubernetes.io/enforce: privileged\n    pod-security.kubernetes.io/audit: privileged\n    pod-security.kubernetes.io/warn: privileged\n```\n\n### 2. Baseline (Minimally restrictive)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: baseline-ns\n  labels:\n    pod-security.kubernetes.io/enforce: baseline\n    pod-security.kubernetes.io/audit: baseline\n    pod-security.kubernetes.io/warn: baseline\n```\n\n### 3. Restricted (Most restrictive)\n```yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: restricted-ns\n  labels:\n    pod-security.kubernetes.io/enforce: restricted\n    pod-security.kubernetes.io/audit: restricted\n    pod-security.kubernetes.io/warn: restricted\n```\n\n## Network Policies\n\n### Default Deny All\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: default-deny-all\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Ingress\n  - Egress\n```\n\n### Allow Frontend to Backend\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-frontend-to-backend\n  namespace: production\nspec:\n  podSelector:\n    matchLabels:\n      app: backend\n  policyTypes:\n  - Ingress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n### Allow DNS\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-dns\n  namespace: production\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: kube-system\n    ports:\n    - protocol: UDP\n      port: 53\n```\n\n**Reference:** See `assets/network-policy-template.yaml`\n\n## RBAC Configuration\n\n### Role (Namespace-scoped)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  name: pod-reader\n  namespace: production\nrules:\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n### ClusterRole (Cluster-wide)\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: secret-reader\nrules:\n- apiGroups: [\"\"]\n  resources: [\"secrets\"]\n  verbs: [\"get\", \"watch\", \"list\"]\n```\n\n### RoleBinding\n```yaml\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: production\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\n- kind: ServiceAccount\n  name: default\n  namespace: production\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n```\n\n**Reference:** See `references/rbac-patterns.md`\n\n## Pod Security Context\n\n### Restricted Pod\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: secure-pod\nspec:\n  securityContext:\n    runAsNonRoot: true\n    runAsUser: 1000\n    fsGroup: 1000\n    seccompProfile:\n      type: RuntimeDefault\n  containers:\n  - name: app\n    image: myapp:1.0\n    securityContext:\n      allowPrivilegeEscalation: false\n      readOnlyRootFilesystem: true\n      capabilities:\n        drop:\n        - ALL\n```\n\n## Policy Enforcement with OPA Gatekeeper\n\n### ConstraintTemplate\n```yaml\napiVersion: templates.gatekeeper.sh/v1\nkind: ConstraintTemplate\nmetadata:\n  name: k8srequiredlabels\nspec:\n  crd:\n    spec:\n      names:\n        kind: K8sRequiredLabels\n      validation:\n        openAPIV3Schema:\n          type: object\n          properties:\n            labels:\n              type: array\n              items:\n                type: string\n  targets:\n    - target: admission.k8s.gatekeeper.sh\n      rego: |\n        package k8srequiredlabels\n        violation[{\"msg\": msg, \"details\": {\"missing_labels\": missing}}] {\n          provided := {label | input.review.object.metadata.labels[label]}\n          required := {label | label := input.parameters.labels[_]}\n          missing := required - provided\n          count(missing) > 0\n          msg := sprintf(\"missing required labels: %v\", [missing])\n        }\n```\n\n### Constraint\n```yaml\napiVersion: constraints.gatekeeper.sh/v1beta1\nkind: K8sRequiredLabels\nmetadata:\n  name: require-app-label\nspec:\n  match:\n    kinds:\n      - apiGroups: [\"apps\"]\n        kinds: [\"Deployment\"]\n  parameters:\n    labels: [\"app\", \"environment\"]\n```\n\n## Service Mesh Security (Istio)\n\n### PeerAuthentication (mTLS)\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: production\nspec:\n  mtls:\n    mode: STRICT\n```\n\n### AuthorizationPolicy\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: allow-frontend\n  namespace: production\nspec:\n  selector:\n    matchLabels:\n      app: backend\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/production/sa/frontend\"]\n```\n\n## Best Practices\n\n1. **Implement Pod Security Standards** at namespace level\n2. **Use Network Policies** for network segmentation\n3. **Apply least-privilege RBAC** for all service accounts\n4. **Enable admission control** (OPA Gatekeeper/Kyverno)\n5. **Run containers as non-root**\n6. **Use read-only root filesystem**\n7. **Drop all capabilities** unless needed\n8. **Implement resource quotas** and limit ranges\n9. **Enable audit logging** for security events\n10. **Regular security scanning** of images\n\n## Compliance Frameworks\n\n### CIS Kubernetes Benchmark\n- Use RBAC authorization\n- Enable audit logging\n- Use Pod Security Standards\n- Configure network policies\n- Implement secrets encryption at rest\n- Enable node authentication\n\n### NIST Cybersecurity Framework\n- Implement defense in depth\n- Use network segmentation\n- Configure security monitoring\n- Implement access controls\n- Enable logging and monitoring\n\n## Troubleshooting\n\n**NetworkPolicy not working:**\n```bash\n# Check if CNI supports NetworkPolicy\nkubectl get nodes -o wide\nkubectl describe networkpolicy <name>\n```\n\n**RBAC permission denied:**\n```bash\n# Check effective permissions\nkubectl auth can-i list pods --as system:serviceaccount:default:my-sa\nkubectl auth can-i ''''*'''' ''''*'''' --as system:serviceaccount:default:my-sa\n```\n\n## Reference Files\n\n- `assets/network-policy-template.yaml` - Network policy examples\n- `assets/pod-security-template.yaml` - Pod security policies\n- `references/rbac-patterns.md` - RBAC configuration patterns\n\n## Related Skills\n\n- `k8s-manifest-generator` - For creating secure manifests\n- `gitops-workflow` - For automated policy deployment\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 5',
  'Score: 85, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-kubernetes-operations-skills-k8s-security-policies-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'cost-optimization - Optimize cloud costs through resource rightsizing, tagging strategies, reserved instances, and spending analysis. Use when reducing cloud expenses, an',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Cloud Cost Optimization\n\nStrategies and patterns for optimizing cloud costs across AWS, Azure, and GCP.\n\n## Purpose\n\nImplement systematic cost optimization strategies to reduce cloud spending while maintaining performance and reliability.\n\n## When to Use\n\n- Reduce cloud spending\n- Right-size resources\n- Implement cost governance\n- Optimize multi-cloud costs\n- Meet budget constraints\n\n## Cost Optimization Framework\n\n### 1. Visibility\n- Implement cost allocation tags\n- Use cloud cost management tools\n- Set up budget alerts\n- Create cost dashboards\n\n### 2. Right-Sizing\n- Analyze resource utilization\n- Downsize over-provisioned resources\n- Use auto-scaling\n- Remove idle resources\n\n### 3. Pricing Models\n- Use reserved capacity\n- Leverage spot/preemptible instances\n- Implement savings plans\n- Use committed use discounts\n\n### 4. Architecture Optimization\n- Use managed services\n- Implement caching\n- Optimize data transfer\n- Use lifecycle policies\n\n## AWS Cost Optimization\n\n### Reserved Instances\n```\nSavings: 30-72% vs On-Demand\nTerm: 1 or 3 years\nPayment: All/Partial/No upfront\nFlexibility: Standard or Convertible\n```\n\n### Savings Plans\n```\nCompute Savings Plans: 66% savings\nEC2 Instance Savings Plans: 72% savings\nApplies to: EC2, Fargate, Lambda\nFlexible across: Instance families, regions, OS\n```\n\n### Spot Instances\n```\nSavings: Up to 90% vs On-Demand\nBest for: Batch jobs, CI/CD, stateless workloads\nRisk: 2-minute interruption notice\nStrategy: Mix with On-Demand for resilience\n```\n\n### S3 Cost Optimization\n```hcl\nresource \"aws_s3_bucket_lifecycle_configuration\" \"example\" {\n  bucket = aws_s3_bucket.example.id\n\n  rule {\n    id     = \"transition-to-ia\"\n    status = \"Enabled\"\n\n    transition {\n      days          = 30\n      storage_class = \"STANDARD_IA\"\n    }\n\n    transition {\n      days          = 90\n      storage_class = \"GLACIER\"\n    }\n\n    expiration {\n      days = 365\n    }\n  }\n}\n```\n\n## Azure Cost Optimization\n\n### Reserved VM Instances\n- 1 or 3 year terms\n- Up to 72% savings\n- Flexible sizing\n- Exchangeable\n\n### Azure Hybrid Benefit\n- Use existing Windows Server licenses\n- Up to 80% savings with RI\n- Available for Windows and SQL Server\n\n### Azure Advisor Recommendations\n- Right-size VMs\n- Delete unused resources\n- Use reserved capacity\n- Optimize storage\n\n## GCP Cost Optimization\n\n### Committed Use Discounts\n- 1 or 3 year commitment\n- Up to 57% savings\n- Applies to vCPUs and memory\n- Resource-based or spend-based\n\n### Sustained Use Discounts\n- Automatic discounts\n- Up to 30% for running instances\n- No commitment required\n- Applies to Compute Engine, GKE\n\n### Preemptible VMs\n- Up to 80% savings\n- 24-hour maximum runtime\n- Best for batch workloads\n\n## Tagging Strategy\n\n### AWS Tagging\n```hcl\nlocals {\n  common_tags = {\n    Environment = \"production\"\n    Project     = \"my-project\"\n    CostCenter  = \"engineering\"\n    Owner       = \"team@example.com\"\n    ManagedBy   = \"terraform\"\n  }\n}\n\nresource \"aws_instance\" \"example\" {\n  ami           = \"ami-12345678\"\n  instance_type = \"t3.medium\"\n\n  tags = merge(\n    local.common_tags,\n    {\n      Name = \"web-server\"\n    }\n  )\n}\n```\n\n**Reference:** See `references/tagging-standards.md`\n\n## Cost Monitoring\n\n### Budget Alerts\n```hcl\n# AWS Budget\nresource \"aws_budgets_budget\" \"monthly\" {\n  name              = \"monthly-budget\"\n  budget_type       = \"COST\"\n  limit_amount      = \"1000\"\n  limit_unit        = \"USD\"\n  time_period_start = \"2024-01-01_00:00\"\n  time_unit         = \"MONTHLY\"\n\n  notification {\n    comparison_operator        = \"GREATER_THAN\"\n    threshold                  = 80\n    threshold_type            = \"PERCENTAGE\"\n    notification_type         = \"ACTUAL\"\n    subscriber_email_addresses = [\"team@example.com\"]\n  }\n}\n```\n\n### Cost Anomaly Detection\n- AWS Cost Anomaly Detection\n- Azure Cost Management alerts\n- GCP Budget alerts\n\n## Architecture Patterns\n\n### Pattern 1: Serverless First\n- Use Lambda/Functions for event-driven\n- Pay only for execution time\n- Auto-scaling included\n- No idle costs\n\n### Pattern 2: Right-Sized Databases\n```\nDevelopment: t3.small RDS\nStaging: t3.large RDS\nProduction: r6g.2xlarge RDS with read replicas\n```\n\n### Pattern 3: Multi-Tier Storage\n```\nHot data: S3 Standard\nWarm data: S3 Standard-IA (30 days)\nCold data: S3 Glacier (90 days)\nArchive: S3 Deep Archive (365 days)\n```\n\n### Pattern 4: Auto-Scaling\n```hcl\nresource \"aws_autoscaling_policy\" \"scale_up\" {\n  name                   = \"scale-up\"\n  scaling_adjustment     = 2\n  adjustment_type        = \"ChangeInCapacity\"\n  cooldown              = 300\n  autoscaling_group_name = aws_autoscaling_group.main.name\n}\n\nresource \"aws_cloudwatch_metric_alarm\" \"cpu_high\" {\n  alarm_name          = \"cpu-high\"\n  comparison_operator = \"GreaterThanThreshold\"\n  evaluation_periods  = \"2\"\n  metric_name         = \"CPUUtilization\"\n  namespace           = \"AWS/EC2\"\n  period              = \"60\"\n  statistic           = \"Average\"\n  threshold           = \"80\"\n  alarm_actions       = [aws_autoscaling_policy.scale_up.arn]\n}\n```\n\n## Cost Optimization Checklist\n\n- [ ] Implement cost allocation tags\n- [ ] Delete unused resources (EBS, EIPs, snapshots)\n- [ ] Right-size instances based on utilization\n- [ ] Use reserved capacity for steady workloads\n- [ ] Implement auto-scaling\n- [ ] Optimize storage classes\n- [ ] Use lifecycle policies\n- [ ] Enable cost anomaly detection\n- [ ] Set budget alerts\n- [ ] Review costs weekly\n- [ ] Use spot/preemptible instances\n- [ ] Optimize data transfer costs\n- [ ] Implement caching layers\n- [ ] Use managed services\n- [ ] Monitor and optimize continuously\n\n## Tools\n\n- **AWS:** Cost Explorer, Cost Anomaly Detection, Compute Optimizer\n- **Azure:** Cost Management, Advisor\n- **GCP:** Cost Management, Recommender\n- **Multi-cloud:** CloudHealth, Cloudability, Kubecost\n\n## Reference Files\n\n- `references/tagging-standards.md` - Tagging conventions\n- `assets/cost-analysis-template.xlsx` - Cost analysis spreadsheet\n\n## Related Skills\n\n- `terraform-module-library` - For resource provisioning\n- `multi-cloud-architecture` - For cloud selection\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 12, workflow_structure: 10, error_handling: 0',
  'Score: 69, Tier: TIER_2_GOOD, Strengths: example_quality: 17, problem_definition: 15, completeness: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-cloud-infrastructure-skills-cost-optimization-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'react-modernization - Upgrade React applications to latest versions, migrate from class components to hooks, and adopt concurrent features. Use when modernizing React codeb',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# React Modernization\n\nMaster React version upgrades, class to hooks migration, concurrent features adoption, and codemods for automated transformation.\n\n## When to Use This Skill\n\n- Upgrading React applications to latest versions\n- Migrating class components to functional components with hooks\n- Adopting concurrent React features (Suspense, transitions)\n- Applying codemods for automated refactoring\n- Modernizing state management patterns\n- Updating to TypeScript\n- Improving performance with React 18+ features\n\n## Version Upgrade Path\n\n### React 16 \u2192 17 \u2192 18\n\n**Breaking Changes by Version:**\n\n**React 17:**\n- Event delegation changes\n- No event pooling\n- Effect cleanup timing\n- JSX transform (no React import needed)\n\n**React 18:**\n- Automatic batching\n- Concurrent rendering\n- Strict Mode changes (double invocation)\n- New root API\n- Suspense on server\n\n## Class to Hooks Migration\n\n### State Management\n```javascript\n// Before: Class component\nclass Counter extends React.Component {\n  constructor(props) {\n    super(props);\n    this.state = {\n      count: 0,\n      name: ''''''''\n    };\n  }\n\n  increment = () => {\n    this.setState({ count: this.state.count + 1 });\n  }\n\n  render() {\n    return (\n      <div>\n        <p>Count: {this.state.count}</p>\n        <button onClick={this.increment}>Increment</button>\n      </div>\n    );\n  }\n}\n\n// After: Functional component with hooks\nfunction Counter() {\n  const [count, setCount] = useState(0);\n  const [name, setName] = useState('''''''');\n\n  const increment = () => {\n    setCount(count + 1);\n  };\n\n  return (\n    <div>\n      <p>Count: {count}</p>\n      <button onClick={increment}>Increment</button>\n    </div>\n  );\n}\n```\n\n### Lifecycle Methods to Hooks\n```javascript\n// Before: Lifecycle methods\nclass DataFetcher extends React.Component {\n  state = { data: null, loading: true };\n\n  componentDidMount() {\n    this.fetchData();\n  }\n\n  componentDidUpdate(prevProps) {\n    if (prevProps.id !== this.props.id) {\n      this.fetchData();\n    }\n  }\n\n  componentWillUnmount() {\n    this.cancelRequest();\n  }\n\n  fetchData = async () => {\n    const data = await fetch(`/api/${this.props.id}`);\n    this.setState({ data, loading: false });\n  };\n\n  cancelRequest = () => {\n    // Cleanup\n  };\n\n  render() {\n    if (this.state.loading) return <div>Loading...</div>;\n    return <div>{this.state.data}</div>;\n  }\n}\n\n// After: useEffect hook\nfunction DataFetcher({ id }) {\n  const [data, setData] = useState(null);\n  const [loading, setLoading] = useState(true);\n\n  useEffect(() => {\n    let cancelled = false;\n\n    const fetchData = async () => {\n      try {\n        const response = await fetch(`/api/${id}`);\n        const result = await response.json();\n\n        if (!cancelled) {\n          setData(result);\n          setLoading(false);\n        }\n      } catch (error) {\n        if (!cancelled) {\n          console.error(error);\n        }\n      }\n    };\n\n    fetchData();\n\n    // Cleanup function\n    return () => {\n      cancelled = true;\n    };\n  }, [id]); // Re-run when id changes\n\n  if (loading) return <div>Loading...</div>;\n  return <div>{data}</div>;\n}\n```\n\n### Context and HOCs to Hooks\n```javascript\n// Before: Context consumer and HOC\nconst ThemeContext = React.createContext();\n\nclass ThemedButton extends React.Component {\n  static contextType = ThemeContext;\n\n  render() {\n    return (\n      <button style={{ background: this.context.theme }}>\n        {this.props.children}\n      </button>\n    );\n  }\n}\n\n// After: useContext hook\nfunction ThemedButton({ children }) {\n  const { theme } = useContext(ThemeContext);\n\n  return (\n    <button style={{ background: theme }}>\n      {children}\n    </button>\n  );\n}\n\n// Before: HOC for data fetching\nfunction withUser(Component) {\n  return class extends React.Component {\n    state = { user: null };\n\n    componentDidMount() {\n      fetchUser().then(user => this.setState({ user }));\n    }\n\n    render() {\n      return <Component {...this.props} user={this.state.user} />;\n    }\n  };\n}\n\n// After: Custom hook\nfunction useUser() {\n  const [user, setUser] = useState(null);\n\n  useEffect(() => {\n    fetchUser().then(setUser);\n  }, []);\n\n  return user;\n}\n\nfunction UserProfile() {\n  const user = useUser();\n  if (!user) return <div>Loading...</div>;\n  return <div>{user.name}</div>;\n}\n```\n\n## React 18 Concurrent Features\n\n### New Root API\n```javascript\n// Before: React 17\nimport ReactDOM from ''''react-dom'''';\n\nReactDOM.render(<App />, document.getElementById(''''root''''));\n\n// After: React 18\nimport { createRoot } from ''''react-dom/client'''';\n\nconst root = createRoot(document.getElementById(''''root''''));\nroot.render(<App />);\n```\n\n### Automatic Batching\n```javascript\n// React 18: All updates are batched\nfunction handleClick() {\n  setCount(c => c + 1);\n  setFlag(f => !f);\n  // Only one re-render (batched)\n}\n\n// Even in async:\nsetTimeout(() => {\n  setCount(c => c + 1);\n  setFlag(f => !f);\n  // Still batched in React 18!\n}, 1000);\n\n// Opt out if needed\nimport { flushSync } from ''''react-dom'''';\n\nflushSync(() => {\n  setCount(c => c + 1);\n});\n// Re-render happens here\nsetFlag(f => !f);\n// Another re-render\n```\n\n### Transitions\n```javascript\nimport { useState, useTransition } from ''''react'''';\n\nfunction SearchResults() {\n  const [query, setQuery] = useState('''''''');\n  const [results, setResults] = useState([]);\n  const [isPending, startTransition] = useTransition();\n\n  const handleChange = (e) => {\n    // Urgent: Update input immediately\n    setQuery(e.target.value);\n\n    // Non-urgent: Update results (can be interrupted)\n    startTransition(() => {\n      setResults(searchResults(e.target.value));\n    });\n  };\n\n  return (\n    <>\n      <input value={query} onChange={handleChange} />\n      {isPending && <Spinner />}\n      <Results data={results} />\n    </>\n  );\n}\n```\n\n### Suspense for Data Fetching\n```javascript\nimport { Suspense } from ''''react'''';\n\n// Resource-based data fetching (with React 18)\nconst resource = fetchProfileData();\n\nfunction ProfilePage() {\n  return (\n    <Suspense fallback={<Loading />}>\n      <ProfileDetails />\n      <Suspense fallback={<Loading />}>\n        <ProfileTimeline />\n      </Suspense>\n    </Suspense>\n  );\n}\n\nfunction ProfileDetails() {\n  // This will suspend if data not ready\n  const user = resource.user.read();\n  return <h1>{user.name}</h1>;\n}\n\nfunction ProfileTimeline() {\n  const posts = resource.posts.read();\n  return <Timeline posts={posts} />;\n}\n```\n\n## Codemods for Automation\n\n### Run React Codemods\n```bash\n# Install jscodeshift\nnpm install -g jscodeshift\n\n# React 16.9 codemod (rename unsafe lifecycle methods)\nnpx react-codeshift <transform> <path>\n\n# Example: Rename UNSAFE_ methods\nnpx react-codeshift --parser=tsx \\\n  --transform=react-codeshift/transforms/rename-unsafe-lifecycles.js \\\n  src/\n\n# Update to new JSX Transform (React 17+)\nnpx react-codeshift --parser=tsx \\\n  --transform=react-codeshift/transforms/new-jsx-transform.js \\\n  src/\n\n# Class to Hooks (third-party)\nnpx codemod react/hooks/convert-class-to-function src/\n```\n\n### Custom Codemod Example\n```javascript\n// custom-codemod.js\nmodule.exports = function(file, api) {\n  const j = api.jscodeshift;\n  const root = j(file.source);\n\n  // Find setState calls\n  root.find(j.CallExpression, {\n    callee: {\n      type: ''''MemberExpression'''',\n      property: { name: ''''setState'''' }\n    }\n  }).forEach(path => {\n    // Transform to useState\n    // ... transformation logic\n  });\n\n  return root.toSource();\n};\n\n// Run: jscodeshift -t custom-codemod.js src/\n```\n\n## Performance Optimization\n\n### useMemo and useCallback\n```javascript\nfunction ExpensiveComponent({ items, filter }) {\n  // Memoize expensive calculation\n  const filteredItems = useMemo(() => {\n    return items.filter(item => item.category === filter);\n  }, [items, filter]);\n\n  // Memoize callback to prevent child re-renders\n  const handleClick = useCallback((id) => {\n    console.log(''''Clicked:'''', id);\n  }, []); // No dependencies, never changes\n\n  return (\n    <List items={filteredItems} onClick={handleClick} />\n  );\n}\n\n// Child component with memo\nconst List = React.memo(({ items, onClick }) => {\n  return items.map(item => (\n    <Item key={item.id} item={item} onClick={onClick} />\n  ));\n});\n```\n\n### Code Splitting\n```javascript\nimport { lazy, Suspense } from ''''react'''';\n\n// Lazy load components\nconst Dashboard = lazy(() => import(''''./Dashboard''''));\nconst Settings = lazy(() => import(''''./Settings''''));\n\nfunction App() {\n  return (\n    <Suspense fallback={<Loading />}>\n      <Routes>\n        <Route path=\"/dashboard\" element={<Dashboard />} />\n        <Route path=\"/settings\" element={<Settings />} />\n      </Routes>\n    </Suspense>\n  );\n}\n```\n\n## TypeScript Migration\n\n```typescript\n// Before: JavaScript\nfunction Button({ onClick, children }) {\n  return <button onClick={onClick}>{children}</button>;\n}\n\n// After: TypeScript\ninterface ButtonProps {\n  onClick: () => void;\n  children: React.ReactNode;\n}\n\nfunction Button({ onClick, children }: ButtonProps) {\n  return <button onClick={onClick}>{children}</button>;\n}\n\n// Generic components\ninterface ListProps<T> {\n  items: T[];\n  renderItem: (item: T) => React.ReactNode;\n}\n\nfunction List<T>({ items, renderItem }: ListProps<T>) {\n  return <>{items.map(renderItem)}</>;\n}\n```\n\n## Migration Checklist\n\n```markdown\n### Pre-Migration\n- [ ] Update dependencies incrementally (not all at once)\n- [ ] Review breaking changes in release notes\n- [ ] Set up testing suite\n- [ ] Create feature branch\n\n### Class \u2192 Hooks Migration\n- [ ] Identify class components to migrate\n- [ ] Start with leaf components (no children)\n- [ ] Convert state to useState\n- [ ] Convert lifecycle to useEffect\n- [ ] Convert context to useContext\n- [ ] Extract custom hooks\n- [ ] Test thoroughly\n\n### React 18 Upgrade\n- [ ] Update to React 17 first (if needed)\n- [ ] Update react and react-dom to 18\n- [ ] Update @types/react if using TypeScript\n- [ ] Change to createRoot API\n- [ ] Test with StrictMode (double invocation)\n- [ ] Address concurrent rendering issues\n- [ ] Adopt Suspense/Transitions where beneficial\n\n### Performance\n- [ ] Identify performance bottlenecks\n- [ ] Add React.memo where appropriate\n- [ ] Use useMemo/useCallback for expensive operations\n- [ ] Implement code splitting\n- [ ] Optimize re-renders\n\n### Testing\n- [ ] Update test utilities (React Testing Library)\n- [ ] Test with React 18 features\n- [ ] Check for warnings in console\n- [ ] Performance testing\n```\n\n## Resources\n\n- **references/breaking-changes.md**: Version-specific breaking changes\n- **references/codemods.md**: Codemod usage guide\n- **references/hooks-migration.md**: Comprehensive hooks patterns\n- **references/concurrent-features.md**: React 18 concurrent features\n- **assets/codemod-config.json**: Codemod configurations\n- **assets/migration-checklist.md**: Step-by-step checklist\n- **scripts/apply-codemods.sh**: Automated codemod script\n\n## Best Practices\n\n1. **Incremental Migration**: Don''''t migrate everything at once\n2. **Test Thoroughly**: Comprehensive testing at each step\n3. **Use Codemods**: Automate repetitive transformations\n4. **Start Simple**: Begin with leaf components\n5. **Leverage StrictMode**: Catch issues early\n6. **Monitor Performance**: Measure before and after\n7. **Document Changes**: Keep migration log\n\n## Common Pitfalls\n\n- Forgetting useEffect dependencies\n- Over-using useMemo/useCallback\n- Not handling cleanup in useEffect\n- Mixing class and functional patterns\n- Ignoring StrictMode warnings\n- Breaking change assumptions\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, completeness: 15, clarity_structure: 12',
  'Score: 92, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-framework-migration-skills-react-modernization-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'rag-implementation - Build Retrieval-Augmented Generation (RAG) systems for LLM applications with vector databases and semantic search. Use when implementing knowledge-gro',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# RAG Implementation\n\nMaster Retrieval-Augmented Generation (RAG) to build LLM applications that provide accurate, grounded responses using external knowledge sources.\n\n## When to Use This Skill\n\n- Building Q&A systems over proprietary documents\n- Creating chatbots with current, factual information\n- Implementing semantic search with natural language queries\n- Reducing hallucinations with grounded responses\n- Enabling LLMs to access domain-specific knowledge\n- Building documentation assistants\n- Creating research tools with source citation\n\n## Core Components\n\n### 1. Vector Databases\n**Purpose**: Store and retrieve document embeddings efficiently\n\n**Options:**\n- **Pinecone**: Managed, scalable, fast queries\n- **Weaviate**: Open-source, hybrid search\n- **Milvus**: High performance, on-premise\n- **Chroma**: Lightweight, easy to use\n- **Qdrant**: Fast, filtered search\n- **FAISS**: Meta''''s library, local deployment\n\n### 2. Embeddings\n**Purpose**: Convert text to numerical vectors for similarity search\n\n**Models:**\n- **text-embedding-ada-002** (OpenAI): General purpose, 1536 dims\n- **all-MiniLM-L6-v2** (Sentence Transformers): Fast, lightweight\n- **e5-large-v2**: High quality, multilingual\n- **Instructor**: Task-specific instructions\n- **bge-large-en-v1.5**: SOTA performance\n\n### 3. Retrieval Strategies\n**Approaches:**\n- **Dense Retrieval**: Semantic similarity via embeddings\n- **Sparse Retrieval**: Keyword matching (BM25, TF-IDF)\n- **Hybrid Search**: Combine dense + sparse\n- **Multi-Query**: Generate multiple query variations\n- **HyDE**: Generate hypothetical documents\n\n### 4. Reranking\n**Purpose**: Improve retrieval quality by reordering results\n\n**Methods:**\n- **Cross-Encoders**: BERT-based reranking\n- **Cohere Rerank**: API-based reranking\n- **Maximal Marginal Relevance (MMR)**: Diversity + relevance\n- **LLM-based**: Use LLM to score relevance\n\n## Quick Start\n\n```python\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\nfrom langchain.llms import OpenAI\n\n# 1. Load documents\nloader = DirectoryLoader(''''./docs'''', glob=\"**/*.txt\")\ndocuments = loader.load()\n\n# 2. Split into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len\n)\nchunks = text_splitter.split_documents(documents)\n\n# 3. Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvectorstore = Chroma.from_documents(chunks, embeddings)\n\n# 4. Create retrieval chain\nqa_chain = RetrievalQA.from_chain_type(\n    llm=OpenAI(),\n    chain_type=\"stuff\",\n    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 4}),\n    return_source_documents=True\n)\n\n# 5. Query\nresult = qa_chain({\"query\": \"What are the main features?\"})\nprint(result[''''result''''])\nprint(result[''''source_documents''''])\n```\n\n## Advanced RAG Patterns\n\n### Pattern 1: Hybrid Search\n```python\nfrom langchain.retrievers import BM25Retriever, EnsembleRetriever\n\n# Sparse retriever (BM25)\nbm25_retriever = BM25Retriever.from_documents(chunks)\nbm25_retriever.k = 5\n\n# Dense retriever (embeddings)\nembedding_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n\n# Combine with weights\nensemble_retriever = EnsembleRetriever(\n    retrievers=[bm25_retriever, embedding_retriever],\n    weights=[0.3, 0.7]\n)\n```\n\n### Pattern 2: Multi-Query Retrieval\n```python\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\n\n# Generate multiple query perspectives\nretriever = MultiQueryRetriever.from_llm(\n    retriever=vectorstore.as_retriever(),\n    llm=OpenAI()\n)\n\n# Single query \u2192 multiple variations \u2192 combined results\nresults = retriever.get_relevant_documents(\"What is the main topic?\")\n```\n\n### Pattern 3: Contextual Compression\n```python\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMChainExtractor\n\ncompressor = LLMChainExtractor.from_llm(llm)\n\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=vectorstore.as_retriever()\n)\n\n# Returns only relevant parts of documents\ncompressed_docs = compression_retriever.get_relevant_documents(\"query\")\n```\n\n### Pattern 4: Parent Document Retriever\n```python\nfrom langchain.retrievers import ParentDocumentRetriever\nfrom langchain.storage import InMemoryStore\n\n# Store for parent documents\nstore = InMemoryStore()\n\n# Small chunks for retrieval, large chunks for context\nchild_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\nparent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n\nretriever = ParentDocumentRetriever(\n    vectorstore=vectorstore,\n    docstore=store,\n    child_splitter=child_splitter,\n    parent_splitter=parent_splitter\n)\n```\n\n## Document Chunking Strategies\n\n### Recursive Character Text Splitter\n```python\nfrom langchain.text_splitters import RecursiveCharacterTextSplitter\n\nsplitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,\n    length_function=len,\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]  # Try these in order\n)\n```\n\n### Token-Based Splitting\n```python\nfrom langchain.text_splitters import TokenTextSplitter\n\nsplitter = TokenTextSplitter(\n    chunk_size=512,\n    chunk_overlap=50\n)\n```\n\n### Semantic Chunking\n```python\nfrom langchain.text_splitters import SemanticChunker\n\nsplitter = SemanticChunker(\n    embeddings=OpenAIEmbeddings(),\n    breakpoint_threshold_type=\"percentile\"\n)\n```\n\n### Markdown Header Splitter\n```python\nfrom langchain.text_splitters import MarkdownHeaderTextSplitter\n\nheaders_to_split_on = [\n    (\"#\", \"Header 1\"),\n    (\"##\", \"Header 2\"),\n    (\"###\", \"Header 3\"),\n]\n\nsplitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n```\n\n## Vector Store Configurations\n\n### Pinecone\n```python\nimport pinecone\nfrom langchain.vectorstores import Pinecone\n\npinecone.init(api_key=\"your-api-key\", environment=\"us-west1-gcp\")\n\nindex = pinecone.Index(\"your-index-name\")\n\nvectorstore = Pinecone(index, embeddings.embed_query, \"text\")\n```\n\n### Weaviate\n```python\nimport weaviate\nfrom langchain.vectorstores import Weaviate\n\nclient = weaviate.Client(\"http://localhost:8080\")\n\nvectorstore = Weaviate(client, \"Document\", \"content\", embeddings)\n```\n\n### Chroma (Local)\n```python\nfrom langchain.vectorstores import Chroma\n\nvectorstore = Chroma(\n    collection_name=\"my_collection\",\n    embedding_function=embeddings,\n    persist_directory=\"./chroma_db\"\n)\n```\n\n## Retrieval Optimization\n\n### 1. Metadata Filtering\n```python\n# Add metadata during indexing\nchunks_with_metadata = []\nfor i, chunk in enumerate(chunks):\n    chunk.metadata = {\n        \"source\": chunk.metadata.get(\"source\"),\n        \"page\": i,\n        \"category\": determine_category(chunk.page_content)\n    }\n    chunks_with_metadata.append(chunk)\n\n# Filter during retrieval\nresults = vectorstore.similarity_search(\n    \"query\",\n    filter={\"category\": \"technical\"},\n    k=5\n)\n```\n\n### 2. Maximal Marginal Relevance\n```python\n# Balance relevance with diversity\nresults = vectorstore.max_marginal_relevance_search(\n    \"query\",\n    k=5,\n    fetch_k=20,  # Fetch 20, return top 5 diverse\n    lambda_mult=0.5  # 0=max diversity, 1=max relevance\n)\n```\n\n### 3. Reranking with Cross-Encoder\n```python\nfrom sentence_transformers import CrossEncoder\n\nreranker = CrossEncoder(''''cross-encoder/ms-marco-MiniLM-L-6-v2'''')\n\n# Get initial results\ncandidates = vectorstore.similarity_search(\"query\", k=20)\n\n# Rerank\npairs = [[query, doc.page_content] for doc in candidates]\nscores = reranker.predict(pairs)\n\n# Sort by score and take top k\nreranked = sorted(zip(candidates, scores), key=lambda x: x[1], reverse=True)[:5]\n```\n\n## Prompt Engineering for RAG\n\n### Contextual Prompt\n```python\nprompt_template = \"\"\"Use the following context to answer the question. If you cannot answer based on the context, say \"I don''''t have enough information.\"\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\"\"\"\n```\n\n### With Citations\n```python\nprompt_template = \"\"\"Answer the question based on the context below. Include citations using [1], [2], etc.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer (with citations):\"\"\"\n```\n\n### With Confidence\n```python\nprompt_template = \"\"\"Answer the question using the context. Provide a confidence score (0-100%) for your answer.\n\nContext:\n{context}\n\nQuestion: {question}\n\nAnswer:\nConfidence:\"\"\"\n```\n\n## Evaluation Metrics\n\n```python\ndef evaluate_rag_system(qa_chain, test_cases):\n    metrics = {\n        ''''accuracy'''': [],\n        ''''retrieval_quality'''': [],\n        ''''groundedness'''': []\n    }\n\n    for test in test_cases:\n        result = qa_chain({\"query\": test[''''question'''']})\n\n        # Check if answer matches expected\n        accuracy = calculate_accuracy(result[''''result''''], test[''''expected''''])\n        metrics[''''accuracy''''].append(accuracy)\n\n        # Check if relevant docs were retrieved\n        retrieval_quality = evaluate_retrieved_docs(\n            result[''''source_documents''''],\n            test[''''relevant_docs'''']\n        )\n        metrics[''''retrieval_quality''''].append(retrieval_quality)\n\n        # Check if answer is grounded in context\n        groundedness = check_groundedness(\n            result[''''result''''],\n            result[''''source_documents'''']\n        )\n        metrics[''''groundedness''''].append(groundedness)\n\n    return {k: sum(v)/len(v) for k, v in metrics.items()}\n```\n\n## Resources\n\n- **references/vector-databases.md**: Detailed comparison of vector DBs\n- **references/embeddings.md**: Embedding model selection guide\n- **references/retrieval-strategies.md**: Advanced retrieval techniques\n- **references/reranking.md**: Reranking methods and when to use them\n- **references/context-window.md**: Managing context limits\n- **assets/vector-store-config.yaml**: Configuration templates\n- **assets/retriever-pipeline.py**: Complete RAG pipeline\n- **assets/embedding-models.md**: Model comparison and benchmarks\n\n## Best Practices\n\n1. **Chunk Size**: Balance between context and specificity (500-1000 tokens)\n2. **Overlap**: Use 10-20% overlap to preserve context at boundaries\n3. **Metadata**: Include source, page, timestamp for filtering and debugging\n4. **Hybrid Search**: Combine semantic and keyword search for best results\n5. **Reranking**: Improve top results with cross-encoder\n6. **Citations**: Always return source documents for transparency\n7. **Evaluation**: Continuously test retrieval quality and answer accuracy\n8. **Monitoring**: Track retrieval metrics in production\n\n## Common Issues\n\n- **Poor Retrieval**: Check embedding quality, chunk size, query formulation\n- **Irrelevant Results**: Add metadata filtering, use hybrid search, rerank\n- **Missing Information**: Ensure documents are properly indexed\n- **Slow Queries**: Optimize vector store, use caching, reduce k\n- **Hallucinations**: Improve grounding prompt, add verification step\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 12, completeness: 10, error_handling: 6',
  'Score: 78, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-llm-application-dev-skills-rag-implementation-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'code-review-excellence - Master effective code review practices to provide constructive feedback, catch bugs early, and foster knowledge sharing while maintaining team morale.',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Code Review Excellence\n\nTransform code reviews from gatekeeping to knowledge sharing through constructive feedback, systematic analysis, and collaborative improvement.\n\n## When to Use This Skill\n\n- Reviewing pull requests and code changes\n- Establishing code review standards for teams\n- Mentoring junior developers through reviews\n- Conducting architecture reviews\n- Creating review checklists and guidelines\n- Improving team collaboration\n- Reducing code review cycle time\n- Maintaining code quality standards\n\n## Core Principles\n\n### 1. The Review Mindset\n\n**Goals of Code Review:**\n- Catch bugs and edge cases\n- Ensure code maintainability\n- Share knowledge across team\n- Enforce coding standards\n- Improve design and architecture\n- Build team culture\n\n**Not the Goals:**\n- Show off knowledge\n- Nitpick formatting (use linters)\n- Block progress unnecessarily\n- Rewrite to your preference\n\n### 2. Effective Feedback\n\n**Good Feedback is:**\n- Specific and actionable\n- Educational, not judgmental\n- Focused on the code, not the person\n- Balanced (praise good work too)\n- Prioritized (critical vs nice-to-have)\n\n```markdown\n\u274c Bad: \"This is wrong.\"\n\u2705 Good: \"This could cause a race condition when multiple users\n         access simultaneously. Consider using a mutex here.\"\n\n\u274c Bad: \"Why didn''''t you use X pattern?\"\n\u2705 Good: \"Have you considered the Repository pattern? It would\n         make this easier to test. Here''''s an example: [link]\"\n\n\u274c Bad: \"Rename this variable.\"\n\u2705 Good: \"[nit] Consider `userCount` instead of `uc` for\n         clarity. Not blocking if you prefer to keep it.\"\n```\n\n### 3. Review Scope\n\n**What to Review:**\n- Logic correctness and edge cases\n- Security vulnerabilities\n- Performance implications\n- Test coverage and quality\n- Error handling\n- Documentation and comments\n- API design and naming\n- Architectural fit\n\n**What Not to Review Manually:**\n- Code formatting (use Prettier, Black, etc.)\n- Import organization\n- Linting violations\n- Simple typos\n\n## Review Process\n\n### Phase 1: Context Gathering (2-3 minutes)\n\n```markdown\nBefore diving into code, understand:\n\n1. Read PR description and linked issue\n2. Check PR size (>400 lines? Ask to split)\n3. Review CI/CD status (tests passing?)\n4. Understand the business requirement\n5. Note any relevant architectural decisions\n```\n\n### Phase 2: High-Level Review (5-10 minutes)\n\n```markdown\n1. **Architecture & Design**\n   - Does the solution fit the problem?\n   - Are there simpler approaches?\n   - Is it consistent with existing patterns?\n   - Will it scale?\n\n2. **File Organization**\n   - Are new files in the right places?\n   - Is code grouped logically?\n   - Are there duplicate files?\n\n3. **Testing Strategy**\n   - Are there tests?\n   - Do tests cover edge cases?\n   - Are tests readable?\n```\n\n### Phase 3: Line-by-Line Review (10-20 minutes)\n\n```markdown\nFor each file:\n\n1. **Logic & Correctness**\n   - Edge cases handled?\n   - Off-by-one errors?\n   - Null/undefined checks?\n   - Race conditions?\n\n2. **Security**\n   - Input validation?\n   - SQL injection risks?\n   - XSS vulnerabilities?\n   - Sensitive data exposure?\n\n3. **Performance**\n   - N+1 queries?\n   - Unnecessary loops?\n   - Memory leaks?\n   - Blocking operations?\n\n4. **Maintainability**\n   - Clear variable names?\n   - Functions doing one thing?\n   - Complex code commented?\n   - Magic numbers extracted?\n```\n\n### Phase 4: Summary & Decision (2-3 minutes)\n\n```markdown\n1. Summarize key concerns\n2. Highlight what you liked\n3. Make clear decision:\n   - \u2705 Approve\n   - \ud83d\udcac Comment (minor suggestions)\n   - \ud83d\udd04 Request Changes (must address)\n4. Offer to pair if complex\n```\n\n## Review Techniques\n\n### Technique 1: The Checklist Method\n\n```markdown\n## Security Checklist\n- [ ] User input validated and sanitized\n- [ ] SQL queries use parameterization\n- [ ] Authentication/authorization checked\n- [ ] Secrets not hardcoded\n- [ ] Error messages don''''t leak info\n\n## Performance Checklist\n- [ ] No N+1 queries\n- [ ] Database queries indexed\n- [ ] Large lists paginated\n- [ ] Expensive operations cached\n- [ ] No blocking I/O in hot paths\n\n## Testing Checklist\n- [ ] Happy path tested\n- [ ] Edge cases covered\n- [ ] Error cases tested\n- [ ] Test names are descriptive\n- [ ] Tests are deterministic\n```\n\n### Technique 2: The Question Approach\n\nInstead of stating problems, ask questions to encourage thinking:\n\n```markdown\n\u274c \"This will fail if the list is empty.\"\n\u2705 \"What happens if `items` is an empty array?\"\n\n\u274c \"You need error handling here.\"\n\u2705 \"How should this behave if the API call fails?\"\n\n\u274c \"This is inefficient.\"\n\u2705 \"I see this loops through all users. Have we considered\n    the performance impact with 100k users?\"\n```\n\n### Technique 3: Suggest, Don''''t Command\n\n```markdown\n## Use Collaborative Language\n\n\u274c \"You must change this to use async/await\"\n\u2705 \"Suggestion: async/await might make this more readable:\n    ```typescript\n    async function fetchUser(id: string) {\n        const user = await db.query(''''SELECT * FROM users WHERE id = ?'''', id);\n        return user;\n    }\n    ```\n    What do you think?\"\n\n\u274c \"Extract this into a function\"\n\u2705 \"This logic appears in 3 places. Would it make sense to\n    extract it into a shared utility function?\"\n```\n\n### Technique 4: Differentiate Severity\n\n```markdown\nUse labels to indicate priority:\n\n\ud83d\udd34 [blocking] - Must fix before merge\n\ud83d\udfe1 [important] - Should fix, discuss if disagree\n\ud83d\udfe2 [nit] - Nice to have, not blocking\n\ud83d\udca1 [suggestion] - Alternative approach to consider\n\ud83d\udcda [learning] - Educational comment, no action needed\n\ud83c\udf89 [praise] - Good work, keep it up!\n\nExample:\n\"\ud83d\udd34 [blocking] This SQL query is vulnerable to injection.\n Please use parameterized queries.\"\n\n\"\ud83d\udfe2 [nit] Consider renaming `data` to `userData` for clarity.\"\n\n\"\ud83c\udf89 [praise] Excellent test coverage! This will catch edge cases.\"\n```\n\n## Language-Specific Patterns\n\n### Python Code Review\n\n```python\n# Check for Python-specific issues\n\n# \u274c Mutable default arguments\ndef add_item(item, items=[]):  # Bug! Shared across calls\n    items.append(item)\n    return items\n\n# \u2705 Use None as default\ndef add_item(item, items=None):\n    if items is None:\n        items = []\n    items.append(item)\n    return items\n\n# \u274c Catching too broad\ntry:\n    result = risky_operation()\nexcept:  # Catches everything, even KeyboardInterrupt!\n    pass\n\n# \u2705 Catch specific exceptions\ntry:\n    result = risky_operation()\nexcept ValueError as e:\n    logger.error(f\"Invalid value: {e}\")\n    raise\n\n# \u274c Using mutable class attributes\nclass User:\n    permissions = []  # Shared across all instances!\n\n# \u2705 Initialize in __init__\nclass User:\n    def __init__(self):\n        self.permissions = []\n```\n\n### TypeScript/JavaScript Code Review\n\n```typescript\n// Check for TypeScript-specific issues\n\n// \u274c Using any defeats type safety\nfunction processData(data: any) {  // Avoid any\n    return data.value;\n}\n\n// \u2705 Use proper types\ninterface DataPayload {\n    value: string;\n}\nfunction processData(data: DataPayload) {\n    return data.value;\n}\n\n// \u274c Not handling async errors\nasync function fetchUser(id: string) {\n    const response = await fetch(`/api/users/${id}`);\n    return response.json();  // What if network fails?\n}\n\n// \u2705 Handle errors properly\nasync function fetchUser(id: string): Promise<User> {\n    try {\n        const response = await fetch(`/api/users/${id}`);\n        if (!response.ok) {\n            throw new Error(`HTTP ${response.status}`);\n        }\n        return await response.json();\n    } catch (error) {\n        console.error(''''Failed to fetch user:'''', error);\n        throw error;\n    }\n}\n\n// \u274c Mutation of props\nfunction UserProfile({ user }: Props) {\n    user.lastViewed = new Date();  // Mutating prop!\n    return <div>{user.name}</div>;\n}\n\n// \u2705 Don''''t mutate props\nfunction UserProfile({ user, onView }: Props) {\n    useEffect(() => {\n        onView(user.id);  // Notify parent to update\n    }, [user.id]);\n    return <div>{user.name}</div>;\n}\n```\n\n## Advanced Review Patterns\n\n### Pattern 1: Architectural Review\n\n```markdown\nWhen reviewing significant changes:\n\n1. **Design Document First**\n   - For large features, request design doc before code\n   - Review design with team before implementation\n   - Agree on approach to avoid rework\n\n2. **Review in Stages**\n   - First PR: Core abstractions and interfaces\n   - Second PR: Implementation\n   - Third PR: Integration and tests\n   - Easier to review, faster to iterate\n\n3. **Consider Alternatives**\n   - \"Have we considered using [pattern/library]?\"\n   - \"What''''s the tradeoff vs. the simpler approach?\"\n   - \"How will this evolve as requirements change?\"\n```\n\n### Pattern 2: Test Quality Review\n\n```typescript\n// \u274c Poor test: Implementation detail testing\ntest(''''increments counter variable'''', () => {\n    const component = render(<Counter />);\n    const button = component.getByRole(''''button'''');\n    fireEvent.click(button);\n    expect(component.state.counter).toBe(1);  // Testing internal state\n});\n\n// \u2705 Good test: Behavior testing\ntest(''''displays incremented count when clicked'''', () => {\n    render(<Counter />);\n    const button = screen.getByRole(''''button'''', { name: /increment/i });\n    fireEvent.click(button);\n    expect(screen.getByText(''''Count: 1'''')).toBeInTheDocument();\n});\n\n// Review questions for tests:\n// - Do tests describe behavior, not implementation?\n// - Are test names clear and descriptive?\n// - Do tests cover edge cases?\n// - Are tests independent (no shared state)?\n// - Can tests run in any order?\n```\n\n### Pattern 3: Security Review\n\n```markdown\n## Security Review Checklist\n\n### Authentication & Authorization\n- [ ] Is authentication required where needed?\n- [ ] Are authorization checks before every action?\n- [ ] Is JWT validation proper (signature, expiry)?\n- [ ] Are API keys/secrets properly secured?\n\n### Input Validation\n- [ ] All user inputs validated?\n- [ ] File uploads restricted (size, type)?\n- [ ] SQL queries parameterized?\n- [ ] XSS protection (escape output)?\n\n### Data Protection\n- [ ] Passwords hashed (bcrypt/argon2)?\n- [ ] Sensitive data encrypted at rest?\n- [ ] HTTPS enforced for sensitive data?\n- [ ] PII handled according to regulations?\n\n### Common Vulnerabilities\n- [ ] No eval() or similar dynamic execution?\n- [ ] No hardcoded secrets?\n- [ ] CSRF protection for state-changing operations?\n- [ ] Rate limiting on public endpoints?\n```\n\n## Giving Difficult Feedback\n\n### Pattern: The Sandwich Method (Modified)\n\n```markdown\nTraditional: Praise + Criticism + Praise (feels fake)\n\nBetter: Context + Specific Issue + Helpful Solution\n\nExample:\n\"I noticed the payment processing logic is inline in the\ncontroller. This makes it harder to test and reuse.\n\n[Specific Issue]\nThe calculateTotal() function mixes tax calculation,\ndiscount logic, and database queries, making it difficult\nto unit test and reason about.\n\n[Helpful Solution]\nCould we extract this into a PaymentService class? That\nwould make it testable and reusable. I can pair with you\non this if helpful.\"\n```\n\n### Handling Disagreements\n\n```markdown\nWhen author disagrees with your feedback:\n\n1. **Seek to Understand**\n   \"Help me understand your approach. What led you to\n    choose this pattern?\"\n\n2. **Acknowledge Valid Points**\n   \"That''''s a good point about X. I hadn''''t considered that.\"\n\n3. **Provide Data**\n   \"I''''m concerned about performance. Can we add a benchmark\n    to validate the approach?\"\n\n4. **Escalate if Needed**\n   \"Let''''s get [architect/senior dev] to weigh in on this.\"\n\n5. **Know When to Let Go**\n   If it''''s working and not a critical issue, approve it.\n   Perfection is the enemy of progress.\n```\n\n## Best Practices\n\n1. **Review Promptly**: Within 24 hours, ideally same day\n2. **Limit PR Size**: 200-400 lines max for effective review\n3. **Review in Time Blocks**: 60 minutes max, take breaks\n4. **Use Review Tools**: GitHub, GitLab, or dedicated tools\n5. **Automate What You Can**: Linters, formatters, security scans\n6. **Build Rapport**: Emoji, praise, and empathy matter\n7. **Be Available**: Offer to pair on complex issues\n8. **Learn from Others**: Review others'''' review comments\n\n## Common Pitfalls\n\n- **Perfectionism**: Blocking PRs for minor style preferences\n- **Scope Creep**: \"While you''''re at it, can you also...\"\n- **Inconsistency**: Different standards for different people\n- **Delayed Reviews**: Letting PRs sit for days\n- **Ghosting**: Requesting changes then disappearing\n- **Rubber Stamping**: Approving without actually reviewing\n- **Bike Shedding**: Debating trivial details extensively\n\n## Templates\n\n### PR Review Comment Template\n\n```markdown\n## Summary\n[Brief overview of what was reviewed]\n\n## Strengths\n- [What was done well]\n- [Good patterns or approaches]\n\n## Required Changes\n\ud83d\udd34 [Blocking issue 1]\n\ud83d\udd34 [Blocking issue 2]\n\n## Suggestions\n\ud83d\udca1 [Improvement 1]\n\ud83d\udca1 [Improvement 2]\n\n## Questions\n\u2753 [Clarification needed on X]\n\u2753 [Alternative approach consideration]\n\n## Verdict\n\u2705 Approve after addressing required changes\n```\n\n## Resources\n\n- **references/code-review-best-practices.md**: Comprehensive review guidelines\n- **references/common-bugs-checklist.md**: Language-specific bugs to watch for\n- **references/security-review-guide.md**: Security-focused review checklist\n- **assets/pr-review-template.md**: Standard review comment template\n- **assets/review-checklist.md**: Quick reference checklist\n- **scripts/pr-analyzer.py**: Analyze PR complexity and suggest reviewers\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 12, completeness: 10',
  'Score: 92, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 20, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-developer-essentials-skills-code-review-excellence-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'github-actions-templates - Create production-ready GitHub Actions workflows for automated testing, building, and deploying applications. Use when setting up CI/CD with GitHub Ac',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# GitHub Actions Templates\n\nProduction-ready GitHub Actions workflow patterns for testing, building, and deploying applications.\n\n## Purpose\n\nCreate efficient, secure GitHub Actions workflows for continuous integration and deployment across various tech stacks.\n\n## When to Use\n\n- Automate testing and deployment\n- Build Docker images and push to registries\n- Deploy to Kubernetes clusters\n- Run security scans\n- Implement matrix builds for multiple environments\n\n## Common Workflow Patterns\n\n### Pattern 1: Test Workflow\n\n```yaml\nname: Test\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n\n    strategy:\n      matrix:\n        node-version: [18.x, 20.x]\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Use Node.js ${{ matrix.node-version }}\n      uses: actions/setup-node@v4\n      with:\n        node-version: ${{ matrix.node-version }}\n        cache: ''''npm''''\n\n    - name: Install dependencies\n      run: npm ci\n\n    - name: Run linter\n      run: npm run lint\n\n    - name: Run tests\n      run: npm test\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        files: ./coverage/lcov.info\n```\n\n**Reference:** See `assets/test-workflow.yml`\n\n### Pattern 2: Build and Push Docker Image\n\n```yaml\nname: Build and Push\n\non:\n  push:\n    branches: [ main ]\n    tags: [ ''''v*'''' ]\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Log in to Container Registry\n      uses: docker/login-action@v3\n      with:\n        registry: ${{ env.REGISTRY }}\n        username: ${{ github.actor }}\n        password: ${{ secrets.GITHUB_TOKEN }}\n\n    - name: Extract metadata\n      id: meta\n      uses: docker/metadata-action@v5\n      with:\n        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n        tags: |\n          type=ref,event=branch\n          type=ref,event=pr\n          type=semver,pattern={{version}}\n          type=semver,pattern={{major}}.{{minor}}\n\n    - name: Build and push\n      uses: docker/build-push-action@v5\n      with:\n        context: .\n        push: true\n        tags: ${{ steps.meta.outputs.tags }}\n        labels: ${{ steps.meta.outputs.labels }}\n        cache-from: type=gha\n        cache-to: type=gha,mode=max\n```\n\n**Reference:** See `assets/deploy-workflow.yml`\n\n### Pattern 3: Deploy to Kubernetes\n\n```yaml\nname: Deploy to Kubernetes\n\non:\n  push:\n    branches: [ main ]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Configure AWS credentials\n      uses: aws-actions/configure-aws-credentials@v4\n      with:\n        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}\n        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n        aws-region: us-west-2\n\n    - name: Update kubeconfig\n      run: |\n        aws eks update-kubeconfig --name production-cluster --region us-west-2\n\n    - name: Deploy to Kubernetes\n      run: |\n        kubectl apply -f k8s/\n        kubectl rollout status deployment/my-app -n production\n        kubectl get services -n production\n\n    - name: Verify deployment\n      run: |\n        kubectl get pods -n production\n        kubectl describe deployment my-app -n production\n```\n\n### Pattern 4: Matrix Build\n\n```yaml\nname: Matrix Build\n\non: [push, pull_request]\n\njobs:\n  build:\n    runs-on: ${{ matrix.os }}\n\n    strategy:\n      matrix:\n        os: [ubuntu-latest, macos-latest, windows-latest]\n        python-version: [''''3.9'''', ''''3.10'''', ''''3.11'''', ''''3.12'''']\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Set up Python\n      uses: actions/setup-python@v5\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install dependencies\n      run: |\n        python -m pip install --upgrade pip\n        pip install -r requirements.txt\n\n    - name: Run tests\n      run: pytest\n```\n\n**Reference:** See `assets/matrix-build.yml`\n\n## Workflow Best Practices\n\n1. **Use specific action versions** (@v4, not @latest)\n2. **Cache dependencies** to speed up builds\n3. **Use secrets** for sensitive data\n4. **Implement status checks** on PRs\n5. **Use matrix builds** for multi-version testing\n6. **Set appropriate permissions**\n7. **Use reusable workflows** for common patterns\n8. **Implement approval gates** for production\n9. **Add notification steps** for failures\n10. **Use self-hosted runners** for sensitive workloads\n\n## Reusable Workflows\n\n```yaml\n# .github/workflows/reusable-test.yml\nname: Reusable Test Workflow\n\non:\n  workflow_call:\n    inputs:\n      node-version:\n        required: true\n        type: string\n    secrets:\n      NPM_TOKEN:\n        required: true\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n    - uses: actions/checkout@v4\n    - uses: actions/setup-node@v4\n      with:\n        node-version: ${{ inputs.node-version }}\n    - run: npm ci\n    - run: npm test\n```\n\n**Use reusable workflow:**\n```yaml\njobs:\n  call-test:\n    uses: ./.github/workflows/reusable-test.yml\n    with:\n      node-version: ''''20.x''''\n    secrets:\n      NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n```\n\n## Security Scanning\n\n```yaml\nname: Security Scan\n\non:\n  push:\n    branches: [ main ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  security:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Run Trivy vulnerability scanner\n      uses: aquasecurity/trivy-action@master\n      with:\n        scan-type: ''''fs''''\n        scan-ref: ''''.''''\n        format: ''''sarif''''\n        output: ''''trivy-results.sarif''''\n\n    - name: Upload Trivy results to GitHub Security\n      uses: github/codeql-action/upload-sarif@v2\n      with:\n        sarif_file: ''''trivy-results.sarif''''\n\n    - name: Run Snyk Security Scan\n      uses: snyk/actions/node@master\n      env:\n        SNYK_TOKEN: ${{ secrets.SNYK_TOKEN }}\n```\n\n## Deployment with Approvals\n\n```yaml\nname: Deploy to Production\n\non:\n  push:\n    tags: [ ''''v*'''' ]\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    environment:\n      name: production\n      url: https://app.example.com\n\n    steps:\n    - uses: actions/checkout@v4\n\n    - name: Deploy application\n      run: |\n        echo \"Deploying to production...\"\n        # Deployment commands here\n\n    - name: Notify Slack\n      if: success()\n      uses: slackapi/slack-github-action@v1\n      with:\n        webhook-url: ${{ secrets.SLACK_WEBHOOK }}\n        payload: |\n          {\n            \"text\": \"Deployment to production completed successfully!\"\n          }\n```\n\n## Reference Files\n\n- `assets/test-workflow.yml` - Testing workflow template\n- `assets/deploy-workflow.yml` - Deployment workflow template\n- `assets/matrix-build.yml` - Matrix build template\n- `references/common-workflows.md` - Common workflow patterns\n\n## Related Skills\n\n- `gitlab-ci-patterns` - For GitLab CI workflows\n- `deployment-pipeline-design` - For pipeline architecture\n- `secrets-management` - For secrets handling\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 10, error_handling: 6',
  'Score: 84, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 18, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-cicd-automation-skills-github-actions-templates-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'fastapi-templates - Create production-ready FastAPI projects with async patterns, dependency injection, and comprehensive error handling. Use when building new FastAPI ap',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# FastAPI Project Templates\n\nProduction-ready FastAPI project structures with async patterns, dependency injection, middleware, and best practices for building high-performance APIs.\n\n## When to Use This Skill\n\n- Starting new FastAPI projects from scratch\n- Implementing async REST APIs with Python\n- Building high-performance web services and microservices\n- Creating async applications with PostgreSQL, MongoDB\n- Setting up API projects with proper structure and testing\n\n## Core Concepts\n\n### 1. Project Structure\n\n**Recommended Layout:**\n```\napp/\n\u251c\u2500\u2500 api/                    # API routes\n\u2502   \u251c\u2500\u2500 v1/\n\u2502   \u2502   \u251c\u2500\u2500 endpoints/\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 users.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 auth.py\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 items.py\n\u2502   \u2502   \u2514\u2500\u2500 router.py\n\u2502   \u2514\u2500\u2500 dependencies.py     # Shared dependencies\n\u251c\u2500\u2500 core/                   # Core configuration\n\u2502   \u251c\u2500\u2500 config.py\n\u2502   \u251c\u2500\u2500 security.py\n\u2502   \u2514\u2500\u2500 database.py\n\u251c\u2500\u2500 models/                 # Database models\n\u2502   \u251c\u2500\u2500 user.py\n\u2502   \u2514\u2500\u2500 item.py\n\u251c\u2500\u2500 schemas/                # Pydantic schemas\n\u2502   \u251c\u2500\u2500 user.py\n\u2502   \u2514\u2500\u2500 item.py\n\u251c\u2500\u2500 services/               # Business logic\n\u2502   \u251c\u2500\u2500 user_service.py\n\u2502   \u2514\u2500\u2500 auth_service.py\n\u251c\u2500\u2500 repositories/           # Data access\n\u2502   \u251c\u2500\u2500 user_repository.py\n\u2502   \u2514\u2500\u2500 item_repository.py\n\u2514\u2500\u2500 main.py                 # Application entry\n```\n\n### 2. Dependency Injection\n\nFastAPI''''s built-in DI system using `Depends`:\n- Database session management\n- Authentication/authorization\n- Shared business logic\n- Configuration injection\n\n### 3. Async Patterns\n\nProper async/await usage:\n- Async route handlers\n- Async database operations\n- Async background tasks\n- Async middleware\n\n## Implementation Patterns\n\n### Pattern 1: Complete FastAPI Application\n\n```python\n# main.py\nfrom fastapi import FastAPI, Depends\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom contextlib import asynccontextmanager\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Application lifespan events.\"\"\"\n    # Startup\n    await database.connect()\n    yield\n    # Shutdown\n    await database.disconnect()\n\napp = FastAPI(\n    title=\"API Template\",\n    version=\"1.0.0\",\n    lifespan=lifespan\n)\n\n# CORS middleware\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n# Include routers\nfrom app.api.v1.router import api_router\napp.include_router(api_router, prefix=\"/api/v1\")\n\n# core/config.py\nfrom pydantic_settings import BaseSettings\nfrom functools import lru_cache\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings.\"\"\"\n    DATABASE_URL: str\n    SECRET_KEY: str\n    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30\n    API_V1_STR: str = \"/api/v1\"\n\n    class Config:\n        env_file = \".env\"\n\n@lru_cache()\ndef get_settings() -> Settings:\n    return Settings()\n\n# core/database.py\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import sessionmaker\nfrom app.core.config import get_settings\n\nsettings = get_settings()\n\nengine = create_async_engine(\n    settings.DATABASE_URL,\n    echo=True,\n    future=True\n)\n\nAsyncSessionLocal = sessionmaker(\n    engine,\n    class_=AsyncSession,\n    expire_on_commit=False\n)\n\nBase = declarative_base()\n\nasync def get_db() -> AsyncSession:\n    \"\"\"Dependency for database session.\"\"\"\n    async with AsyncSessionLocal() as session:\n        try:\n            yield session\n            await session.commit()\n        except Exception:\n            await session.rollback()\n            raise\n        finally:\n            await session.close()\n```\n\n### Pattern 2: CRUD Repository Pattern\n\n```python\n# repositories/base_repository.py\nfrom typing import Generic, TypeVar, Type, Optional, List\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom sqlalchemy import select\nfrom pydantic import BaseModel\n\nModelType = TypeVar(\"ModelType\")\nCreateSchemaType = TypeVar(\"CreateSchemaType\", bound=BaseModel)\nUpdateSchemaType = TypeVar(\"UpdateSchemaType\", bound=BaseModel)\n\nclass BaseRepository(Generic[ModelType, CreateSchemaType, UpdateSchemaType]):\n    \"\"\"Base repository for CRUD operations.\"\"\"\n\n    def __init__(self, model: Type[ModelType]):\n        self.model = model\n\n    async def get(self, db: AsyncSession, id: int) -> Optional[ModelType]:\n        \"\"\"Get by ID.\"\"\"\n        result = await db.execute(\n            select(self.model).where(self.model.id == id)\n        )\n        return result.scalars().first()\n\n    async def get_multi(\n        self,\n        db: AsyncSession,\n        skip: int = 0,\n        limit: int = 100\n    ) -> List[ModelType]:\n        \"\"\"Get multiple records.\"\"\"\n        result = await db.execute(\n            select(self.model).offset(skip).limit(limit)\n        )\n        return result.scalars().all()\n\n    async def create(\n        self,\n        db: AsyncSession,\n        obj_in: CreateSchemaType\n    ) -> ModelType:\n        \"\"\"Create new record.\"\"\"\n        db_obj = self.model(**obj_in.dict())\n        db.add(db_obj)\n        await db.flush()\n        await db.refresh(db_obj)\n        return db_obj\n\n    async def update(\n        self,\n        db: AsyncSession,\n        db_obj: ModelType,\n        obj_in: UpdateSchemaType\n    ) -> ModelType:\n        \"\"\"Update record.\"\"\"\n        update_data = obj_in.dict(exclude_unset=True)\n        for field, value in update_data.items():\n            setattr(db_obj, field, value)\n        await db.flush()\n        await db.refresh(db_obj)\n        return db_obj\n\n    async def delete(self, db: AsyncSession, id: int) -> bool:\n        \"\"\"Delete record.\"\"\"\n        obj = await self.get(db, id)\n        if obj:\n            await db.delete(obj)\n            return True\n        return False\n\n# repositories/user_repository.py\nfrom app.repositories.base_repository import BaseRepository\nfrom app.models.user import User\nfrom app.schemas.user import UserCreate, UserUpdate\n\nclass UserRepository(BaseRepository[User, UserCreate, UserUpdate]):\n    \"\"\"User-specific repository.\"\"\"\n\n    async def get_by_email(self, db: AsyncSession, email: str) -> Optional[User]:\n        \"\"\"Get user by email.\"\"\"\n        result = await db.execute(\n            select(User).where(User.email == email)\n        )\n        return result.scalars().first()\n\n    async def is_active(self, db: AsyncSession, user_id: int) -> bool:\n        \"\"\"Check if user is active.\"\"\"\n        user = await self.get(db, user_id)\n        return user.is_active if user else False\n\nuser_repository = UserRepository(User)\n```\n\n### Pattern 3: Service Layer\n\n```python\n# services/user_service.py\nfrom typing import Optional\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom app.repositories.user_repository import user_repository\nfrom app.schemas.user import UserCreate, UserUpdate, User\nfrom app.core.security import get_password_hash, verify_password\n\nclass UserService:\n    \"\"\"Business logic for users.\"\"\"\n\n    def __init__(self):\n        self.repository = user_repository\n\n    async def create_user(\n        self,\n        db: AsyncSession,\n        user_in: UserCreate\n    ) -> User:\n        \"\"\"Create new user with hashed password.\"\"\"\n        # Check if email exists\n        existing = await self.repository.get_by_email(db, user_in.email)\n        if existing:\n            raise ValueError(\"Email already registered\")\n\n        # Hash password\n        user_in_dict = user_in.dict()\n        user_in_dict[\"hashed_password\"] = get_password_hash(user_in_dict.pop(\"password\"))\n\n        # Create user\n        user = await self.repository.create(db, UserCreate(**user_in_dict))\n        return user\n\n    async def authenticate(\n        self,\n        db: AsyncSession,\n        email: str,\n        password: str\n    ) -> Optional[User]:\n        \"\"\"Authenticate user.\"\"\"\n        user = await self.repository.get_by_email(db, email)\n        if not user:\n            return None\n        if not verify_password(password, user.hashed_password):\n            return None\n        return user\n\n    async def update_user(\n        self,\n        db: AsyncSession,\n        user_id: int,\n        user_in: UserUpdate\n    ) -> Optional[User]:\n        \"\"\"Update user.\"\"\"\n        user = await self.repository.get(db, user_id)\n        if not user:\n            return None\n\n        if user_in.password:\n            user_in_dict = user_in.dict(exclude_unset=True)\n            user_in_dict[\"hashed_password\"] = get_password_hash(\n                user_in_dict.pop(\"password\")\n            )\n            user_in = UserUpdate(**user_in_dict)\n\n        return await self.repository.update(db, user, user_in)\n\nuser_service = UserService()\n```\n\n### Pattern 4: API Endpoints with Dependencies\n\n```python\n# api/v1/endpoints/users.py\nfrom fastapi import APIRouter, Depends, HTTPException, status\nfrom sqlalchemy.ext.asyncio import AsyncSession\nfrom typing import List\n\nfrom app.core.database import get_db\nfrom app.schemas.user import User, UserCreate, UserUpdate\nfrom app.services.user_service import user_service\nfrom app.api.dependencies import get_current_user\n\nrouter = APIRouter()\n\n@router.post(\"/\", response_model=User, status_code=status.HTTP_201_CREATED)\nasync def create_user(\n    user_in: UserCreate,\n    db: AsyncSession = Depends(get_db)\n):\n    \"\"\"Create new user.\"\"\"\n    try:\n        user = await user_service.create_user(db, user_in)\n        return user\n    except ValueError as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\n@router.get(\"/me\", response_model=User)\nasync def read_current_user(\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Get current user.\"\"\"\n    return current_user\n\n@router.get(\"/{user_id}\", response_model=User)\nasync def read_user(\n    user_id: int,\n    db: AsyncSession = Depends(get_db),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Get user by ID.\"\"\"\n    user = await user_service.repository.get(db, user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    return user\n\n@router.patch(\"/{user_id}\", response_model=User)\nasync def update_user(\n    user_id: int,\n    user_in: UserUpdate,\n    db: AsyncSession = Depends(get_db),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Update user.\"\"\"\n    if current_user.id != user_id:\n        raise HTTPException(status_code=403, detail=\"Not authorized\")\n\n    user = await user_service.update_user(db, user_id, user_in)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    return user\n\n@router.delete(\"/{user_id}\", status_code=status.HTTP_204_NO_CONTENT)\nasync def delete_user(\n    user_id: int,\n    db: AsyncSession = Depends(get_db),\n    current_user: User = Depends(get_current_user)\n):\n    \"\"\"Delete user.\"\"\"\n    if current_user.id != user_id:\n        raise HTTPException(status_code=403, detail=\"Not authorized\")\n\n    deleted = await user_service.repository.delete(db, user_id)\n    if not deleted:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n```\n\n### Pattern 5: Authentication & Authorization\n\n```python\n# core/security.py\nfrom datetime import datetime, timedelta\nfrom typing import Optional\nfrom jose import JWTError, jwt\nfrom passlib.context import CryptContext\nfrom app.core.config import get_settings\n\nsettings = get_settings()\npwd_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\n\nALGORITHM = \"HS256\"\n\ndef create_access_token(data: dict, expires_delta: Optional[timedelta] = None):\n    \"\"\"Create JWT access token.\"\"\"\n    to_encode = data.copy()\n    if expires_delta:\n        expire = datetime.utcnow() + expires_delta\n    else:\n        expire = datetime.utcnow() + timedelta(minutes=15)\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=ALGORITHM)\n    return encoded_jwt\n\ndef verify_password(plain_password: str, hashed_password: str) -> bool:\n    \"\"\"Verify password against hash.\"\"\"\n    return pwd_context.verify(plain_password, hashed_password)\n\ndef get_password_hash(password: str) -> str:\n    \"\"\"Hash password.\"\"\"\n    return pwd_context.hash(password)\n\n# api/dependencies.py\nfrom fastapi import Depends, HTTPException, status\nfrom fastapi.security import OAuth2PasswordBearer\nfrom jose import JWTError, jwt\nfrom sqlalchemy.ext.asyncio import AsyncSession\n\nfrom app.core.database import get_db\nfrom app.core.security import ALGORITHM\nfrom app.core.config import get_settings\nfrom app.repositories.user_repository import user_repository\n\noauth2_scheme = OAuth2PasswordBearer(tokenUrl=f\"{settings.API_V1_STR}/auth/login\")\n\nasync def get_current_user(\n    db: AsyncSession = Depends(get_db),\n    token: str = Depends(oauth2_scheme)\n):\n    \"\"\"Get current authenticated user.\"\"\"\n    credentials_exception = HTTPException(\n        status_code=status.HTTP_401_UNAUTHORIZED,\n        detail=\"Could not validate credentials\",\n        headers={\"WWW-Authenticate\": \"Bearer\"},\n    )\n\n    try:\n        payload = jwt.decode(token, settings.SECRET_KEY, algorithms=[ALGORITHM])\n        user_id: int = payload.get(\"sub\")\n        if user_id is None:\n            raise credentials_exception\n    except JWTError:\n        raise credentials_exception\n\n    user = await user_repository.get(db, user_id)\n    if user is None:\n        raise credentials_exception\n\n    return user\n```\n\n## Testing\n\n```python\n# tests/conftest.py\nimport pytest\nimport asyncio\nfrom httpx import AsyncClient\nfrom sqlalchemy.ext.asyncio import create_async_engine, AsyncSession\nfrom sqlalchemy.orm import sessionmaker\n\nfrom app.main import app\nfrom app.core.database import get_db, Base\n\nTEST_DATABASE_URL = \"sqlite+aiosqlite:///:memory:\"\n\n@pytest.fixture(scope=\"session\")\ndef event_loop():\n    loop = asyncio.get_event_loop_policy().new_event_loop()\n    yield loop\n    loop.close()\n\n@pytest.fixture\nasync def db_session():\n    engine = create_async_engine(TEST_DATABASE_URL, echo=True)\n    async with engine.begin() as conn:\n        await conn.run_sync(Base.metadata.create_all)\n\n    AsyncSessionLocal = sessionmaker(\n        engine, class_=AsyncSession, expire_on_commit=False\n    )\n\n    async with AsyncSessionLocal() as session:\n        yield session\n\n@pytest.fixture\nasync def client(db_session):\n    async def override_get_db():\n        yield db_session\n\n    app.dependency_overrides[get_db] = override_get_db\n\n    async with AsyncClient(app=app, base_url=\"http://test\") as client:\n        yield client\n\n# tests/test_users.py\nimport pytest\n\n@pytest.mark.asyncio\nasync def test_create_user(client):\n    response = await client.post(\n        \"/api/v1/users/\",\n        json={\n            \"email\": \"test@example.com\",\n            \"password\": \"testpass123\",\n            \"name\": \"Test User\"\n        }\n    )\n    assert response.status_code == 201\n    data = response.json()\n    assert data[\"email\"] == \"test@example.com\"\n    assert \"id\" in data\n```\n\n## Resources\n\n- **references/fastapi-architecture.md**: Detailed architecture guide\n- **references/async-best-practices.md**: Async/await patterns\n- **references/testing-strategies.md**: Comprehensive testing guide\n- **assets/project-template/**: Complete FastAPI project\n- **assets/docker-compose.yml**: Development environment setup\n\n## Best Practices\n\n1. **Async All The Way**: Use async for database, external APIs\n2. **Dependency Injection**: Leverage FastAPI''''s DI system\n3. **Repository Pattern**: Separate data access from business logic\n4. **Service Layer**: Keep business logic out of routes\n5. **Pydantic Schemas**: Strong typing for request/response\n6. **Error Handling**: Consistent error responses\n7. **Testing**: Test all layers independently\n\n## Common Pitfalls\n\n- **Blocking Code in Async**: Using synchronous database drivers\n- **No Service Layer**: Business logic in route handlers\n- **Missing Type Hints**: Loses FastAPI''''s benefits\n- **Ignoring Sessions**: Not properly managing database sessions\n- **No Testing**: Skipping integration tests\n- **Tight Coupling**: Direct database access in routes\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, completeness: 15, clarity_structure: 12',
  'Score: 92, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-api-scaffolding-skills-fastapi-templates-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'monorepo-management - Master monorepo management with Turborepo, Nx, and pnpm workspaces to build efficient, scalable multi-package repositories with optimized builds and d',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Monorepo Management\n\nBuild efficient, scalable monorepos that enable code sharing, consistent tooling, and atomic changes across multiple packages and applications.\n\n## When to Use This Skill\n\n- Setting up new monorepo projects\n- Migrating from multi-repo to monorepo\n- Optimizing build and test performance\n- Managing shared dependencies\n- Implementing code sharing strategies\n- Setting up CI/CD for monorepos\n- Versioning and publishing packages\n- Debugging monorepo-specific issues\n\n## Core Concepts\n\n### 1. Why Monorepos?\n\n**Advantages:**\n- Shared code and dependencies\n- Atomic commits across projects\n- Consistent tooling and standards\n- Easier refactoring\n- Simplified dependency management\n- Better code visibility\n\n**Challenges:**\n- Build performance at scale\n- CI/CD complexity\n- Access control\n- Large Git repository\n\n### 2. Monorepo Tools\n\n**Package Managers:**\n- pnpm workspaces (recommended)\n- npm workspaces\n- Yarn workspaces\n\n**Build Systems:**\n- Turborepo (recommended for most)\n- Nx (feature-rich, complex)\n- Lerna (older, maintenance mode)\n\n## Turborepo Setup\n\n### Initial Setup\n\n```bash\n# Create new monorepo\nnpx create-turbo@latest my-monorepo\ncd my-monorepo\n\n# Structure:\n# apps/\n#   web/          - Next.js app\n#   docs/         - Documentation site\n# packages/\n#   ui/           - Shared UI components\n#   config/       - Shared configurations\n#   tsconfig/     - Shared TypeScript configs\n# turbo.json      - Turborepo configuration\n# package.json    - Root package.json\n```\n\n### Configuration\n\n```json\n// turbo.json\n{\n  \"$schema\": \"https://turbo.build/schema.json\",\n  \"globalDependencies\": [\"**/.env.*local\"],\n  \"pipeline\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": [\"dist/**\", \".next/**\", \"!.next/cache/**\"]\n    },\n    \"test\": {\n      \"dependsOn\": [\"build\"],\n      \"outputs\": [\"coverage/**\"]\n    },\n    \"lint\": {\n      \"outputs\": []\n    },\n    \"dev\": {\n      \"cache\": false,\n      \"persistent\": true\n    },\n    \"type-check\": {\n      \"dependsOn\": [\"^build\"],\n      \"outputs\": []\n    }\n  }\n}\n```\n\n```json\n// package.json (root)\n{\n  \"name\": \"my-monorepo\",\n  \"private\": true,\n  \"workspaces\": [\n    \"apps/*\",\n    \"packages/*\"\n  ],\n  \"scripts\": {\n    \"build\": \"turbo run build\",\n    \"dev\": \"turbo run dev\",\n    \"test\": \"turbo run test\",\n    \"lint\": \"turbo run lint\",\n    \"format\": \"prettier --write \\\"**/*.{ts,tsx,md}\\\"\",\n    \"clean\": \"turbo run clean && rm -rf node_modules\"\n  },\n  \"devDependencies\": {\n    \"turbo\": \"^1.10.0\",\n    \"prettier\": \"^3.0.0\",\n    \"typescript\": \"^5.0.0\"\n  },\n  \"packageManager\": \"pnpm@8.0.0\"\n}\n```\n\n### Package Structure\n\n```json\n// packages/ui/package.json\n{\n  \"name\": \"@repo/ui\",\n  \"version\": \"0.0.0\",\n  \"private\": true,\n  \"main\": \"./dist/index.js\",\n  \"types\": \"./dist/index.d.ts\",\n  \"exports\": {\n    \".\": {\n      \"import\": \"./dist/index.js\",\n      \"types\": \"./dist/index.d.ts\"\n    },\n    \"./button\": {\n      \"import\": \"./dist/button.js\",\n      \"types\": \"./dist/button.d.ts\"\n    }\n  },\n  \"scripts\": {\n    \"build\": \"tsup src/index.ts --format esm,cjs --dts\",\n    \"dev\": \"tsup src/index.ts --format esm,cjs --dts --watch\",\n    \"lint\": \"eslint src/\",\n    \"type-check\": \"tsc --noEmit\"\n  },\n  \"devDependencies\": {\n    \"@repo/tsconfig\": \"workspace:*\",\n    \"tsup\": \"^7.0.0\",\n    \"typescript\": \"^5.0.0\"\n  },\n  \"dependencies\": {\n    \"react\": \"^18.2.0\"\n  }\n}\n```\n\n## pnpm Workspaces\n\n### Setup\n\n```yaml\n# pnpm-workspace.yaml\npackages:\n  - ''''apps/*''''\n  - ''''packages/*''''\n  - ''''tools/*''''\n```\n\n```json\n// .npmrc\n# Hoist shared dependencies\nshamefully-hoist=true\n\n# Strict peer dependencies\nauto-install-peers=true\nstrict-peer-dependencies=true\n\n# Performance\nstore-dir=~/.pnpm-store\n```\n\n### Dependency Management\n\n```bash\n# Install dependency in specific package\npnpm add react --filter @repo/ui\npnpm add -D typescript --filter @repo/ui\n\n# Install workspace dependency\npnpm add @repo/ui --filter web\n\n# Install in all packages\npnpm add -D eslint -w\n\n# Update all dependencies\npnpm update -r\n\n# Remove dependency\npnpm remove react --filter @repo/ui\n```\n\n### Scripts\n\n```bash\n# Run script in specific package\npnpm --filter web dev\npnpm --filter @repo/ui build\n\n# Run in all packages\npnpm -r build\npnpm -r test\n\n# Run in parallel\npnpm -r --parallel dev\n\n# Filter by pattern\npnpm --filter \"@repo/*\" build\npnpm --filter \"...web\" build  # Build web and dependencies\n```\n\n## Nx Monorepo\n\n### Setup\n\n```bash\n# Create Nx monorepo\nnpx create-nx-workspace@latest my-org\n\n# Generate applications\nnx generate @nx/react:app my-app\nnx generate @nx/next:app my-next-app\n\n# Generate libraries\nnx generate @nx/react:lib ui-components\nnx generate @nx/js:lib utils\n```\n\n### Configuration\n\n```json\n// nx.json\n{\n  \"extends\": \"nx/presets/npm.json\",\n  \"$schema\": \"./node_modules/nx/schemas/nx-schema.json\",\n  \"targetDefaults\": {\n    \"build\": {\n      \"dependsOn\": [\"^build\"],\n      \"inputs\": [\"production\", \"^production\"],\n      \"cache\": true\n    },\n    \"test\": {\n      \"inputs\": [\"default\", \"^production\", \"{workspaceRoot}/jest.preset.js\"],\n      \"cache\": true\n    },\n    \"lint\": {\n      \"inputs\": [\"default\", \"{workspaceRoot}/.eslintrc.json\"],\n      \"cache\": true\n    }\n  },\n  \"namedInputs\": {\n    \"default\": [\"{projectRoot}/**/*\", \"sharedGlobals\"],\n    \"production\": [\n      \"default\",\n      \"!{projectRoot}/**/?(*.)+(spec|test).[jt]s?(x)?(.snap)\",\n      \"!{projectRoot}/tsconfig.spec.json\"\n    ],\n    \"sharedGlobals\": []\n  }\n}\n```\n\n### Running Tasks\n\n```bash\n# Run task for specific project\nnx build my-app\nnx test ui-components\nnx lint utils\n\n# Run for affected projects\nnx affected:build\nnx affected:test --base=main\n\n# Visualize dependencies\nnx graph\n\n# Run in parallel\nnx run-many --target=build --all --parallel=3\n```\n\n## Shared Configurations\n\n### TypeScript Configuration\n\n```json\n// packages/tsconfig/base.json\n{\n  \"compilerOptions\": {\n    \"strict\": true,\n    \"esModuleInterop\": true,\n    \"skipLibCheck\": true,\n    \"forceConsistentCasingInFileNames\": true,\n    \"module\": \"ESNext\",\n    \"moduleResolution\": \"bundler\",\n    \"resolveJsonModule\": true,\n    \"isolatedModules\": true,\n    \"incremental\": true,\n    \"declaration\": true\n  },\n  \"exclude\": [\"node_modules\"]\n}\n\n// packages/tsconfig/react.json\n{\n  \"extends\": \"./base.json\",\n  \"compilerOptions\": {\n    \"jsx\": \"react-jsx\",\n    \"lib\": [\"ES2022\", \"DOM\", \"DOM.Iterable\"]\n  }\n}\n\n// apps/web/tsconfig.json\n{\n  \"extends\": \"@repo/tsconfig/react.json\",\n  \"compilerOptions\": {\n    \"outDir\": \"dist\",\n    \"rootDir\": \"src\"\n  },\n  \"include\": [\"src\"],\n  \"exclude\": [\"node_modules\", \"dist\"]\n}\n```\n\n### ESLint Configuration\n\n```javascript\n// packages/config/eslint-preset.js\nmodule.exports = {\n  extends: [\n    ''''eslint:recommended'''',\n    ''''plugin:@typescript-eslint/recommended'''',\n    ''''plugin:react/recommended'''',\n    ''''plugin:react-hooks/recommended'''',\n    ''''prettier'''',\n  ],\n  plugins: [''''@typescript-eslint'''', ''''react'''', ''''react-hooks''''],\n  parser: ''''@typescript-eslint/parser'''',\n  parserOptions: {\n    ecmaVersion: 2022,\n    sourceType: ''''module'''',\n    ecmaFeatures: {\n      jsx: true,\n    },\n  },\n  settings: {\n    react: {\n      version: ''''detect'''',\n    },\n  },\n  rules: {\n    ''''@typescript-eslint/no-unused-vars'''': ''''error'''',\n    ''''react/react-in-jsx-scope'''': ''''off'''',\n  },\n};\n\n// apps/web/.eslintrc.js\nmodule.exports = {\n  extends: [''''@repo/config/eslint-preset''''],\n  rules: {\n    // App-specific rules\n  },\n};\n```\n\n## Code Sharing Patterns\n\n### Pattern 1: Shared UI Components\n\n```typescript\n// packages/ui/src/button.tsx\nimport * as React from ''''react'''';\n\nexport interface ButtonProps {\n  variant?: ''''primary'''' | ''''secondary'''';\n  children: React.ReactNode;\n  onClick?: () => void;\n}\n\nexport function Button({ variant = ''''primary'''', children, onClick }: ButtonProps) {\n  return (\n    <button\n      className={`btn btn-${variant}`}\n      onClick={onClick}\n    >\n      {children}\n    </button>\n  );\n}\n\n// packages/ui/src/index.ts\nexport { Button, type ButtonProps } from ''''./button'''';\nexport { Input, type InputProps } from ''''./input'''';\n\n// apps/web/src/app.tsx\nimport { Button } from ''''@repo/ui'''';\n\nexport function App() {\n  return <Button variant=\"primary\">Click me</Button>;\n}\n```\n\n### Pattern 2: Shared Utilities\n\n```typescript\n// packages/utils/src/string.ts\nexport function capitalize(str: string): string {\n  return str.charAt(0).toUpperCase() + str.slice(1);\n}\n\nexport function truncate(str: string, length: number): string {\n  return str.length > length ? str.slice(0, length) + ''''...'''' : str;\n}\n\n// packages/utils/src/index.ts\nexport * from ''''./string'''';\nexport * from ''''./array'''';\nexport * from ''''./date'''';\n\n// Usage in apps\nimport { capitalize, truncate } from ''''@repo/utils'''';\n```\n\n### Pattern 3: Shared Types\n\n```typescript\n// packages/types/src/user.ts\nexport interface User {\n  id: string;\n  email: string;\n  name: string;\n  role: ''''admin'''' | ''''user'''';\n}\n\nexport interface CreateUserInput {\n  email: string;\n  name: string;\n  password: string;\n}\n\n// Used in both frontend and backend\nimport type { User, CreateUserInput } from ''''@repo/types'''';\n```\n\n## Build Optimization\n\n### Turborepo Caching\n\n```json\n// turbo.json\n{\n  \"pipeline\": {\n    \"build\": {\n      // Build depends on dependencies being built first\n      \"dependsOn\": [\"^build\"],\n\n      // Cache these outputs\n      \"outputs\": [\"dist/**\", \".next/**\"],\n\n      // Cache based on these inputs (default: all files)\n      \"inputs\": [\"src/**/*.tsx\", \"src/**/*.ts\", \"package.json\"]\n    },\n    \"test\": {\n      // Run tests in parallel, don''''t depend on build\n      \"cache\": true,\n      \"outputs\": [\"coverage/**\"]\n    }\n  }\n}\n```\n\n### Remote Caching\n\n```bash\n# Turborepo Remote Cache (Vercel)\nnpx turbo login\nnpx turbo link\n\n# Custom remote cache\n# turbo.json\n{\n  \"remoteCache\": {\n    \"signature\": true,\n    \"enabled\": true\n  }\n}\n```\n\n## CI/CD for Monorepos\n\n### GitHub Actions\n\n```yaml\n# .github/workflows/ci.yml\nname: CI\n\non:\n  push:\n    branches: [main]\n  pull_request:\n    branches: [main]\n\njobs:\n  build:\n    runs-on: ubuntu-latest\n\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          fetch-depth: 0  # For Nx affected commands\n\n      - uses: pnpm/action-setup@v2\n        with:\n          version: 8\n\n      - uses: actions/setup-node@v3\n        with:\n          node-version: 18\n          cache: ''''pnpm''''\n\n      - name: Install dependencies\n        run: pnpm install --frozen-lockfile\n\n      - name: Build\n        run: pnpm turbo run build\n\n      - name: Test\n        run: pnpm turbo run test\n\n      - name: Lint\n        run: pnpm turbo run lint\n\n      - name: Type check\n        run: pnpm turbo run type-check\n```\n\n### Deploy Affected Only\n\n```yaml\n# Deploy only changed apps\n- name: Deploy affected apps\n  run: |\n    if pnpm nx affected:apps --base=origin/main --head=HEAD | grep -q \"web\"; then\n      echo \"Deploying web app\"\n      pnpm --filter web deploy\n    fi\n```\n\n## Best Practices\n\n1. **Consistent Versioning**: Lock dependency versions across workspace\n2. **Shared Configs**: Centralize ESLint, TypeScript, Prettier configs\n3. **Dependency Graph**: Keep it acyclic, avoid circular dependencies\n4. **Cache Effectively**: Configure inputs/outputs correctly\n5. **Type Safety**: Share types between frontend/backend\n6. **Testing Strategy**: Unit tests in packages, E2E in apps\n7. **Documentation**: README in each package\n8. **Release Strategy**: Use changesets for versioning\n\n## Common Pitfalls\n\n- **Circular Dependencies**: A depends on B, B depends on A\n- **Phantom Dependencies**: Using deps not in package.json\n- **Incorrect Cache Inputs**: Missing files in Turborepo inputs\n- **Over-Sharing**: Sharing code that should be separate\n- **Under-Sharing**: Duplicating code across packages\n- **Large Monorepos**: Without proper tooling, builds slow down\n\n## Publishing Packages\n\n```bash\n# Using Changesets\npnpm add -Dw @changesets/cli\npnpm changeset init\n\n# Create changeset\npnpm changeset\n\n# Version packages\npnpm changeset version\n\n# Publish\npnpm changeset publish\n```\n\n```yaml\n# .github/workflows/release.yml\n- name: Create Release Pull Request or Publish\n  uses: changesets/action@v1\n  with:\n    publish: pnpm release\n  env:\n    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n    NPM_TOKEN: ${{ secrets.NPM_TOKEN }}\n```\n\n## Resources\n\n- **references/turborepo-guide.md**: Comprehensive Turborepo documentation\n- **references/nx-guide.md**: Nx monorepo patterns\n- **references/pnpm-workspaces.md**: pnpm workspace features\n- **assets/monorepo-checklist.md**: Setup checklist\n- **assets/migration-guide.md**: Multi-repo to monorepo migration\n- **scripts/dependency-graph.ts**: Visualize package dependencies\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 15, completeness: 15, error_handling: 6',
  'Score: 89, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, example_quality: 18, problem_definition: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-developer-essentials-skills-monorepo-management-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'llm-evaluation - Implement comprehensive evaluation strategies for LLM applications using automated metrics, human feedback, and benchmarking. Use when testing LLM per',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# LLM Evaluation\n\nMaster comprehensive evaluation strategies for LLM applications, from automated metrics to human evaluation and A/B testing.\n\n## When to Use This Skill\n\n- Measuring LLM application performance systematically\n- Comparing different models or prompts\n- Detecting performance regressions before deployment\n- Validating improvements from prompt changes\n- Building confidence in production systems\n- Establishing baselines and tracking progress over time\n- Debugging unexpected model behavior\n\n## Core Evaluation Types\n\n### 1. Automated Metrics\nFast, repeatable, scalable evaluation using computed scores.\n\n**Text Generation:**\n- **BLEU**: N-gram overlap (translation)\n- **ROUGE**: Recall-oriented (summarization)\n- **METEOR**: Semantic similarity\n- **BERTScore**: Embedding-based similarity\n- **Perplexity**: Language model confidence\n\n**Classification:**\n- **Accuracy**: Percentage correct\n- **Precision/Recall/F1**: Class-specific performance\n- **Confusion Matrix**: Error patterns\n- **AUC-ROC**: Ranking quality\n\n**Retrieval (RAG):**\n- **MRR**: Mean Reciprocal Rank\n- **NDCG**: Normalized Discounted Cumulative Gain\n- **Precision@K**: Relevant in top K\n- **Recall@K**: Coverage in top K\n\n### 2. Human Evaluation\nManual assessment for quality aspects difficult to automate.\n\n**Dimensions:**\n- **Accuracy**: Factual correctness\n- **Coherence**: Logical flow\n- **Relevance**: Answers the question\n- **Fluency**: Natural language quality\n- **Safety**: No harmful content\n- **Helpfulness**: Useful to the user\n\n### 3. LLM-as-Judge\nUse stronger LLMs to evaluate weaker model outputs.\n\n**Approaches:**\n- **Pointwise**: Score individual responses\n- **Pairwise**: Compare two responses\n- **Reference-based**: Compare to gold standard\n- **Reference-free**: Judge without ground truth\n\n## Quick Start\n\n```python\nfrom llm_eval import EvaluationSuite, Metric\n\n# Define evaluation suite\nsuite = EvaluationSuite([\n    Metric.accuracy(),\n    Metric.bleu(),\n    Metric.bertscore(),\n    Metric.custom(name=\"groundedness\", fn=check_groundedness)\n])\n\n# Prepare test cases\ntest_cases = [\n    {\n        \"input\": \"What is the capital of France?\",\n        \"expected\": \"Paris\",\n        \"context\": \"France is a country in Europe. Paris is its capital.\"\n    },\n    # ... more test cases\n]\n\n# Run evaluation\nresults = suite.evaluate(\n    model=your_model,\n    test_cases=test_cases\n)\n\nprint(f\"Overall Accuracy: {results.metrics[''''accuracy'''']}\")\nprint(f\"BLEU Score: {results.metrics[''''bleu'''']}\")\n```\n\n## Automated Metrics Implementation\n\n### BLEU Score\n```python\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n\ndef calculate_bleu(reference, hypothesis):\n    \"\"\"Calculate BLEU score between reference and hypothesis.\"\"\"\n    smoothie = SmoothingFunction().method4\n\n    return sentence_bleu(\n        [reference.split()],\n        hypothesis.split(),\n        smoothing_function=smoothie\n    )\n\n# Usage\nbleu = calculate_bleu(\n    reference=\"The cat sat on the mat\",\n    hypothesis=\"A cat is sitting on the mat\"\n)\n```\n\n### ROUGE Score\n```python\nfrom rouge_score import rouge_scorer\n\ndef calculate_rouge(reference, hypothesis):\n    \"\"\"Calculate ROUGE scores.\"\"\"\n    scorer = rouge_scorer.RougeScorer([''''rouge1'''', ''''rouge2'''', ''''rougeL''''], use_stemmer=True)\n    scores = scorer.score(reference, hypothesis)\n\n    return {\n        ''''rouge1'''': scores[''''rouge1''''].fmeasure,\n        ''''rouge2'''': scores[''''rouge2''''].fmeasure,\n        ''''rougeL'''': scores[''''rougeL''''].fmeasure\n    }\n```\n\n### BERTScore\n```python\nfrom bert_score import score\n\ndef calculate_bertscore(references, hypotheses):\n    \"\"\"Calculate BERTScore using pre-trained BERT.\"\"\"\n    P, R, F1 = score(\n        hypotheses,\n        references,\n        lang=''''en'''',\n        model_type=''''microsoft/deberta-xlarge-mnli''''\n    )\n\n    return {\n        ''''precision'''': P.mean().item(),\n        ''''recall'''': R.mean().item(),\n        ''''f1'''': F1.mean().item()\n    }\n```\n\n### Custom Metrics\n```python\ndef calculate_groundedness(response, context):\n    \"\"\"Check if response is grounded in provided context.\"\"\"\n    # Use NLI model to check entailment\n    from transformers import pipeline\n\n    nli = pipeline(\"text-classification\", model=\"microsoft/deberta-large-mnli\")\n\n    result = nli(f\"{context} [SEP] {response}\")[0]\n\n    # Return confidence that response is entailed by context\n    return result[''''score''''] if result[''''label''''] == ''''ENTAILMENT'''' else 0.0\n\ndef calculate_toxicity(text):\n    \"\"\"Measure toxicity in generated text.\"\"\"\n    from detoxify import Detoxify\n\n    results = Detoxify(''''original'''').predict(text)\n    return max(results.values())  # Return highest toxicity score\n\ndef calculate_factuality(claim, knowledge_base):\n    \"\"\"Verify factual claims against knowledge base.\"\"\"\n    # Implementation depends on your knowledge base\n    # Could use retrieval + NLI, or fact-checking API\n    pass\n```\n\n## LLM-as-Judge Patterns\n\n### Single Output Evaluation\n```python\ndef llm_judge_quality(response, question):\n    \"\"\"Use GPT-5 to judge response quality.\"\"\"\n    prompt = f\"\"\"Rate the following response on a scale of 1-10 for:\n1. Accuracy (factually correct)\n2. Helpfulness (answers the question)\n3. Clarity (well-written and understandable)\n\nQuestion: {question}\nResponse: {response}\n\nProvide ratings in JSON format:\n{{\n  \"accuracy\": <1-10>,\n  \"helpfulness\": <1-10>,\n  \"clarity\": <1-10>,\n  \"reasoning\": \"<brief explanation>\"\n}}\n\"\"\"\n\n    result = openai.ChatCompletion.create(\n        model=\"gpt-5\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0\n    )\n\n    return json.loads(result.choices[0].message.content)\n```\n\n### Pairwise Comparison\n```python\ndef compare_responses(question, response_a, response_b):\n    \"\"\"Compare two responses using LLM judge.\"\"\"\n    prompt = f\"\"\"Compare these two responses to the question and determine which is better.\n\nQuestion: {question}\n\nResponse A: {response_a}\n\nResponse B: {response_b}\n\nWhich response is better and why? Consider accuracy, helpfulness, and clarity.\n\nAnswer with JSON:\n{{\n  \"winner\": \"A\" or \"B\" or \"tie\",\n  \"reasoning\": \"<explanation>\",\n  \"confidence\": <1-10>\n}}\n\"\"\"\n\n    result = openai.ChatCompletion.create(\n        model=\"gpt-5\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=0\n    )\n\n    return json.loads(result.choices[0].message.content)\n```\n\n## Human Evaluation Frameworks\n\n### Annotation Guidelines\n```python\nclass AnnotationTask:\n    \"\"\"Structure for human annotation task.\"\"\"\n\n    def __init__(self, response, question, context=None):\n        self.response = response\n        self.question = question\n        self.context = context\n\n    def get_annotation_form(self):\n        return {\n            \"question\": self.question,\n            \"context\": self.context,\n            \"response\": self.response,\n            \"ratings\": {\n                \"accuracy\": {\n                    \"scale\": \"1-5\",\n                    \"description\": \"Is the response factually correct?\"\n                },\n                \"relevance\": {\n                    \"scale\": \"1-5\",\n                    \"description\": \"Does it answer the question?\"\n                },\n                \"coherence\": {\n                    \"scale\": \"1-5\",\n                    \"description\": \"Is it logically consistent?\"\n                }\n            },\n            \"issues\": {\n                \"factual_error\": False,\n                \"hallucination\": False,\n                \"off_topic\": False,\n                \"unsafe_content\": False\n            },\n            \"feedback\": \"\"\n        }\n```\n\n### Inter-Rater Agreement\n```python\nfrom sklearn.metrics import cohen_kappa_score\n\ndef calculate_agreement(rater1_scores, rater2_scores):\n    \"\"\"Calculate inter-rater agreement.\"\"\"\n    kappa = cohen_kappa_score(rater1_scores, rater2_scores)\n\n    interpretation = {\n        kappa < 0: \"Poor\",\n        kappa < 0.2: \"Slight\",\n        kappa < 0.4: \"Fair\",\n        kappa < 0.6: \"Moderate\",\n        kappa < 0.8: \"Substantial\",\n        kappa <= 1.0: \"Almost Perfect\"\n    }\n\n    return {\n        \"kappa\": kappa,\n        \"interpretation\": interpretation[True]\n    }\n```\n\n## A/B Testing\n\n### Statistical Testing Framework\n```python\nfrom scipy import stats\nimport numpy as np\n\nclass ABTest:\n    def __init__(self, variant_a_name=\"A\", variant_b_name=\"B\"):\n        self.variant_a = {\"name\": variant_a_name, \"scores\": []}\n        self.variant_b = {\"name\": variant_b_name, \"scores\": []}\n\n    def add_result(self, variant, score):\n        \"\"\"Add evaluation result for a variant.\"\"\"\n        if variant == \"A\":\n            self.variant_a[\"scores\"].append(score)\n        else:\n            self.variant_b[\"scores\"].append(score)\n\n    def analyze(self, alpha=0.05):\n        \"\"\"Perform statistical analysis.\"\"\"\n        a_scores = self.variant_a[\"scores\"]\n        b_scores = self.variant_b[\"scores\"]\n\n        # T-test\n        t_stat, p_value = stats.ttest_ind(a_scores, b_scores)\n\n        # Effect size (Cohen''''s d)\n        pooled_std = np.sqrt((np.std(a_scores)**2 + np.std(b_scores)**2) / 2)\n        cohens_d = (np.mean(b_scores) - np.mean(a_scores)) / pooled_std\n\n        return {\n            \"variant_a_mean\": np.mean(a_scores),\n            \"variant_b_mean\": np.mean(b_scores),\n            \"difference\": np.mean(b_scores) - np.mean(a_scores),\n            \"relative_improvement\": (np.mean(b_scores) - np.mean(a_scores)) / np.mean(a_scores),\n            \"p_value\": p_value,\n            \"statistically_significant\": p_value < alpha,\n            \"cohens_d\": cohens_d,\n            \"effect_size\": self.interpret_cohens_d(cohens_d),\n            \"winner\": \"B\" if np.mean(b_scores) > np.mean(a_scores) else \"A\"\n        }\n\n    @staticmethod\n    def interpret_cohens_d(d):\n        \"\"\"Interpret Cohen''''s d effect size.\"\"\"\n        abs_d = abs(d)\n        if abs_d < 0.2:\n            return \"negligible\"\n        elif abs_d < 0.5:\n            return \"small\"\n        elif abs_d < 0.8:\n            return \"medium\"\n        else:\n            return \"large\"\n```\n\n## Regression Testing\n\n### Regression Detection\n```python\nclass RegressionDetector:\n    def __init__(self, baseline_results, threshold=0.05):\n        self.baseline = baseline_results\n        self.threshold = threshold\n\n    def check_for_regression(self, new_results):\n        \"\"\"Detect if new results show regression.\"\"\"\n        regressions = []\n\n        for metric in self.baseline.keys():\n            baseline_score = self.baseline[metric]\n            new_score = new_results.get(metric)\n\n            if new_score is None:\n                continue\n\n            # Calculate relative change\n            relative_change = (new_score - baseline_score) / baseline_score\n\n            # Flag if significant decrease\n            if relative_change < -self.threshold:\n                regressions.append({\n                    \"metric\": metric,\n                    \"baseline\": baseline_score,\n                    \"current\": new_score,\n                    \"change\": relative_change\n                })\n\n        return {\n            \"has_regression\": len(regressions) > 0,\n            \"regressions\": regressions\n        }\n```\n\n## Benchmarking\n\n### Running Benchmarks\n```python\nclass BenchmarkRunner:\n    def __init__(self, benchmark_dataset):\n        self.dataset = benchmark_dataset\n\n    def run_benchmark(self, model, metrics):\n        \"\"\"Run model on benchmark and calculate metrics.\"\"\"\n        results = {metric.name: [] for metric in metrics}\n\n        for example in self.dataset:\n            # Generate prediction\n            prediction = model.predict(example[\"input\"])\n\n            # Calculate each metric\n            for metric in metrics:\n                score = metric.calculate(\n                    prediction=prediction,\n                    reference=example[\"reference\"],\n                    context=example.get(\"context\")\n                )\n                results[metric.name].append(score)\n\n        # Aggregate results\n        return {\n            metric: {\n                \"mean\": np.mean(scores),\n                \"std\": np.std(scores),\n                \"min\": min(scores),\n                \"max\": max(scores)\n            }\n            for metric, scores in results.items()\n        }\n```\n\n## Resources\n\n- **references/metrics.md**: Comprehensive metric guide\n- **references/human-evaluation.md**: Annotation best practices\n- **references/benchmarking.md**: Standard benchmarks\n- **references/a-b-testing.md**: Statistical testing guide\n- **references/regression-testing.md**: CI/CD integration\n- **assets/evaluation-framework.py**: Complete evaluation harness\n- **assets/benchmark-dataset.jsonl**: Example datasets\n- **scripts/evaluate-model.py**: Automated evaluation runner\n\n## Best Practices\n\n1. **Multiple Metrics**: Use diverse metrics for comprehensive view\n2. **Representative Data**: Test on real-world, diverse examples\n3. **Baselines**: Always compare against baseline performance\n4. **Statistical Rigor**: Use proper statistical tests for comparisons\n5. **Continuous Evaluation**: Integrate into CI/CD pipeline\n6. **Human Validation**: Combine automated metrics with human judgment\n7. **Error Analysis**: Investigate failures to understand weaknesses\n8. **Version Control**: Track evaluation results over time\n\n## Common Pitfalls\n\n- **Single Metric Obsession**: Optimizing for one metric at the expense of others\n- **Small Sample Size**: Drawing conclusions from too few examples\n- **Data Contamination**: Testing on training data\n- **Ignoring Variance**: Not accounting for statistical uncertainty\n- **Metric Mismatch**: Using metrics not aligned with business goals\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'clarity_structure: 12, error_handling: 10, completeness: 10',
  'Score: 82, Tier: TIER_1_EXCELLENT, Strengths: example_quality: 20, problem_definition: 15, workflow_structure: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-llm-application-dev-skills-llm-evaluation-skill-md',
  'admin:MINER_BATCH_1'
);

INSERT INTO knowledge_entries (query, category, type, solutions, executable_type, prerequisites, common_pitfalls, success_indicators, source_url, contributor_email)
VALUES (
  'database-migration - Execute database migrations across ORMs and platforms with zero-downtime strategies, data transformation, and rollback procedures. Use when migrating ',
  'claude-code',
  'skill',
  '[{"solution": "Complete Skill", "cli": {"macos": "echo ''See SKILL.md''", "linux": "echo ''See SKILL.md''", "windows": "echo ''See SKILL.md''"}, "manual": "\n# Database Migration\n\nMaster database schema and data migrations across ORMs (Sequelize, TypeORM, Prisma), including rollback strategies and zero-downtime deployments.\n\n## When to Use This Skill\n\n- Migrating between different ORMs\n- Performing schema transformations\n- Moving data between databases\n- Implementing rollback procedures\n- Zero-downtime deployments\n- Database version upgrades\n- Data model refactoring\n\n## ORM Migrations\n\n### Sequelize Migrations\n```javascript\n// migrations/20231201-create-users.js\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    await queryInterface.createTable(''''users'''', {\n      id: {\n        type: Sequelize.INTEGER,\n        primaryKey: true,\n        autoIncrement: true\n      },\n      email: {\n        type: Sequelize.STRING,\n        unique: true,\n        allowNull: false\n      },\n      createdAt: Sequelize.DATE,\n      updatedAt: Sequelize.DATE\n    });\n  },\n\n  down: async (queryInterface, Sequelize) => {\n    await queryInterface.dropTable(''''users'''');\n  }\n};\n\n// Run: npx sequelize-cli db:migrate\n// Rollback: npx sequelize-cli db:migrate:undo\n```\n\n### TypeORM Migrations\n```typescript\n// migrations/1701234567-CreateUsers.ts\nimport { MigrationInterface, QueryRunner, Table } from ''''typeorm'''';\n\nexport class CreateUsers1701234567 implements MigrationInterface {\n  public async up(queryRunner: QueryRunner): Promise<void> {\n    await queryRunner.createTable(\n      new Table({\n        name: ''''users'''',\n        columns: [\n          {\n            name: ''''id'''',\n            type: ''''int'''',\n            isPrimary: true,\n            isGenerated: true,\n            generationStrategy: ''''increment''''\n          },\n          {\n            name: ''''email'''',\n            type: ''''varchar'''',\n            isUnique: true\n          },\n          {\n            name: ''''created_at'''',\n            type: ''''timestamp'''',\n            default: ''''CURRENT_TIMESTAMP''''\n          }\n        ]\n      })\n    );\n  }\n\n  public async down(queryRunner: QueryRunner): Promise<void> {\n    await queryRunner.dropTable(''''users'''');\n  }\n}\n\n// Run: npm run typeorm migration:run\n// Rollback: npm run typeorm migration:revert\n```\n\n### Prisma Migrations\n```prisma\n// schema.prisma\nmodel User {\n  id        Int      @id @default(autoincrement())\n  email     String   @unique\n  createdAt DateTime @default(now())\n}\n\n// Generate migration: npx prisma migrate dev --name create_users\n// Apply: npx prisma migrate deploy\n```\n\n## Schema Transformations\n\n### Adding Columns with Defaults\n```javascript\n// Safe migration: add column with default\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    await queryInterface.addColumn(''''users'''', ''''status'''', {\n      type: Sequelize.STRING,\n      defaultValue: ''''active'''',\n      allowNull: false\n    });\n  },\n\n  down: async (queryInterface) => {\n    await queryInterface.removeColumn(''''users'''', ''''status'''');\n  }\n};\n```\n\n### Renaming Columns (Zero Downtime)\n```javascript\n// Step 1: Add new column\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    await queryInterface.addColumn(''''users'''', ''''full_name'''', {\n      type: Sequelize.STRING\n    });\n\n    // Copy data from old column\n    await queryInterface.sequelize.query(\n      ''''UPDATE users SET full_name = name''''\n    );\n  },\n\n  down: async (queryInterface) => {\n    await queryInterface.removeColumn(''''users'''', ''''full_name'''');\n  }\n};\n\n// Step 2: Update application to use new column\n\n// Step 3: Remove old column\nmodule.exports = {\n  up: async (queryInterface) => {\n    await queryInterface.removeColumn(''''users'''', ''''name'''');\n  },\n\n  down: async (queryInterface, Sequelize) => {\n    await queryInterface.addColumn(''''users'''', ''''name'''', {\n      type: Sequelize.STRING\n    });\n  }\n};\n```\n\n### Changing Column Types\n```javascript\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    // For large tables, use multi-step approach\n\n    // 1. Add new column\n    await queryInterface.addColumn(''''users'''', ''''age_new'''', {\n      type: Sequelize.INTEGER\n    });\n\n    // 2. Copy and transform data\n    await queryInterface.sequelize.query(`\n      UPDATE users\n      SET age_new = CAST(age AS INTEGER)\n      WHERE age IS NOT NULL\n    `);\n\n    // 3. Drop old column\n    await queryInterface.removeColumn(''''users'''', ''''age'''');\n\n    // 4. Rename new column\n    await queryInterface.renameColumn(''''users'''', ''''age_new'''', ''''age'''');\n  },\n\n  down: async (queryInterface, Sequelize) => {\n    await queryInterface.changeColumn(''''users'''', ''''age'''', {\n      type: Sequelize.STRING\n    });\n  }\n};\n```\n\n## Data Transformations\n\n### Complex Data Migration\n```javascript\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    // Get all records\n    const [users] = await queryInterface.sequelize.query(\n      ''''SELECT id, address_string FROM users''''\n    );\n\n    // Transform each record\n    for (const user of users) {\n      const addressParts = user.address_string.split('''','''');\n\n      await queryInterface.sequelize.query(\n        `UPDATE users\n         SET street = :street,\n             city = :city,\n             state = :state\n         WHERE id = :id`,\n        {\n          replacements: {\n            id: user.id,\n            street: addressParts[0]?.trim(),\n            city: addressParts[1]?.trim(),\n            state: addressParts[2]?.trim()\n          }\n        }\n      );\n    }\n\n    // Drop old column\n    await queryInterface.removeColumn(''''users'''', ''''address_string'''');\n  },\n\n  down: async (queryInterface, Sequelize) => {\n    // Reconstruct original column\n    await queryInterface.addColumn(''''users'''', ''''address_string'''', {\n      type: Sequelize.STRING\n    });\n\n    await queryInterface.sequelize.query(`\n      UPDATE users\n      SET address_string = CONCAT(street, '''', '''', city, '''', '''', state)\n    `);\n\n    await queryInterface.removeColumn(''''users'''', ''''street'''');\n    await queryInterface.removeColumn(''''users'''', ''''city'''');\n    await queryInterface.removeColumn(''''users'''', ''''state'''');\n  }\n};\n```\n\n## Rollback Strategies\n\n### Transaction-Based Migrations\n```javascript\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    const transaction = await queryInterface.sequelize.transaction();\n\n    try {\n      await queryInterface.addColumn(\n        ''''users'''',\n        ''''verified'''',\n        { type: Sequelize.BOOLEAN, defaultValue: false },\n        { transaction }\n      );\n\n      await queryInterface.sequelize.query(\n        ''''UPDATE users SET verified = true WHERE email_verified_at IS NOT NULL'''',\n        { transaction }\n      );\n\n      await transaction.commit();\n    } catch (error) {\n      await transaction.rollback();\n      throw error;\n    }\n  },\n\n  down: async (queryInterface) => {\n    await queryInterface.removeColumn(''''users'''', ''''verified'''');\n  }\n};\n```\n\n### Checkpoint-Based Rollback\n```javascript\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    // Create backup table\n    await queryInterface.sequelize.query(\n      ''''CREATE TABLE users_backup AS SELECT * FROM users''''\n    );\n\n    try {\n      // Perform migration\n      await queryInterface.addColumn(''''users'''', ''''new_field'''', {\n        type: Sequelize.STRING\n      });\n\n      // Verify migration\n      const [result] = await queryInterface.sequelize.query(\n        \"SELECT COUNT(*) as count FROM users WHERE new_field IS NULL\"\n      );\n\n      if (result[0].count > 0) {\n        throw new Error(''''Migration verification failed'''');\n      }\n\n      // Drop backup\n      await queryInterface.dropTable(''''users_backup'''');\n    } catch (error) {\n      // Restore from backup\n      await queryInterface.sequelize.query(''''DROP TABLE users'''');\n      await queryInterface.sequelize.query(\n        ''''CREATE TABLE users AS SELECT * FROM users_backup''''\n      );\n      await queryInterface.dropTable(''''users_backup'''');\n      throw error;\n    }\n  }\n};\n```\n\n## Zero-Downtime Migrations\n\n### Blue-Green Deployment Strategy\n```javascript\n// Phase 1: Make changes backward compatible\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    // Add new column (both old and new code can work)\n    await queryInterface.addColumn(''''users'''', ''''email_new'''', {\n      type: Sequelize.STRING\n    });\n  }\n};\n\n// Phase 2: Deploy code that writes to both columns\n\n// Phase 3: Backfill data\nmodule.exports = {\n  up: async (queryInterface) => {\n    await queryInterface.sequelize.query(`\n      UPDATE users\n      SET email_new = email\n      WHERE email_new IS NULL\n    `);\n  }\n};\n\n// Phase 4: Deploy code that reads from new column\n\n// Phase 5: Remove old column\nmodule.exports = {\n  up: async (queryInterface) => {\n    await queryInterface.removeColumn(''''users'''', ''''email'''');\n  }\n};\n```\n\n## Cross-Database Migrations\n\n### PostgreSQL to MySQL\n```javascript\n// Handle differences\nmodule.exports = {\n  up: async (queryInterface, Sequelize) => {\n    const dialectName = queryInterface.sequelize.getDialect();\n\n    if (dialectName === ''''mysql'''') {\n      await queryInterface.createTable(''''users'''', {\n        id: {\n          type: Sequelize.INTEGER,\n          primaryKey: true,\n          autoIncrement: true\n        },\n        data: {\n          type: Sequelize.JSON  // MySQL JSON type\n        }\n      });\n    } else if (dialectName === ''''postgres'''') {\n      await queryInterface.createTable(''''users'''', {\n        id: {\n          type: Sequelize.INTEGER,\n          primaryKey: true,\n          autoIncrement: true\n        },\n        data: {\n          type: Sequelize.JSONB  // PostgreSQL JSONB type\n        }\n      });\n    }\n  }\n};\n```\n\n## Resources\n\n- **references/orm-switching.md**: ORM migration guides\n- **references/schema-migration.md**: Schema transformation patterns\n- **references/data-transformation.md**: Data migration scripts\n- **references/rollback-strategies.md**: Rollback procedures\n- **assets/schema-migration-template.sql**: SQL migration templates\n- **assets/data-migration-script.py**: Data migration utilities\n- **scripts/test-migration.sh**: Migration testing script\n\n## Best Practices\n\n1. **Always Provide Rollback**: Every up() needs a down()\n2. **Test Migrations**: Test on staging first\n3. **Use Transactions**: Atomic migrations when possible\n4. **Backup First**: Always backup before migration\n5. **Small Changes**: Break into small, incremental steps\n6. **Monitor**: Watch for errors during deployment\n7. **Document**: Explain why and how\n8. **Idempotent**: Migrations should be rerunnable\n\n## Common Pitfalls\n\n- Not testing rollback procedures\n- Making breaking changes without downtime strategy\n- Forgetting to handle NULL values\n- Not considering index performance\n- Ignoring foreign key constraints\n- Migrating too much data at once\n"}]'::jsonb,
  'manual',
  'Claude Code or compatible AI assistant',
  'error_handling: 15, clarity_structure: 12, completeness: 5',
  'Score: 82, Tier: TIER_1_EXCELLENT, Strengths: workflow_structure: 20, problem_definition: 15, example_quality: 15',
  'https://skillsmp.com/skills/wshobson-agents-plugins-framework-migration-skills-database-migration-skill-md',
  'admin:MINER_BATCH_1'
);