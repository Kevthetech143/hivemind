-- Add Google Cloud Platform incident postmortems (batch 1)
-- Focus: Compute Engine, Cloud Storage, BigQuery outages from 2024-2025

INSERT INTO knowledge_entries (query, category, solutions, common_pitfalls, success_rate, hit_frequency, prerequisites, success_indicators, source_url, claude_version, last_verified, thumbs_up)
VALUES
(
  'Google Cloud global outage June 2025 Service Control quota policy null pointer',
  'incident-postmortem',
  '[{"solution": "Implement feature flags (disabled by default) for all critical binary changes", "percentage": 95, "note": "Prevents untested code paths from activating in production"}, {"solution": "Add validation delays to global data replication pipelines before propagation", "percentage": 90, "note": "Prevents corrupted metadata from spreading system-wide instantly"}, {"solution": "Modularize Service Control so failures remain isolated and requests can proceed", "percentage": 85, "note": "Reduces single point of failure impact"}]'::jsonb,
  'Feature deployed May 29, 2025 lacked feature flag protection. Malformed policy with blank fields triggered null pointer vulnerability June 12 at 10:45 AM PDT. Spanner replicated bad data globally within seconds causing synchronized crash loops across all regions. 100+ services affected for 3 hours. Status dashboard ran on affected infrastructure leaving customers blind.',
  0.95,
  'VERY_HIGH',
  'Service Control handles API authorization and quota checking for all GCP services',
  'Graceful degradation on invalid input, randomized exponential backoff on restarts, monitoring infrastructure independent from production services',
  'https://status.cloud.google.com/incidents/ow5i3PPK96RduMcb1SsW',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Compute Engine May 2025 feature flag rollout safety check failure',
  'incident-postmortem',
  '[{"solution": "Enforce phased rollout safety checks to prevent rapid global deployments", "percentage": 95, "note": "Configuration change disabled feature flag controlling VM state reporting"}, {"solution": "Improve monitoring coverage of Spot VM deletion workflows", "percentage": 85, "note": "Spot VMs stuck in termination caused backlog affecting all VM types"}, {"solution": "Address GCE scalability issues during high-load recovery scenarios", "percentage": 80, "note": "Control plane median latency went from seconds to minutes during incident"}]'::jsonb,
  'Configuration change accidentally disabled feature flag for VM instance state reporting. Safety checks meant to enforce gradual rollout failed to trigger causing unplanned rapid global deployment. Spot VMs stuck in unexpected state created termination backlogs degrading performance across all VM types in us-central1 and us-east4. Lasted 8 hours 42 minutes affecting 14+ dependent services including GKE, Cloud SQL, Dataflow, Cloud Composer.',
  0.90,
  'HIGH',
  'GCE control plane operations for VM lifecycle management',
  'Phased rollout with automatic rollback on error rate thresholds, comprehensive safety check audit for feature flag systems',
  'https://status.cloud.google.com/incidents/SXRPpPwx2RZ5VHjTwFLx',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud us-east1 July 2025 network switch hardware maintenance error',
  'incident-postmortem',
  '[{"solution": "Strengthen safety controls for hardware upgrade workflows with physical verification", "percentage": 90, "note": "Technician disconnected active switch instead of redundant unit scheduled for removal"}, {"solution": "Design control plane partitioning prevention mechanisms", "percentage": 85, "note": "Topology change during failed control plane state caused stale information and packet loss"}, {"solution": "Pause non-critical workflows until safety controls implemented", "percentage": 95, "note": "Immediate mitigation completed; comprehensive controls targeted for Q3-Q4 2025"}]'::jsonb,
  'Procedural error during planned hardware maintenance: incorrect physical disconnection made to active network switch serving control plane rather than redundant unit. Topology change while network control plane in failed state led to stale topology information causing packet loss and service disruption. 0.1% of Persistent Disks in us-east1-b temporarily unavailable. Workspace users in Southeast experienced disruptions across Gmail, Meet, Drive, Chat, Calendar.',
  0.90,
  'HIGH',
  'Network control plane with redundant switches for high availability',
  'Automated safety verification before hardware changes, control plane resilience during topology changes',
  'https://status.cloud.google.com/incidents/8cY8jdUpEGGbsSMSQk7J',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud europe-west3-c October 2024 power arc flash cooling failure',
  'incident-postmortem',
  '[{"solution": "Conduct root cause investigation on arc flash and repair power distribution unit", "percentage": 95, "note": "Electrical arc flash in PDU caused partial power outage and cooling damage"}, {"solution": "Implement risk mitigation across data centers to ensure arc flash causes not present elsewhere", "percentage": 90, "note": "Review all facilities for similar vulnerability"}, {"solution": "Harden Persistent Disk services to prevent regional impact during single-zone failures", "percentage": 85, "note": "Regional disks operated in degraded state during zone outage"}]'::jsonb,
  'Electrical arc flash occurred in power distribution unit at 18:22 US/Pacific causing partial power outage and cooling infrastructure damage. Forced controlled shutdown of IT equipment to prevent thermal damage. 7 hours 39 minutes of service degradation. Multiple services affected: VMs offline, increased GCE instance creation latency/errors, Pub/Sub API errors, Dataflow scaling delays, GKE node creation failures, Cloud Build startup delays, Vertex AI batch prediction failures. Misconfiguration in access control caused additional latency/errors across all europe-west3 zones (19:12-23:34).',
  0.85,
  'HIGH',
  'Redundant power feeds and cooling systems for data center infrastructure',
  'Power distribution unit resilience, thermal monitoring with automatic equipment protection, zone-isolated service architectures',
  'https://status.cloud.google.com/incidents/e3yQSE1ysCGjCVEn2q1h',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google BigQuery December 2024 API key project deletion Drive export failure',
  'incident-postmortem',
  '[{"solution": "Eliminate dependency on API keys for BigQuery-Google service integrations", "percentage": 95, "note": "Use service-to-service authentication instead of API keys"}, {"solution": "Implement deletion protection mechanisms like project liens for critical resources", "percentage": 90, "note": "Prevent accidental deletion of infrastructure projects"}, {"solution": "Establish regular project metadata reviews to validate ownership accuracy", "percentage": 85, "note": "Project flagged for policy non-compliance with unclear ownership"}]'::jsonb,
  'Internal API key project inadvertently deleted due to: flagged for policy non-compliance, unclear project ownership, system mistakenly classified as abandoned. Deletion caused BigQuery data export to Google Drive failures globally across all regions for 3 hours 25 minutes. Users received "API key not valid" and "Failed to read the spreadsheet" errors preventing data transfer and impacting downstream workflows. Engineers restored deleted project at 15:45 PST recovering API keys.',
  0.95,
  'HIGH',
  'API key management for inter-service authentication',
  'Enhanced approval processes for project deprecation with mandatory impact assessments, access controls restricting critical project deletion',
  'https://status.cloud.google.com/incidents/eEsQrpyy4PEE14RG8aHf',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Compute Engine asia-northeast1 September 2024 metadata store failover',
  'incident-postmortem',
  '[{"solution": "Enhance automated failover logic in metadata storage infrastructure", "percentage": 90, "note": "Secondary issue caused automated failover to not work despite healthy replicas"}, {"solution": "Improve testing for resource contention corner cases", "percentage": 85, "note": "Software update introduced poor handling of rare resource contention"}, {"solution": "Strengthen internal processes and documentation for faster manual failover response", "percentage": 80, "note": "Manual failover to healthy zones took 1 hour 50 minutes"}]'::jsonb,
  'Metadata store software update introduced poor handling of rare resource contention corner case causing write failures in GCE internal DNS. Automated failover did not work leaving entire metadata storage unavailable despite healthy replicas in other zones. Six services experienced degradation: GCE (VM creation/deletion/start/modification failed), Cloud Build (deployments via API failed), Cloud Dataflow (job startup failures and autoscaling disruption), Cloud Deploy (render/deploy/verification unresponsive), Cloud SQL (database creation/deletion impossible), GKE (cluster creation, node autoscaling, upgrades unsuccessful). Duration: 1 hour 50 minutes.',
  0.85,
  'MEDIUM',
  'Distributed metadata storage with zone-redundant replicas for high availability',
  'Automated failover triggers on primary zone failure, comprehensive resource contention testing before production deployment',
  'https://status.cloud.google.com/incidents/XwnegjADrYy2GHJphG2V',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud VPC May 2024 maintenance automation zone parameter refactoring bug',
  'incident-postmortem',
  '[{"solution": "Add validation to ensure maintenance operations have properly scoped zone information", "percentage": 95, "note": "Parameter refactoring error caused shutdown to affect all zones globally instead of single zone"}, {"solution": "Improve safety in underlying tools to reject unzoned operations", "percentage": 90, "note": "Critical safety guard limiting scope inadvertently disabled during migration"}, {"solution": "Increase access controls limiting multi-zone operations in single invocations", "percentage": 85, "note": "Prevent global scope changes without explicit multi-zone intent"}]'::jsonb,
  'Bug in maintenance automation during router replacement. Zone-specification parameter modified during automation software refactoring earlier in 2024. Test environment caught failure but engineers misinterpreted as expected test failure. Critical safety guard limiting maintenance scope inadvertently disabled during migration to new version. Intended shutdown of unused VPC Controller in single zone affected all zones globally. Deleted BGP control plane sessions linking datacenter to backbone routers. 35+ services affected globally for 2 hours 48 minutes. New instances deployed without network connectivity. GKE cluster/node creation and scaling failed. Cloud Run/App Engine Flex unable to deploy or scale. Cloud SQL creation/clone/restore operations failed. Up to 3% packet loss in some zones. 0.07% of VMs lost connectivity during migration.',
  0.90,
  'VERY_HIGH',
  'VPC Controllers manage network infrastructure and BGP sessions for cloud connectivity',
  'Required safety checks and testing including positive/negative test cases, zone-scoped operations validated before execution',
  'https://status.cloud.google.com/incidents/xVSEV3kVaJBmS7SZbnre',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Kubernetes Engine March 2024 NVIDIA GPU driver bucket 403 Forbidden',
  'incident-postmortem',
  '[{"solution": "Establish partner coordination protocols for external dependency failures", "percentage": 85, "note": "Access issue prevented downloads from NVIDIA-managed storage bucket"}, {"solution": "Implement local caching or mirroring for critical third-party dependencies", "percentage": 80, "note": "GKE nodes downloaded drivers directly from NVIDIA bucket on creation"}, {"solution": "Halt automatic node recreation during external dependency outages as emergency mitigation", "percentage": 90, "note": "Google stopped cascading failures by preventing new driver download attempts"}]'::jsonb,
  'Storage bucket managed by NVIDIA (Google GPU driver partner) returned 403 Forbidden errors when GKE nodes attempted to retrieve GPU drivers. Error: "Failed to download GPU driver installer, status: 403 Forbidden". Impact limited to newer GPU models (T4, L4, H100 80GB, A100) with COS milestone 105+ and driver versions R525+. Older GPUs (P4, P100, V100, K80) unaffected. Nodes became unavailable when newly created or recreated. Duration: 6 hours 55 minutes across all GPU-enabled regions globally. Google halted automatic node recreation to prevent further failures while engineers contacted partner to restore bucket access.',
  0.80,
  'MEDIUM',
  'External partner dependency for GPU driver distribution to GKE nodes',
  'Redundant driver sources, partner SLA monitoring with automated alerts, graceful degradation when external dependencies unavailable',
  'https://status.cloud.google.com/incidents/aRSt8sTQLKMTVgdbbK6P',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud Load Balancing August 2024 europe-west2 substation switchgear failure',
  'incident-postmortem',
  '[{"solution": "Review automation improvements for tasks requiring manual traffic redistribution intervention", "percentage": 85, "note": "Automated recovery triggered but manual engineering needed to redistribute overloaded traffic"}, {"solution": "Increase Cloud Interconnect control plane resilience to power events", "percentage": 80, "note": "Cloud Interconnect affected by POP power loss"}, {"solution": "Enhance automated reaction protocols for facility isolation events", "percentage": 90, "note": "One-third of first-layer Google Front Ends and distributed networking equipment offline"}]'::jsonb,
  'Substation switchgear failure caused simultaneous loss of primary and backup power feeds to Google Point of Presence (POP) in London. Facility hosted approximately one-third of first-layer Google Front Ends and distributed networking equipment for region. Power failure at 06:20 US/Pacific caused connectivity issues. Google automatically redirected connections to nearest available infrastructure at 06:23. Power restored at 06:43, networking equipment fully operational at 06:57. Workspace services recovered at 07:00, all GCP services fully resolved at 08:32. Duration: 2 hours 12 minutes for GCP, 40 minutes for Workspace. Affected: Cloud CDN, Cloud Load Balancing, Hybrid Connectivity, VPC, Cloud Interconnect, Workspace (Gmail, Calendar, Chat, Docs, Drive, Meet, Tasks). Customers experienced intermittent timeout (500s) followed by elevated latency in UK region.',
  0.85,
  'MEDIUM',
  'Redundant power feeds with automatic failover for critical networking infrastructure',
  'Power distribution redundancy, geographic load balancing with automatic traffic shifting, partner facility coordination for infrastructure resilience',
  'https://status.cloud.google.com/incidents/ETJGhvY9Xaktw7tgi8dF',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud Networking October 2018 BGP session deletion during router rollback',
  'incident-postmortem',
  '[{"solution": "Fix automated workflows to use correct router-specific versions for hardware replacements", "percentage": 95, "note": "Workflow version incompatible with older routers triggered during rollback"}, {"solution": "Implement alerts for BGP session deletions and traffic failover events", "percentage": 90, "note": "Detection took 33 minutes from first BGP deletion to engineer alert"}, {"solution": "Add pre-deployment validation for configuration changes on legacy network equipment", "percentage": 85, "note": "Configuration pushed to legacy routers deleted BGP control plane sessions"}]'::jsonb,
  'During router replacement operations in us-central1-c, network team initiated rollback using workflow version incompatible with older routers. Configuration changes pushed to legacy routers deleted Border Gateway Protocol (BGP) control plane sessions linking datacenter to backbone routers severing external connectivity. First BGP session deletion at 15:43 PDT, traffic failed over to alternate routers. Second BGP deletion wave at 16:13. Initial alert at 16:16. Root cause identified (deleted BGP sessions) at 16:41. Configuration rollback completed at 16:52 resolving incident. Duration: 41 minutes of external connectivity loss. 61% of VMs in us-central1-c affected (zone contains two separate clusters). VM-to-VM cross-zone communication blocked, internal traffic unaffected. 30% of Cloud Bigtable clusters unreachable. 10% of Cloud SQL instances lost external connectivity. Network load balancer traffic partially failed.',
  0.90,
  'MEDIUM',
  'BGP routing protocol for datacenter-to-backbone connectivity',
  'Version compatibility checks before applying configuration changes, real-time BGP session monitoring with immediate alerting',
  'https://status.cloud.google.com/incidents/vozgQNxfrynmjvbh3iAC',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud cascading failure pattern synchronized crash loops from replicated metadata',
  'incident-postmortem',
  '[{"solution": "Implement circuit breakers and exponential backoff to prevent synchronized crash loops", "percentage": 95, "note": "Lack of crash loop containment prevented graceful degradation requiring complete rollback"}, {"solution": "Add validation checkpoints before global metadata replication to prevent bad data propagation", "percentage": 90, "note": "Spanner near-instant replication spread corrupted metadata globally within seconds"}, {"solution": "Decouple monitoring infrastructure from production services to maintain visibility during outages", "percentage": 85, "note": "Status dashboard ran on affected infrastructure leaving customers blind during crisis"}]'::jsonb,
  'Cascading failure pattern identified across multiple GCP incidents (June 2025 Service Control, May 2025 GCE): Corrupted metadata propagates globally via Spanner replication within seconds. Global synchronization: every regional instance processes identical bad data simultaneously creating synchronized crashes. Service collapse: control plane gates all API traffic so crashes return HTTP 503 errors across 40+ regions. Downstream effects: consumer platforms (Gmail, Spotify, Discord), enterprise tools (GitLab, Shopify), infrastructure services (Cloudflare auth) fail within minutes. Recovery challenges: us-central1 exposed "herd effect" where simultaneous restart attempts overwhelm backend infrastructure without exponential backoff.',
  0.95,
  'VERY_HIGH',
  'Global distributed systems with replicated metadata stores',
  'Graceful degradation on invalid input, randomized restart timing to prevent herd effect, independent monitoring infrastructure, staged metadata validation before replication',
  'https://blog.bytebytego.com/p/how-the-google-cloud-outage-crashed',
  'sonnet-4',
  NOW(),
  0
),
(
  'Google Cloud single point of failure control plane architecture dependency',
  'incident-postmortem',
  '[{"solution": "Modularize critical control plane systems to isolate failure blast radius", "percentage": 90, "note": "Service Control single system disrupted many dependent APIs in June 2025 outage"}, {"solution": "Implement request path fallbacks that allow operations to proceed during control plane degradation", "percentage": 85, "note": "Design services to function with reduced capability rather than complete failure"}, {"solution": "Create independent execution paths for critical operations to avoid single chokepoint dependencies", "percentage": 80, "note": "VPC Controller failure May 2024 affected 35+ services due to centralized network control"}]'::jsonb,
  'Architectural pattern identified across multiple incidents: Service Control (June 2025) single control plane system for API authorization/quota checking caused 100+ service failures when crashed. VPC Controllers (May 2024) centralized network infrastructure management affected 35+ services when automation bug caused global shutdown. Metadata storage (September 2024) single source of truth for GCE internal DNS caused multi-service degradation when failover failed. Pattern: tight coupling to single control plane component creates cascading failures where one system crash impacts dozens of dependent services. Lack of modularization and fallback paths prevents graceful degradation.',
  0.90,
  'VERY_HIGH',
  'Distributed cloud architecture with control plane and data plane separation',
  'Modular control plane design with isolated failure domains, request path redundancy with degraded-mode operation capability, circuit breakers preventing cascading failures',
  'https://hyperframeresearch.com/2025/06/24/google-cloud-anatomy-of-a-systemic-failure/',
  'sonnet-4',
  NOW(),
  0
);
