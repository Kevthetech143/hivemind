-- Add AWS incident postmortems batch 1
-- Major AWS service outages with root cause analysis and remediation patterns

INSERT INTO knowledge_entries (
    query, category, hit_frequency, solutions, prerequisites, success_indicators,
    common_pitfalls, success_rate, claude_version, last_verified, source_url
) VALUES (
    'AWS S3 outage 2017-02-28: 4-hour degradation in us-east-1',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Modify operational tools to prevent removing capacity below minimum thresholds", "percentage": 95, "note": "Root cause: command with incorrect input removed critical subsystem capacity"},
        {"solution": "Partition large subsystems into smaller cells to reduce blast radius", "percentage": 90, "note": "Index subsystem cellularization prioritized after incident"},
        {"solution": "Implement safeguards in capacity removal tools to work gradually rather than all-at-once", "percentage": 92, "note": "Prevents cascading failures during maintenance"},
        {"solution": "Test full restart procedures for critical subsystems regularly, especially in large regions", "percentage": 88, "note": "Systems had not been fully restarted in years, making recovery validation slower"}
    ]'::jsonb,
    'Established playbook for capacity management, Multi-region Service Health Dashboard architecture',
    'Capacity removal completes without subsystem degradation, Service Health Dashboard remains operational during S3 issues, Full subsystem restarts complete within expected timeframes',
    'Do not assume large-scale systems can recover quickly without regular restart testing. Service Health Dashboard should not depend on S3. Input validation on operational commands is critical.',
    0.92,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/41926/'
),
(
    'AWS EC2/EBS outage 2011-04-21: Re-mirroring storm in us-east-1',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Implement aggressive exponential backoff when nodes cannot find re-mirroring capacity", "percentage": 90, "note": "Root cause: nodes searched repeatedly without backing off, exhausting cluster capacity"},
        {"solution": "Fix race condition causing node crashes during high-volume concurrent connection closures", "percentage": 93, "note": "Bug caused crashed nodes to require re-mirroring, compounding the storm"},
        {"solution": "Increase capacity buffer significantly for large-scale recovery events", "percentage": 88, "note": "13% of volumes stuck triggered cluster-wide capacity exhaustion"},
        {"solution": "Improve timeout logic in control plane to prevent thread starvation during slow API calls", "percentage": 91, "note": "Long-running Create Volume requests backed up, causing cascading failures"},
        {"solution": "Isolate Availability Zone failures: migrate control plane functionality to per-cluster services in same AZ", "percentage": 87, "note": "Prevents cross-AZ cascading failures"}
    ]'::jsonb,
    'Multi-AZ architecture for critical workloads, Snapshots enabled for volume recovery, Sufficient spare capacity in cluster',
    'Re-mirroring completes without exhausting cluster capacity, Control plane APIs maintain normal response times, Multi-AZ RDS instances failover automatically within expected timeframes',
    'Single-AZ deployments are vulnerable to zone-wide failures. Multi-AZ RDS instances can fail to auto-failover if control plane is degraded. Network changes during maintenance can route traffic to wrong network.',
    0.90,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/65648/'
),
(
    'AWS Kinesis outage 2020-11-25: Thread exhaustion from capacity addition',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Monitor OS-level thread consumption with fine-grained alarming before hitting limits", "percentage": 92, "note": "Root cause: new capacity caused all servers to exceed OS thread limits"},
        {"solution": "Increase OS thread count limits or migrate to larger instances to reduce thread requirements", "percentage": 88, "note": "Short-term fix to prevent recurrence"},
        {"solution": "Cellularize front-end fleet to isolate failures and bound scaling impact", "percentage": 90, "note": "Prevents single scaling operation from affecting entire fleet"},
        {"solution": "Move caching layer to dedicated fleet to reduce thread consumption on front-end servers", "percentage": 85, "note": "Architectural improvement to decouple concerns"},
        {"solution": "Implement fallback to authoritative metadata store when cache construction fails", "percentage": 87, "note": "Allowed gradual recovery during incident"}
    ]'::jsonb,
    'Monitoring of OS resource limits, Cascading service resilience (Cognito, CloudWatch, Lambda should buffer/cache through Kinesis outages)',
    'Thread count remains well below OS limits after scaling operations, Front-end fleet can construct cache successfully, Dependent services maintain availability during Kinesis degradation',
    'Capacity additions can trigger latent bugs at unprecedented scale thresholds. Dependent services must implement buffering/caching. Service Health Dashboard updates require Cognito, creating circular dependency.',
    0.91,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/11201/'
),
(
    'AWS us-east-1 outage 2021-12-07: Network device overload from retry storm',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Fix client backoff behavior to prevent retry storms during network congestion", "percentage": 93, "note": "Root cause: unexpected client behavior created surge overwhelming network devices"},
        {"solution": "Deploy additional network capacity and configuration to protect devices during congestion events", "percentage": 90, "note": "Infrastructure hardening to handle similar events"},
        {"solution": "Reroute critical internal traffic (DNS, monitoring) away from congested paths during incidents", "percentage": 88, "note": "Emergency mitigation that proved effective"},
        {"solution": "Isolate top traffic sources to dedicated network devices to prevent cascading failures", "percentage": 85, "note": "Identify and segregate heavy traffic early"},
        {"solution": "Architect support and monitoring systems to run across multiple regions independently", "percentage": 92, "note": "Internal network dependency prevented incident response visibility"}
    ]'::jsonb,
    'Multi-region monitoring and deployment systems, Network device capacity headroom, Real-time traffic analysis capabilities',
    'Automated scaling operations complete without triggering retry storms, Internal monitoring remains operational during network stress, Service Health Dashboard updates consistently across regions',
    'Internal network degradation severely impairs incident response capabilities. Monitoring systems must not depend on the systems being monitored. Service Health Dashboard global banner format makes regional information hard to find.',
    0.89,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/12721/'
),
(
    'AWS Tokyo EC2/EBS outage 2019-08-23: Datacenter cooling failure',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Disable failover modes in control systems that trigger vendor bugs causing unresponsiveness", "percentage": 90, "note": "Root cause: third-party control system bug during failover rendered PLCs unresponsive"},
        {"solution": "Modify air handling unit controls so purge mode completely bypasses PLCs", "percentage": 92, "note": "Prevents control system bugs from blocking emergency cooling"},
        {"solution": "Train operations teams to quickly identify and manually reset control system equipment", "percentage": 85, "note": "Manual intervention was required during incident"},
        {"solution": "Deploy applications across multiple Availability Zones for resilience against datacenter-level failures", "percentage": 95, "note": "Customers with multi-AZ deployments maintained availability"}
    ]'::jsonb,
    'Multi-AZ application architecture, Trained operations staff for datacenter equipment, Vendor coordination for control system bugs',
    'Cooling systems activate properly during temperature anomalies, Control system failovers complete without causing unresponsiveness, Multi-AZ applications maintain availability during single-AZ outages',
    'Third-party control systems may have latent bugs in failover paths. Default maximum cooling and purge modes must not depend on PLCs that can fail. Single-AZ deployments are vulnerable to physical infrastructure failures.',
    0.88,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/56489/'
),
(
    'AWS DynamoDB outage 2015-09-20: Metadata service capacity exhaustion',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Monitor metadata service performance dimensions like membership list size, not just throughput", "percentage": 91, "note": "Root cause: Global Secondary Index adoption increased partition sizes to near timeout limits"},
        {"solution": "Segment metadata service into multiple instances, each serving portions of storage fleet", "percentage": 93, "note": "Prevents single metadata service from becoming bottleneck"},
        {"solution": "Reduce storage node membership request rates and lengthen processing timeframes", "percentage": 87, "note": "Gives metadata service more time to process large membership lists"},
        {"solution": "Significantly increase metadata service capacity when usage patterns change", "percentage": 89, "note": "Rapid GSI adoption outpaced capacity planning"},
        {"solution": "Implement caching in dependent services (SQS, CloudWatch) to maintain operations during brief DynamoDB unavailability", "percentage": 90, "note": "Reduces cascading failures"}
    ]'::jsonb,
    'Monitoring of metadata service processing times, Capacity headroom for metadata services, Usage pattern tracking (e.g., GSI adoption rates)',
    'Metadata service processing times remain well below timeout thresholds, Storage servers receive membership assignments promptly after network disruptions, Dependent services maintain availability through brief DynamoDB issues',
    'Low aggregate error rates can mask disproportionate impact on specific customer subsets. Rapid feature adoption (GSIs) can create unexpected capacity bottlenecks. Dashboard communication should not wait for full clarity.',
    0.90,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/5467D2/'
),
(
    'AWS Lambda outage 2023-06-13: Latent defect at unprecedented capacity threshold',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Bound all cellular architectures to well-tested capacity sizes to prevent scaling surprises", "percentage": 92, "note": "Root cause: crossing unprecedented threshold triggered latent bug in cellular architecture"},
        {"solution": "Scale down fleet below threshold triggering defect as immediate mitigation", "percentage": 88, "note": "Emergency response that allowed recovery"},
        {"solution": "Fix and deploy latent bug across all regions after reproducing at scale", "percentage": 94, "note": "Permanent resolution of compute provisioning defect"},
        {"solution": "Implement containment measures for cellular scaling to prevent cross-cell impact", "percentage": 89, "note": "Architectural gap exposed by incident"}
    ]'::jsonb,
    'Load testing at unprecedented capacity thresholds, Cellular architecture with proper isolation, Monitoring for execution environment allocation vs utilization gaps',
    'Lambda Frontend scaling completes without triggering provisioning defects, Execution environments allocated are fully utilized, Dependent services (STS, Console, EKS, Connect, EventBridge) maintain normal operations',
    'Cellular architectures must be tested at capacity boundaries before reaching them in production. Execution environment allocation without utilization indicates provisioning pipeline breakage. Synchronous vs asynchronous invocations recover at different rates.',
    0.91,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/061323/'
),
(
    'AWS EU-West outage 2014-08-07: Power loss and cascading failures',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Add PLC redundancy and environmental isolation to prevent single points of failure in power control systems", "percentage": 90, "note": "Root cause: ground fault caused synchronization controller malfunction"},
        {"solution": "Implement better load balancing for EC2 API management servers to handle failed zone hosts", "percentage": 88, "note": "Management servers accepting requests for inaccessible zone caused cascade"},
        {"solution": "Complete control plane isolation across Availability Zones to prevent cross-zone failures", "percentage": 92, "note": "Critical architectural improvement"},
        {"solution": "Enable direct EBS volume recovery on servers without requiring S3 snapshot intermediary", "percentage": 85, "note": "58% of volumes losing power simultaneously exhausted snapshot capacity"},
        {"solution": "Implement instrumentation alarms for snapshot cleanup anomalies and longer holding periods", "percentage": 89, "note": "Software bug in cleanup process caused incomplete snapshot deletion"},
        {"solution": "Fix RDS health checks to handle DNS connectivity issues during failover", "percentage": 91, "note": "Extended failover delays when primary/secondary status indeterminate"}
    ]'::jsonb,
    'Multi-AZ deployments, Sufficient spare EBS capacity for large-scale re-mirroring, Snapshot cleanup verification processes, RDS Multi-AZ configuration',
    'Backup generators synchronize successfully when utility power fails, EC2 APIs remain responsive when zones become inaccessible, EBS volumes recover within expected timeframes, RDS Multi-AZ failovers complete promptly',
    'Utility transformer failures can occur without warning. Backup generators may fail to synchronize due to ground faults. Snapshot cleanup bugs can persist undetected. Human verification can miss software-generated errors. DNS issues can prevent RDS health checks.',
    0.87,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/2329B7/'
),
(
    'AWS Seoul EC2 DNS outage 2018-11-22: Configuration error removing minimum capacity',
    'incident-postmortem',
    'MEDIUM',
    '[
        {"solution": "Implement semantic validation for configuration updates to ensure minimum healthy host requirements are preserved", "percentage": 94, "note": "Root cause: config update mistakenly deleted minimum host settings"},
        {"solution": "Add rate limiting to throttle healthy host removal to controlled amounts per hour", "percentage": 91, "note": "Prevents rapid capacity degradation"},
        {"solution": "Deploy safeguards preventing invalid configuration parameters from causing fleet downscaling", "percentage": 93, "note": "Defense in depth against configuration errors"},
        {"solution": "Halt additional host removal immediately when DNS resolution issues detected (15-minute response achieved)", "percentage": 88, "note": "Fast incident response limited impact to 84 minutes"}
    ]'::jsonb,
    'Configuration validation pipelines, DNS resolver fleet capacity monitoring, Regional configuration consistency checks',
    'Configuration updates preserve minimum healthy host thresholds, DNS resolver fleet maintains capacity above operational minimums, EC2 instances successfully resolve internal DNS queries',
    'Configuration updates that delete required settings can cause operational tools to use extremely low defaults. DNS resolution failures affect EC2 instances but not network connectivity or external DNS. Recovery requires rebuilding fleet capacity.',
    0.93,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/74876-2/'
),
(
    'AWS incident pattern: Cascading failures from dependency on degraded internal services',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Implement local caching/buffering in dependent services to maintain functionality during brief upstream outages", "percentage": 92, "note": "CloudWatch caching 3 hours of metrics locally reduced Kinesis dependency"},
        {"solution": "Architect critical services (monitoring, support, deployment) to run across multiple regions independently", "percentage": 90, "note": "Internal network dependency prevented incident response in 2021-12-07 event"},
        {"solution": "Prevent thread exhaustion in control planes by improving timeout logic and load-shedding capabilities", "percentage": 88, "note": "DynamoDB metadata service and EC2 control plane both suffered thread starvation"},
        {"solution": "Design Service Health Dashboard to not depend on services it monitors", "percentage": 94, "note": "S3 outage prevented dashboard updates; Kinesis outage affected Cognito-dependent updates"}
    ]'::jsonb,
    'Multi-region architecture for operational services, Client-side buffering/caching implementations, Circuit breakers and fallback mechanisms',
    'Dependent services maintain availability during upstream degradation, Monitoring and deployment systems remain functional during incidents, Service Health Dashboard updates consistently',
    'Circular dependencies create cascading failures (monitoring depends on services being monitored). Thread/connection exhaustion spreads across services sharing control planes. Retry storms compound network congestion.',
    0.91,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/41926/'
),
(
    'AWS incident pattern: Latent bugs triggered at unprecedented scale thresholds',
    'incident-postmortem',
    'HIGH',
    '[
        {"solution": "Test all cellular architectures and control systems at capacity boundaries before reaching them in production", "percentage": 93, "note": "Lambda 2023, Kinesis 2020 both hit untested thresholds"},
        {"solution": "Implement fine-grained monitoring for resource limits (threads, connections, memory) before hitting OS/application maximums", "percentage": 90, "note": "Kinesis exceeded OS thread limits; Lambda hit cellular capacity limits"},
        {"solution": "Bound cells to well-tested sizes and implement containment for scaling operations", "percentage": 91, "note": "Prevents single scaling event from affecting entire fleet"},
        {"solution": "Add instrumentation to detect allocation-vs-utilization gaps in provisioning pipelines", "percentage": 87, "note": "Lambda allocated execution environments that were never utilized"}
    ]'::jsonb,
    'Load testing methodology for unprecedented scales, Cellular architecture with proper isolation, Resource limit monitoring and alarming',
    'Scaling operations complete successfully at new capacity thresholds, Resource consumption stays well below OS/application limits, Provisioning pipelines show matching allocation and utilization metrics',
    'Growth can trigger latent bugs that only manifest at unprecedented scale. Systems that have worked for years may have never been tested at current capacity. Cellular isolation must be tested, not assumed.',
    0.89,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/061323/'
),
(
    'AWS incident pattern: Operational tool misconfiguration causing large-scale capacity removal',
    'incident-postmortem',
    'VERY_HIGH',
    '[
        {"solution": "Add safeguards to operational tools preventing capacity removal below minimum thresholds", "percentage": 95, "note": "S3 2017: incorrect input removed critical subsystem; Seoul 2018: config deleted minimum hosts"},
        {"solution": "Implement semantic validation on configuration updates to verify critical parameters are preserved", "percentage": 92, "note": "Seoul DNS incident: validation would have caught deleted minimum host settings"},
        {"solution": "Make capacity removal work gradually with rate limiting rather than all-at-once", "percentage": 90, "note": "S3 incident prioritized this as immediate remediation"},
        {"solution": "Require multi-step confirmation for operations affecting large capacity percentages", "percentage": 88, "note": "Human verification as defense against input errors"}
    ]'::jsonb,
    'Validated operational playbooks, Configuration management with semantic checks, Capacity monitoring and alarming, Staged rollout procedures',
    'Capacity removal operations complete without violating minimum thresholds, Configuration updates preserve critical operational parameters, Large-scale changes require explicit confirmation, Capacity degradation triggers immediate alerts',
    'Established playbooks can still be executed with incorrect inputs. Configuration systems may use dangerous defaults when required parameters are missing. Human operators can miss errors in auto-generated data.',
    0.93,
    'sonnet-4',
    NOW(),
    'https://aws.amazon.com/message/41926/'
);
